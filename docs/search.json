[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reproducible data treatment with R",
    "section": "",
    "text": "Welcome"
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Reproducible data treatment with R",
    "section": "License",
    "text": "License\nThis book and its code samples are licensed to you under the MIT License."
  },
  {
    "objectID": "01-about.html#objectives-of-the-class",
    "href": "01-about.html#objectives-of-the-class",
    "title": "\n1  About the class\n",
    "section": "\n1.1 Objectives of the class",
    "text": "1.1 Objectives of the class\nThe goal of this class is that at the end, the students are able to:\n\nTreat their data with the free and open source language R, i.e.:\n\nRead, browse, manipulate and plot their data\nModel or simulate their data\n\n\nMake automatic reporting through Rmarkdown\n\nBuild a graphical interface with Shiny to interact with their data and output something (a value, a pdf report, a graph…)\n\nWhat you will learn here will be useful in any scientific domain. The examples in this course are however mainly coming from the type of data you might encounter in Materials Science because, well, it’s what I have on hand…"
  },
  {
    "objectID": "01-about.html#prerequisites",
    "href": "01-about.html#prerequisites",
    "title": "\n1  About the class\n",
    "section": "\n1.2 Prerequisites",
    "text": "1.2 Prerequisites\n\n\nCoding skills: none expected.\nThe students should come with a laptop with admin rights (i.e. you should be able to install stuff)."
  },
  {
    "objectID": "01-about.html#motivations",
    "href": "01-about.html#motivations",
    "title": "\n1  About the class\n",
    "section": "\n1.3 Motivations",
    "text": "1.3 Motivations\n\n1.3.1 Reproducible data treatment: why it matters\nHere is an introduction from the Wikipedia page on reproducible research:\n\nIn 2016, Nature conducted a survey of 1576 researchers who took a brief online questionnaire on reproducibility in research. According to the survey, more than 70% of researchers have tried and failed to reproduce another scientist’s experiments, and more than half have failed to reproduce their own experiments. […] Although 52% of those surveyed agree there is a significant ‘crisis’ of reproducibility, less than 31% think failure to reproduce published results means the result is probably wrong, and most say they still trust the published literature.1\n\nReplicability and reproducibility are some of the keys to scientific integrity. Establishing a workflow in which your data are always treated in the same manner is a necessity, because it is a way to:\n\n\nMinimize errors inherent to human manipulation\n\nKeep track of all the treatments you perform on your data and document your methodology: this allows others to reproduce your data, but also yourself.\nHelp you to make sense of all your data, and avoid disregarding some data (hence help you keep your scientific integrity)\nGain tremendous amounts of time\n\n\n\n\n\n\n\nGoal of this class\n\n\n\nIt is the objective of this class to provide you the tools necessary to work within this philosophy.\n\n\n\n1.3.2 Why with R and not python?\nThe eternal question… R was originally designed by statisticians for statisticians and it might still suffers from this “statistics only” label that sticks to it.\nPython is a wide spectrum programming language with very efficient numerical libraries used in the computer science community.\nR is focused on data treatment, statistics and representation. In R, the object is the data, and base R allows you to read, treat, fit and plot your data very easily – although you will still most certainly need additional packages.\nSo with python, you can do everything, including treating and analyzing scientific data – with the right packages. With R, you can do less but do very well what you do, and in my opinion more seamlessly (probably because I learned and used R for years before starting with python…). In my opinion, this xkcd comic about python environment is only slightly exaggerated… while for R, installation and maintenance is sooooo easy in comparison…\nEach language has his own strengths and weaknesses. To my tastes, I would say that python and R compare like that (although a pythonist would probably say the opposite):\n\n\n\n\n\n\n\n\nR\nPython\n\n\n\nFree and open source\n✔✔✔\n✔✔✔\n\n\nIDE\n✔✔✔\n✔✔✔\n\n\nLarge code repository\n✔✔✔\n✔✔✔\n\n\nLarge community\n✔✔✔\n✔✔✔\n\n\nNotebooks\n✔✔✔\n✔✔✔\n\n\nMachine Learning\n✔✔✔\n✔✔✔\n\n\nPerformances\n✔✔\n✔✔✔\n\n\nEase of installation and maintenance\n✔✔✔\n✔\n\n\nData visualization\n✔✔✔\n✔\n\n\nStatistical analysis\n✔✔✔\n✔\n\n\nMulti-purpose\n✔\n✔✔✔\n\n\nSyntax, productivity, flexibility\n✔✔✔\n✔✔✔\n\n\nRmarkdown\n✔✔✔✔✔\n✔\n\n\nQuarto\n✔✔✔✔✔\n✔✔✔✔✔\n\n\n\nWell, it’s all very subjective, really. In the end, I still use both languages, each one for a different purpose:\n\nLet’s say I want to produce an initial atomic configuration for a molecular dynamics simulation, or read a molecular dynamics trajectory and compute some quantities such as a pair correlation or a mean square displacement, or perform some image-based machine learning: python (or even C, if I need to treat large trajectories).\nNow if I want to make sense of some experimental measurements or results of simulations, do some fits and produce publication-quality graphs or experimental reports: R.\n\nBoth languages are great and being able to use both is the best thing that can happen to you (relatively speaking) – especially since you can combine them in Rmarkdown using the reticulate package, which we will see later in this class.\nSo, since my goal is to provide you with tools for seamlessly read, make sense, and plot your data in the reproducible science philosophy, let’s go with R. Also, R has a great IDE (Rstudio) that really eases working with data and code. Such a nice IDE is still lacking for python."
  },
  {
    "objectID": "01-about.html#further-reading",
    "href": "01-about.html#further-reading",
    "title": "\n1  About the class\n",
    "section": "\n1.4 Further reading",
    "text": "1.4 Further reading\nThis class is indented to provide the students with the tools to handle themselves with R, Rmarkdown and Shiny, and not to provide an extensive review of everything that is possible with R. To go further:\n\n\nR\n\nR manual on CRAN\nSome cheatsheets\n\nThe tidyverse website\n\n\nTibbles.\nTidy your data\n\nTips to improve your code\n\n\n\nPlotting\n\nThe R Graph Gallery\n\nThe R Graph Cookbook\n\nThe ggplot cheatsheet\n\nAnother one\n\nAnother one quite extensive\nAnother one\n\n\n\n\n\nRmarkdown\n\nRmarkdown complete guide\n\nRmarkdown cheatsheet\n\nRmarkdown cookbook\n\nRmarkdown code chunks\n\nRmarkdown mixing languages\n\n\n\n\nShiny\n\nThe Shiny cheatsheet\n\nGuide to application layout\n\nThe Shiny Gallery: find what you want to do and adapt it to your needs\nThe official Shiny video tutorial\n\n\n\nAnd as always, if you have a question, Google is your friend!"
  },
  {
    "objectID": "01-about.html#teaser",
    "href": "01-about.html#teaser",
    "title": "\n1  About the class\n",
    "section": "\n1.5 Teaser",
    "text": "1.5 Teaser\nYou want to be able to produce interactive plots like these in an automatic experimental report?\n\n\n\n\n\n\nYou want to produce publication-quality graphs like these?\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou want to be able to build graphical interfaces like this to help you in your data treatment?\n\n\n\n\n\nStay tuned! You’ve come to the right place."
  },
  {
    "objectID": "02-stats.html#why-are-statistical-tools-necessary-in-physical-science",
    "href": "02-stats.html#why-are-statistical-tools-necessary-in-physical-science",
    "title": "\n2  A little reminder on Statistics\n",
    "section": "\n2.1 Why are statistical tools necessary in physical science?",
    "text": "2.1 Why are statistical tools necessary in physical science?\nWhen doing Science, one has to fully grasp the concept of physical measurement. Let’s take an example to visualize the importance of this concept.\n\n2.1.1 A practical example\nLet’s say you want to communicate to someone a temperature, and tell this person that the temperature is “38”. If this is a random person in the street, they might think: “nice, let’s go to the beach today!”. If this random person is from the USA, they’re gonna think: “damn, where did I put my coat?”. If that person happens to be a physician, they might think: “that kid’s got a slight fever”. If they are a physicist doing a cryostat experiment, they might think “let’s check the He tank level”… you see that one of the most important part of the measurement is missing: its unit. Units are there so that people understand each other when exchanging data, and you see here that 38 Celsius, 38 Fahrenheit or 38 Kelvin are quite different, and this quantity will mean different things in different contexts. A physical quantity given without its unit would be absolutely meaningless (unless, of course, you are looking at a unit-less quantity, like a count).\nNow let’s consider the body temperature of 38 °C given to a physician. How did you measure this temperature? With a mercury graduated thermometer or with a thermocouple? In the first case, you can probably assume that this value is given with a measurement error of at least 1 °C, meaning that the temperature you give to the physician is (38±1) °C, i.e. the physician won’t be able to decide whether they should be concerned or not. In the second case, the temperature is often given with a 0.1 °C precision, so the physician, seeing that the body temperature is (38±0.1) °C, will probably tell you to take an aspirin and rest instead of giving you something stronger to treat a possible infection. Given that the uncertainty on the given value is of 0.1 °C, one should in fact give the temperature with matching decimal precision, i.e. (38.0±0.1) °C. Writing (38±0.1) °C, (38.00001±0.1) °C or (38.00±0.10000) °C would be meaningless too.\n\n\n\n\n\n\nImportant\n\n\n\nWith this, we see that a physical measurement should be given with four parts: its actual value, its decimal precision, its uncertainty, and its unit. Should any of these four parts be missing in a physical quantity that you wanted to share, it would at best be imprecise, and at worst be utterly meaningless.\n\n\n\n2.1.2 Probabilistic description of physical systems\nLet’s continue with our example of the body temperature measured with a thermocouple or a laser thermometer with a 0.1 °C precision. Our first measurement of the body temperature yielded (38.0±0.1) °C. Now let’s repeat this measurement a number of times in various area of the body (which are left to your imagination). Let’s say it then shows (38.1±0.1) °C, (38.0±0.1) °C, (38.3±0.1) °C, (37.9±0.1) °C, (38.2±0.1) °C, (38.1±0.1) °C, (38.1±0.1) °C, (39.8±0.1) °C. What is the actual body temperature then? Should we stick to a single measurement? Of course not. We have to make an histogram of the measured values, and study the distribution of the measurements (Figure 2.1). We can then see that one of the values is clearly an outlier – something might have gone wrong there. What if we had done the measurement only once and only measured that value? We might have jumped to a very wrong conclusion, with possibly a very serious consequence like giving the wrong medicine.\n\n\n\n\nFigure 2.1: Histogram of the body temperature measurements. The red line is the mean value, the orange one is the mode and the blue one is the median.\n\n\n\n\nWith this example, we see that a physical measurement is not absolute. In fact, a physical measurement is an assessment of the probability that the physical value is within a certain range. In the case of our example, after removing the outlier for which we are certain that the measurement is wrong, it means that the measured body temperature has a high probability to be somewhere between 38.0 °C and 38.2 °C. In other (more general) terms, one could consider a measurement of a quantity \\(X\\) as a probability \\(P(x - \\sigma < X < x + \\sigma )\\) that the quantity \\(X\\) has a value between \\(x-\\sigma\\) and \\(x+\\sigma\\). The uncertainty \\(\\sigma\\) around the mean value \\(x\\) is usually given as the standard deviation of the distribution of measurements around the mean.\n\n\n\n\n\n\nImportant\n\n\n\nSince physical measurements are in fact probabilities, we can – and must – use statistical tools to characterize them."
  },
  {
    "objectID": "02-stats.html#quantifying-the-properties-of-data",
    "href": "02-stats.html#quantifying-the-properties-of-data",
    "title": "\n2  A little reminder on Statistics\n",
    "section": "\n2.2 Quantifying the properties of data",
    "text": "2.2 Quantifying the properties of data\n\n2.2.1 Data representation – presenting a measurement\nDepending on the data you are looking at, various ways of representing them are possible. I can’t stress enough the importance of picking the right representation for your data, it is the expression of your physical sense. A good representation will help you make sense of your data and communicate your results. A bad representation, well…\n\n2.2.1.1 Histograms\nWhen looking at discrete values or when you want to characterize the distribution of a measurement, it is often a good idea to use the histogram representation, which represents the frequency at which a measurement is made within a certain range, called bin. Let’s take Figure 2.1 and plot it with various bin sizes. One can see that the choice of bin size is important, as it determines whether your data are noisy or lack fine information.\n\n\n\n\n\n(a) Bin width = 0.1 °C\n\n\n\n\n\n\n(b) Bin width = 0.2 °C\n\n\n\n\n\n\n(c) Bin width = 1 °C\n\n\n\n\nFigure 2.2: Histogram of the body temperature measurements with different bin widths.\n\n\n\n2.2.1.2 Graphs\nIn case you want to represent continuous data, say the evolution of a quantity \\(y\\) with respect to a quantity \\(x\\), you should then use the graph representation. As we saw before, any physical quantity should be given with its uncertainty and unit. The same applies to a graph: it must clearly display the units of the quantities \\(x\\) and \\(y\\), and error bars that are usually taken as the standard deviation of each individual measurement (that should thus be performed a number of times, depending on what you are looking at).\n\n\n\n\n\n(a) Meaningless graph\n\n\n\n\n\n\n(b) Noise\n\n\n\n\n\n\n(c) Meaningful graph\n\n\n\n\nFigure 2.3: Representing the same datapoints without error bars, with large error bars with respect to the data, and with small error bars with respect to the data: the difference between meaningless data, noise, and meaningful data.\n\n\nYou can think of each set of {datapoint + error bar} as an histogram: the displayed point is the mean value of the histogram, and the error bar is its standard deviation. Therefore, plotting a straight line between points is usually pointless. Plotting a line going through the data points only has meaning if this line results from a physical model explaining the variation of the quantity \\(y\\) with the evolution of the quantity \\(x\\) – this is called a fit, and we will see more about it in the R class later.\n\n2.2.2 Characterizing an ensemble of measurements\nIf we take \\(N\\) repeated measurements of an observable \\(x\\), it is then natural to try to assess our knowledge of the ensemble of measures through (1) a single number representing the measured quantity, and (2) a second number representing the spread of measurements. As we saw before, the observable \\(x\\) is thus generally defined by its central (mean) value \\(\\left< x \\right>\\), its spread \\(\\sigma_x\\) (standard deviation or uncertainty), and its unit.\n\n2.2.2.1 Central value: mode, median and mean\nThe mode of an ensemble of measurements is its most frequent value. If the measurement in question is of a continuous variable, one has to bin the data in terms of a histogram in order to quantify the modal value of that distribution: the mode will be the position of the maximum of the histogram.\nThe median value of the ensemble is the value of \\(x\\) for which there are an equal number of measurements above and below that point. If there is an even number of measurements, then the median value is taken as the midpoint between the two most central values.\nThe mean (or arithmetic average) is more often used than the two previous quantities, as it usually provides a better way to quantify the “typical” value measured. The mean value is denoted either by \\(\\overline{x}\\) or \\(\\left< x \\right>\\), and is given by:\n\\[\n\\overline{x}=\\left< x \\right>=\\frac{1}{N}\\sum_{i=1}^Nx_i,\n\\] where \\(x_i\\) is the \\(i\\)-th measurement of \\(x\\).\nFigure 2.1 shows the representation of a sample of data plotted in a histogram. This figure shows the mode, mean and median. For this particular sample of data, the mean is 38.3 °C, the median is 38.1 °C, and the mode is 38.0 °C. The fact that the mode is smaller than the mean is an indication that the data are asymmetric about the mean. We usually refer to such a distribution as being skewed, and in this case the data are skewed to the right.\n\n2.2.2.2 Quantifying the spread of data: variance and standard deviation\nThe mean of an ensemble of data doesn’t provide any information as to how the data are distributed. So any description of a set of data just quoting a mean value is incomplete. We need a second number in order to quantify the dispersion of data around the mean value. The average deviations from the mean, \\(\\left< x-\\overline{x} \\right>\\), is not a useful quantity as, by definition, this will be zero for a symmetrically distributed sample of data (which is always the case for randomly distributed data – a consequence of the central limit theorem, as we will see later). We should rather consider the average value of the squared deviations from the mean as a measure of the spread of our ensemble of measurements. This is called the variance \\(V(x)\\), which is given by:\n\\[\n\\begin{aligned}\nV(x)&=\\left< (x-\\overline{x})^2 \\right>\\\\\n    &=\\frac{1}{N}\\sum_{i=1}^N(x_i-\\overline{x})^2\\\\\n    &=\\overline{x^2}-\\overline{x}^2\n\\end{aligned}\n\\tag{2.1}\\]\nThe square root of the mean-squared (root-mean-squared or RMS) deviation is called the standard deviation, and this is given by:\n\\[\n\\begin{aligned}\n\\sigma(x)&=\\sqrt{V(x)}\\\\\n         &=\\sqrt{\\overline{x^2}-\\overline{x}^2}\n\\end{aligned}\n\\tag{2.2}\\]\nThe standard deviation quantifies the amount by which it is reasonable for a measurement of \\(x\\) to differ from the mean value \\(\\overline{x}\\). Considering a Gaussian distribution, we would expect to have 31.7% of measurements deviating from the mean value by more than 1\\(\\sigma\\), and this goes down to 4.5% of measurements to deviate by more than 2\\(\\sigma\\), and 0.3% of measurements to deviate by more than 3\\(\\sigma\\). Thus, if we perform a measurement that deviates by a significant margin from the expected value of \\(\\left< x \\right>\\pm\\sigma\\), we need to ask ourselves about the significance of our measurement.\nIn general, scientists often prefer using the standard deviation rather than the variance when describing data, since as the former has the same units as the observable being measured.\n\n\n\n\n\n\nImportant\n\n\n\nA measurement of a quantity \\(x\\) is therefore usually presented under the form \\(\\left< x \\right>\\pm\\sigma_x\\), where \\(\\left< x \\right>\\) is the arithmetic average and \\(\\sigma_x\\) is the standard deviation of the data.\n\n\n\n2.2.2.3 Caveats\nThe above considerations all assume that the distribution of measured values is mono-modal, i.e. the histogram of the measured values is centered around a single value. In the case of a multimodal distribution such as shown in Figure 2.4, it would be meaningless to use such tools as the fine information on the distribution would be lost.\n\n\n\n\nFigure 2.4: A trimodal distribution of measurements. The red line shows the mean value of the distribution: it fails to grasp the reality of the distribution.\n\n\n\n\nIn this case, one should try to deconvolute the distribution in terms of individual peaks, and gather their positions, widths and intensities."
  },
  {
    "objectID": "02-stats.html#useful-distributions",
    "href": "02-stats.html#useful-distributions",
    "title": "\n2  A little reminder on Statistics\n",
    "section": "\n2.3 Useful distributions",
    "text": "2.3 Useful distributions\n\n2.3.1 Probability Density Functions\nWe should now introduce the notion of Probability Density Function (PDF). By definition, a PDF is a distribution where the total area is unity. The variation of the PDF is represents the probability of something occurring at that point in the parameter space. In general, a PDF will be described by some function \\(P(x)\\), where\n\\[\n\\int_a^b P(x)dx=1,\n\\tag{2.3}\\] where \\(a\\) and \\(b\\) are the limits of the valid domain for the \\(P(x)\\) function. The probability of obtaining a result between \\(x\\) and \\(x + dx\\) is thus \\(P(x)dx\\). Usual PDFs encountered in physics are the Poisson distribution as well as the Gaussian distribution, that we will describe in a bit.\n\n2.3.2 PDFs, mean and variance\nLet us define a PDF \\(P(x)\\) describing a continuous distribution. We can compute the average value of some quantity by computing the integral over this quantity multiplied by the PDF.\nFor example, the average value of the variable \\(x\\), distributed according to the PDF \\(P(x)\\) in the domain \\(-\\infty < x <\\infty\\), is given by:\n\\[\n\\begin{aligned}\n\\left< x \\right>&=\\int_{-\\infty}^{\\infty}xP(x)dx\\\\\n\\text{or } \\left< x \\right>&=\\sum_{i}x_iP(x_i) \\text{ in the case of a discrete distribution}\n\\end{aligned}\n\\tag{2.4}\\]\nThis is called the first moment of the PDF.\nThis method can be used to compute average values of more complicated expressions. The mean value of \\((x - \\overline{x})^2\\), i.e. the variance \\(V\\), is thus given by the \\(\\overline{x}\\)-centered second moment of the PDF, such as:\n\\[\n\\begin{aligned}\nV&=\\int_{-\\infty}^{\\infty}(x - \\overline{x})^2P(x)dx\\\\\n\\text{or } V&=\\sum_{i}(x_i - \\overline{x})^2P(x_i) \\text{ in the case of a discrete distribution}\n\\end{aligned}\n\\tag{2.5}\\]\n\n2.3.3 The Poisson distribution\n\n2.3.3.1 Definition\nWhen a certain reaction happens randomly in time with an average frequency \\(\\lambda\\) in a given time interval, then the number \\(k\\) of reactions in that time interval will follow a Poisson distribution:\n\\[\nP_\\lambda(k) = \\frac{\\lambda^ke^{-\\lambda}}{k!}\n\\tag{2.6}\\]\nExamples of encounters of Poisson distributions could be as various as the number of calls received per hours in a call center, the yearly number of Prussian soldiers killed by horse kicks… or the number of particles (photons, neutrons, neutrinos…) hitting a detector every second.\n\n\n\n\n\n(a)\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n(c)\n\n\n\n\nFigure 2.5: Poisson distribution for various parameters. While asymmetric for small values of \\(k\\) and \\(\\lambda\\), it tends towards a Gaussian lineshape at larger values.\n\n\n\n2.3.3.2 Characteristics\nAs shown on Figure 2.5, for small \\(\\lambda\\) the distribution is asymmetric and skewed to the right. As \\(\\lambda\\) increases the Poisson distribution becomes more symmetric.\nFollowing Equation 2.4, the average number of observed events, \\(\\left< k \\right>\\), is given by:\n\\[\n\\begin{aligned}\n\\left< k \\right> &= \\sum_{k=0}^\\infty kP_\\lambda(k) = \\sum_{k=1}^\\infty k\\frac{\\lambda^ke^{-\\lambda}}{k!}\\\\\n        &= \\lambda e^{-\\lambda} \\sum_{k=1}^\\infty \\frac{\\lambda^{k-1}}{(k-1)!}= \\lambda e^{-\\lambda} \\sum_{k=0}^\\infty \\frac{\\lambda^{k}}{k!}\\\\\n        &= \\lambda\n\\end{aligned}\n\\]\nIn the same manner and by using the “trick” \\(x^2=x(x-1)+x\\), the variance \\(\\sigma^2(k)\\) of the distribution is given by:\n\\[\n\\begin{aligned}\n\\sigma^2(k) &= \\sum_{k=1}^\\infty (k-\\lambda)^2\\frac{\\lambda^k e^{-\\lambda}}{k!}\\\\\n        &= \\lambda e^{-\\lambda} \\left[\\sum_{k=1}^\\infty k^2\\frac{\\lambda^{k-1}}{k!} \\underbrace{-2\\lambda\\sum_{k=1}^\\infty \\frac{\\lambda^{k-1}}{(k-1)!}}_{-2\\lambda e^\\lambda}+\\underbrace{\\sum_{k=1}^\\infty \\lambda^2\\frac{\\lambda^{k-1}}{k!}}_{\\lambda e^\\lambda}\\right]\\\\\n        &= \\lambda e^{-\\lambda} \\left[ \\underbrace{\\sum_{k=2}^\\infty k(k-1)\\frac{\\lambda^{k-1}}{k!}}_{\\lambda e^\\lambda} + \\underbrace{\\sum_{k=1}^\\infty k\\frac{\\lambda^{k-1}}{k!}}_{e^\\lambda} - \\lambda e^\\lambda\\right]\\\\\n        &=\\lambda = \\left< k \\right>\n\\end{aligned}\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nThe important result here is that, when counting random events with an average of \\(\\left< N \\right>\\), the standard deviation is \\(\\sigma=\\sqrt{\\left< N \\right>}\\). This is typically what happens when performing a diffraction or spectroscopic measurement, such as X-ray diffraction, Raman, IR or neutron spectroscopy, etc.: the longer we acquire data, the higher the number of detected “events” \\(N\\) (particle hits detector), and the “better is the statistics”. Indeed, the relative error is thus \\(\\sqrt{N}/N=1/\\sqrt{N}\\).\nThe consequence of this is that to make a factor 10 improvement on the relative error, one has to increase by 100 the number of events. This is usually done by increasing the acquisition time, which is fine as long as it is short enough. If irrealistic acquisition times start to become necessary, one should maybe try to find another way to increase \\(N\\): this can be done by improving the detector efficiency, increasing the probe (laser, neutron/x-ray) brightness, changing the experimental geometry, etc.\nFinally, for “large” numbers (\\(\\lambda\\gtrsim 100\\)) the Poisson distribution tends towards a symmetric Gaussian distribution that we will describe just after.\n\n\n\n2.3.4 The Gaussian distribution\n\n2.3.4.1 Definition\nThe Gaussian distribution, also known as the normal distribution, with a mean value \\(\\mu\\) and standard deviation \\(\\sigma\\) as a function of some variable \\(x\\) is given by:\n\\[\nP(x, \\mu, \\sigma)=\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-(x-\\mu)^2/2\\sigma^2}\n\\]\nIt is useful to transform data from the \\(x\\) space to a corresponding \\(z\\) space which has a mean value of zero, and a standard deviation of one. This transformation is given by the mapping \\(z=\\frac{x-\\mu}{\\sigma}\\), and the Gaussian distribution in terms of \\(z\\) is thus:\n\\[\nP(z)=\\frac{1}{\\sqrt{2\\pi}}e^{-z^2/2}\n\\]\n\n2.3.4.2 Characteristics\n\n\n\n\nFigure 2.6: A zero-centered Gaussian distribution with standard deviation of 1, \\(P(z)\\). The red line marks the half maximum, \\(P(z_{HM})=1/2\\sqrt{2\\pi}\\), and the blue lines the values of \\(z\\) for which the half maximum is obtained, \\(z_{HM}=\\pm\\sqrt{2\\ln{2}}\\).\n\n\n\n\nSometimes instead of quantifying a Gaussian distribution (or any monomodal distribution, for that matter) using the variance or standard deviation, scientists will speak about the full width at half maximum (FWHM). This has the advantage that any extreme outliers of the distribution do not contribute to the quantification of the spread of data. As the name suggests, the FWHM is the width of the distribution (the spread above and below the mean) at the points where the distribution reaches half of its maximum.\nFor a Gaussian distribution \\(P(z)\\), the half maximum is attained when \\(z_{HM}\\) is so that:\n\\[\n\\begin{aligned}\n\\frac{1}{\\sqrt{2\\pi}}e^{-z_{HM}^2/2}&= \\frac{1}{2}\\frac{1}{\\sqrt{2\\pi}}\\\\\n\\Rightarrow z_{HM}&=\\pm\\sqrt{2\\ln{2}}\n\\end{aligned}\n\\]\nThe FWHM of \\(P(z)\\) is therefore \\(FWHM=2\\sqrt{2\\ln{2}}\\simeq2.355\\). Using the relation between \\(z\\) and \\(\\sigma\\), we get the relation between the FWHM and the standard deviation:\n\\[\nFWHM=2\\sqrt{2\\ln{2}}\\times\\sigma\n\\]\nAs can be seen on Table 2.1, using the FWHM ensures that roughly 76% of the data are comprised between \\(\\mu-\\sigma\\) and \\(\\mu+\\sigma\\), and this goes up to \\(\\sim95\\)% for \\(\\mu-2\\sigma\\) and \\(\\mu+2\\sigma\\).\n\n\nTable 2.1: Integral values for various values of \\(a\\) in \\(\\int_{-a}^aP(z)dz\\).\n\nIntegration range \\(a\\)\n\n\\(\\int_{-a}^aP(z)dz\\)\n\n\n\n\\(\\sigma\\)\n0.68293\n\n\n\\(\\sqrt{2\\ln{2}}\\sigma\\)\n0.76100\n\n\n\\(2\\sigma\\)\n0.95455\n\n\n\\(3\\sigma\\)\n0.99730\n\n\n\\(4\\sigma\\)\n0.99994"
  },
  {
    "objectID": "02-stats.html#uncertainty-and-errors",
    "href": "02-stats.html#uncertainty-and-errors",
    "title": "\n2  A little reminder on Statistics\n",
    "section": "\n2.4 Uncertainty and errors",
    "text": "2.4 Uncertainty and errors\n\n2.4.1 Central limit theorem: on the Gaussian nature of statistical uncertainty\nThe central limit theorem states that if one takes \\(N\\) random independent samples of a distribution of data that describes some variable \\(x\\), then as \\(N\\) tends to infinity, the distribution of the sum of the samples tends to a Gaussian distribution.\nIn other terms: the mean value of a large number \\(N\\) of independent random variables (that can be distributed following any distribution with finite variance), obeying the same distribution with variance \\(\\sigma_0^2\\), approaches a normal distribution with variance \\(\\sigma^2 = \\sigma _0^2/N\\).\n\n\n\n\n\n\nImportant\n\n\n\nThis result is fundamental as it implies that independent measurements of any observable will show values that will be spread following a Gaussian distribution, and thus statistical uncertainties that are Gaussian in nature.\nMoreover, we see here the typical property of statistical errors, which is that the relative error is proportional to \\(1/\\sqrt{N}\\). Increasing the number of observations thus decreases the error, i.e. increases the precision.\n\n\n\n2.4.2 Combination of errors\nLet us consider a function of \\(n\\) variables, \\(f(u_1, u_2, ..., u_n)\\). We can Taylor expand this function about the various mean values \\(u_i=\\overline{u_i}\\), so that, at the first order:\n\\[\nf(u_1, ..., u_n) = f(\\overline{u_1}, ..., \\overline{u_n}) + \\sum_{i=1}^n (u_i-\\overline{u_i})\\frac{\\partial f}{\\partial u_i}\n\\]\nConsidering that the variance of a quantity \\(f\\) is given by \\(\\sigma^2(f) = (f - \\overline{f} )^2\\), it follows that the variance of our multivariable function is given by:\n\\[\n\\begin{aligned}\n\\sigma^2(f) &= \\left(\\sum_{i=1}^n (u_i-\\overline{u_i})\\frac{\\partial f}{\\partial u_i}\\right)^2\\\\\n         &= \\sum_{i=1}^n \\left(\\frac{\\partial f}{\\partial u_i}\\right)^2\\sigma_{u_i}^2 + 2\\sum_{i\\ne j}\\frac{\\partial f}{\\partial u_i}\\frac{\\partial f}{\\partial u_j}\\sigma_{u_iu_j}\\\\\n\\end{aligned}\n\\] where we have replaced \\((u_i-\\overline{u_i})^2\\) by the variance \\(\\sigma_{u_i}^2\\) and \\((u_i-\\overline{u_i})(u_j-\\overline{u_j})\\) by the covariance \\(\\sigma_{u_iu_j}\\).\nIf the variables \\(u_i\\) are independent then the covariance \\(\\sigma_{u_iu_j}\\) is null, and it follows the general expression of the standard error that can be applied to any function of independent variables:\n\n\n\n\n\n\n\\[\n\\sigma(f) = \\sqrt{\\sum_{i=1}^n \\left(\\frac{\\partial f}{\\partial u_i}\\right)^2\\sigma_{u_i}^2}\n\\tag{2.7}\\]\n\n\n\n\n2.4.2.1 Functions of one variable\nLet us consider a function \\(f\\) having a form that depends only on one observable \\(x\\), for example:\n\\[\nf = Ax + B\n\\]\nThen, following Equation 2.7, the standard error on that function is given by:\n\\[\n\\begin{aligned}\n\\sigma_f &= \\sqrt{\\left(\\frac{\\partial f}{\\partial x}\\right)^2\\sigma_x^2}\\\\\n         &= A\\sigma_x\n\\end{aligned}\n\\]\nSo, independently of any offset of the measured observable, the resulting error must be corrected by the same factor as the intensity.\n\n\n\n\n\n\nImportant\n\n\n\nIn practice, let’s say we measure a Raman spectrum. As we saw above, the error on each intensity count is given by the square root of this intensity count.\n\nIt is possible to shift vertically this spectrum without having to recompute the error bars.\nBut if you want to normalize (say, to 1) this spectrum, you have to multiply all the errors by the renormalization constant.\n\n\n\n\n2.4.2.2 Functions of two variables\nNow consider the function \\(f = Ax + By\\), where we have measured the mean and standard deviation of both \\(x\\) and \\(y\\), and want to compute the standard deviation on their sum/subtraction. We can use the general formula of Equation 2.7 to determine how to do this, hence:\n\\[\n\\begin{aligned}\n\\sigma_f &= \\sqrt{\\left(\\frac{\\partial f}{\\partial x}\\right)^2\\sigma_x^2 + \\left(\\frac{\\partial f}{\\partial y}\\right)^2\\sigma_y^2}\\\\\n         &= \\sqrt{A^2\\sigma_x^2 + B^2\\sigma_y^2}\n\\end{aligned}\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nIn practice, let’s say we measure an UV spectrum of a solution (a molecule in a solvent), and a reference spectrum of this solvent. As we saw above, the error on each intensity count is given by the square root of this intensity count. We want to subtract the signal of the solvent to get only the signal of the molecule.\nWe thus have to perform the above operation on the errors, \\(\\sigma_{result}=\\sqrt{\\sigma^2_{solution}+\\sigma^2_{reference}}\\). It means that in order to have a statistically sound resulting spectrum, the reference needs to be measured with a very good statistics in order to not dominate the resulting error.\n\n\nIt is a good thing to think about error propagation and where it comes from… but you don’t have to bother computing it by hand, as packages are here to do it for you, as we will see later in the class."
  },
  {
    "objectID": "02-stats.html#further-reading",
    "href": "02-stats.html#further-reading",
    "title": "\n2  A little reminder on Statistics\n",
    "section": "\n2.5 Further reading",
    "text": "2.5 Further reading\n\nA. Bevan, Statistical Data Analysis for the Physical Sciences, Cambridge University Press (2013)\nG. Bohm, Introduction to Statistics and Data Analysis for Physicists, Hamburg: Verl. Dt. Elektronen-Synchrotron (2010).\nJ. Watkins, An Introduction to the Science of Statistics"
  },
  {
    "objectID": "03-getting_ready.html#the-easy-way",
    "href": "03-getting_ready.html#the-easy-way",
    "title": "\n3  Getting ready\n",
    "section": "\n3.1 The easy way",
    "text": "3.1 The easy way\n\n\nDownload and install R\n\nDownload and install Rstudio\n\nWindows only: Download and install Rtools\nYou’re good to go."
  },
  {
    "objectID": "03-getting_ready.html#the-more-advanced-way",
    "href": "03-getting_ready.html#the-more-advanced-way",
    "title": "\n3  Getting ready\n",
    "section": "\n3.2 The more advanced way",
    "text": "3.2 The more advanced way\nIf you don’t want to use Rstudio but rather want to keep with your favorite text editor, like I do (Visual Studio Code or Sublime Text, see configuration below)\n\nI still recommend downloading and installing R via CRAN (I had some packages problems due to a homebrew installation on Mac).\nTo be fully operational with Rmarkdown files without using Rstudio, you need to install pandoc.\n\n\n\n\n\n\n\nConfiguring VS Code\n\n\n\n\n\nI personally use VS Code.\n\nFirst, install the radian console.\nInstall the language server protocol package in R with: install.packages(\"languageserver\")\n\nIn VS Code, install the following extensions:\n\nVSCode R\nR LSP Client\n\n\nEnable r.bracketedPaste for using Radian\nSet up r.rterm.windows, r.rterm.mac or r.rterm.linux: Path to Radian (where you installed radian)\nYou should be good to go: ⌘+⏎ will send the current line/selection to the radian console and ⌘+Shift+K will render the current Rmd file.\n\nHere is the relevant part of my settings.json file:\n{\n    \"r.rterm.mac\": \"/usr/local/bin/radian\",\n    \"r.rpath.mac\": \"/usr/local/bin/R\",\n    \"r.bracketedPaste\": true,\n    \"r.lsp.diagnostics\": false,\n    \"r.sessionWatcher\": true,\n    \"r.rmarkdown.knit.useBackgroundProcess\": false,\n    \"editor.guides.bracketPairs\": true\n}\nAlso, I recommend turning on the session watcher (\"r.sessionWatcher\": true), and then adding the following code to your .Rprofile. This way, the help, tables and figures can be viewed in the VS Code browser panel.\noptions(vsc.browser = \"Beside\")\noptions(vsc.viewer = \"Beside\")\noptions(vsc.page_viewer = \"Beside\")\noptions(vsc.view = \"Beside\")\noptions(vsc.helpPanel = \"Beside\")\n\nif (interactive() && Sys.getenv(\"TERM_PROGRAM\") == \"vscode\") {\n    if (\"httpgd\" %in% .packages(all.available = TRUE)) {\n        options(vsc.plot = FALSE)\n        options(device = function(...) {\n            httpgd::hgd(silent = TRUE)\n            .vsc.browser(httpgd::hgd_url(), viewer = \"Beside\")\n        })\n    }\n}\nHere are also a few keybindings that I use (keybindings.json):\n[\n    {\n        \"description\": \"Create R terminal\",\n        \"key\": \"alt+cmd+r\",\n        \"command\": \"r.createRTerm\"\n    },\n    {\n        \"description\": \"Insert code block\",\n        \"key\": \"cmd+shift+i\",\n        \"command\": \"editor.action.insertSnippet\",\n        \"when\": \"editorTextFocus && editorLangId == 'rmd'\",\n        \"args\": {\n            \"snippet\": \"``` {r}\\n$0\\n```\"\n            }\n    },\n    {\n        \"description\": \"Setwd to current file path\",\n        \"key\": \"cmd+\\\\\",\n        \"command\": \"r.runCommandWithEditorPath\",\n        \"when\": \"editorTextFocus && editorLangId =~ /r|rmd/\",\n        \"args\": \"setwd(dirname(\\\"$$\\\"))\"\n    },\n    {\n        \"description\": \"Insert R arrow\",\n        \"key\": \"ctrl+,\",\n        \"command\": \"editor.action.insertSnippet\",\n        \"when\": \"editorTextFocus && editorLangId =~ /r|rmd/\",\n        \"args\": {\n            \"snippet\": \" <- \"\n            }\n    },\n    {\n        \"description\": \"Insert pipe\",\n        \"key\": \"ctrl+.\",\n        \"command\": \"editor.action.insertSnippet\",\n        \"when\": \"editorTextFocus && editorLangId =~ /r|rmd/\",\n        \"args\": {\n            \"snippet\": \" %>% \"\n         }\n    },\n    {\n        \"description\": \"help document\",\n        \"key\": \"ctrl+h\",\n        \"command\": \"r.runCommandWithSelectionOrWord\",\n        \"when\": \"editorTextFocus && editorLangId =~ /r|rmd/\",\n        \"args\": \"help($$)\"\n    },\n    {\n        \"description\": \"view table\",\n        \"key\": \"cmd+shift+u\",\n        \"command\": \"r.runCommandWithSelectionOrWord\",\n        \"when\": \"editorTextFocus && editorLangId =~ /r|rmd/\",\n        \"args\": \"DT::datatable($$)\"\n    },\n    {\n        \"description\": \"reopen figure panel if closed\",\n        \"key\": \"ctrl+alt+p\",\n        \"command\": \"r.runCommand\",\n        \"when\": \"editorTextFocus && editorLangId =~ /r|rmd/\",\n        \"args\": \".vsc.browser(httpgd::hgd_url(), viewer = \\\"Beside\\\")\"\n    },\n    {\n        \"description\": \"view object\",\n        \"key\": \"cmd+u\",\n        \"command\": \"r.runCommandWithSelectionOrWord\",\n        \"when\": \"editorTextFocus && editorLangId =~ /r|rmd/\",\n        \"args\": \"View($$)\"\n    }\n]\n\n\n\n\n\n\n\n\n\n\nConfiguring Sublime Text\n\n\n\n\n\nI have personally used Sublime Text 3 for a long time, but switched to VS Code recently. Here is what I used to do:\n\nFirst, install Package control.\nTo set up command line launch: ln -s \"/Applications/Sublime Text.app/Contents/SharedSupport/bin/subl\" /usr/local/bin/sublime.\nInstall a minima the packages LSP, R-IDE, Terminus, and SendCode, and also the radian console.\nYou can also add the useful packages LatexTools, BracketHighlighter, RainbowBrackets, Citer, Path Tools, SidebarEnhancements, SidebarTools, Git, GitGutter, Alignment, AutoFileName.\nInstall the language server protocol package in R with: install.packages(\"languageserver\")\n\nTo create a keyboard shortcut to open a terminal with the radian console as the R interpreter, add this to your keybinding file:\n\n{ \"keys\": [\"super+option+r\"], # put whatever you want here\n    \"command\": \"terminus_open\",\n    \"args\": {\n        \"post_window_hooks\": [\n            [\"carry_file_to_pane\", {\"direction\": \"right\"}]\n        ],\n        \"cmd\" : \"radian\"\n    }\n}\n\nSelect Terminus as the destination of SendCode\nYou should be good to go: ⌘+⏎ will send the current line/selection to the radian console, ⌘+B will render the current Rmd file, ⌘+\\ will set the working directory to the current file’s folder."
  },
  {
    "objectID": "03-getting_ready.html#in-any-case-install-latex",
    "href": "03-getting_ready.html#in-any-case-install-latex",
    "title": "\n3  Getting ready\n",
    "section": "\n3.3 In any case: install LaTeX",
    "text": "3.3 In any case: install LaTeX\nA full \\(\\LaTeX\\) distribution (emphasis on full) will be needed to knit markdown files to PDFs (you don’t need it to output html files though):\n\n\nWindows: go here and download the Net Installer to install the complete distribution\n\n\nMac: go here or type brew cask install mactex in the terminal if you have Homebrew installed\n\nLinux: here fore example\n\nAlternatively, you can also work with TinyTeX that will install the needed packages on the fly. It is recommended on the knitr package help, but I only had problems with this, so I recommend the full \\(\\LaTeX\\) distribution option if you don’t mind taking up a few gigabytes of your disk. If you do, just run this in the R console:\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\nTo uninstall TinyTeX, run:\ntinytex::uninstall_tinytex()"
  },
  {
    "objectID": "03-getting_ready.html#working-in-rstudio",
    "href": "03-getting_ready.html#working-in-rstudio",
    "title": "\n3  Getting ready\n",
    "section": "\n3.4 Working in Rstudio",
    "text": "3.4 Working in Rstudio\n\n\n\n\n\n\nRemember\n\n\n\nAlways Work with Projects!!\n\n\nLaunch Rstudio, click File > New Project, and follow the dialog box (existing directory or not, etc.). When you have several ongoing projects, you can switch between them using the Project navigator (see Figure 3.1).\n\n\n\n\nFigure 3.1: The Rstudio interface.\n\n\n\n\nThe great interest of working with Projects is that the default working directory will be relative to the root directory of the Project. In other words, if you store your data in a Data folder, you can read it by running read_function(\"Data/your_file.txt\"). This is one of the fundamentals of reproducible data treatment as:\n\nYou won’t have to write the absolute path towards your file, like read_function(\"C://path/to/your_file.txt\"), path that is unlikely to be the same depending on the computer it is located on. This helps you share your whole project folder with others, or just move it around on your computer and still have a working code.\nYou won’t have to start your script by a setwd(\"/path/to/your/data\") command (for set working directory), which is much better for the same reasons as above.\n\nWrite whatever you want in the “Source code” panel and save it in a .R (or .Rmd) file, and run it by selecting it and hitting ⌘+⏎ (Ctrl+⏎ on Windows, Linux). If no text is selected, hitting ⌘+⏎ will launch the current line. You can see the file contents of your project in the project’s file explorer (see bottom right corner of Figure 3.1).\nThe code output will be seen in the “R Console” panel if it’s a text, or in the “Graph” panel if it’s a graph. A list of all defined variables and functions is available in the “Environment” panel. You can also directly write and run code in the “R Console” panel, if it’s code you don’t care to save in a script (like installing a package or whatever).\nYou can install packages by running the install.packages(\"package_name\") command in the R console or R script, or you can also click on the “Packages” tab in the bottom right corner, and then “Install” or “Update” in case you want to install or update your packages. “Update” will show you a list of installed packages that have a new published version. All verified packages are located on the CRAN (Comprehensive R Archive Network). It is thus really easy to install packages and maintain (update) your packages in R. There is also the possibility to install packages from source if you want to install custom packages – “homemade” packages that didn’t go through the CRAN verification process: do so at your own risks.\nMore on the Rstudio cheatsheet."
  },
  {
    "objectID": "03-getting_ready.html#setting-up-the-environment",
    "href": "03-getting_ready.html#setting-up-the-environment",
    "title": "\n3  Getting ready\n",
    "section": "\n3.5 Setting up the environment",
    "text": "3.5 Setting up the environment\nMake sure you have the following packages installed by launching the following commands: copy-paste them in the “Source code” panel (upper left after having crated a new R script), select all the lines and hit Ctrl+⏎ (Windows, Linux) or ⌘+⏎ (Mac). These are the main packages that we will use in this class:\n# Necessary for the exercises\ninstall.packages(\"tidyverse\")\ninstall.packages(\"patchwork\")\ninstall.packages(\"broom\")\n# Recommended for smooth working and some exercises\ninstall.packages(\"devtools\")\ninstall.packages(\"rmarkdown\")\ninstall.packages(\"knitr\")\ninstall.packages(\"shiny\")\ninstall.packages(\"plotly\")\ninstall.packages(\"tikzDevice\")\ninstall.packages(\"quantities\")\ninstall.packages(\"ggforce\")\nLater on, a package can be loaded by calling:\nlibrary(package_name)\nor by checking it in the “Graphs” panel under the “Packages” tab. If you want to access a function from a given package without loading it (or because several packages define the same function and you want to specify which one to use), type:\npackage_name::function_name(parameters)\nIf you want to access the documentation on a given package, click the link on this package in the “Packages” tab. In a more general way, help on a function is accessed by typing in ?function_name, the help appearing in the “Graph” panel."
  },
  {
    "objectID": "03-getting_ready.html#the-tutor-package",
    "href": "03-getting_ready.html#the-tutor-package",
    "title": "\n3  Getting ready\n",
    "section": "\n3.6 The tutor package",
    "text": "3.6 The tutor package\nFinally, I also made some interactive exercises that are bundled in the tutor package. I let you go there and follow the instructions for installing it – you have to get used to going to GitHub to install packages and find information on them.\nAlternatively, you can just download the archive with all the exercises files, unzip it in your R class RStudio project, and edit the R files."
  },
  {
    "objectID": "05-variables.html#scalars-and-booleans",
    "href": "05-variables.html#scalars-and-booleans",
    "title": "\n4  Variables, booleans and strings\n",
    "section": "\n4.1 Scalars and booleans",
    "text": "4.1 Scalars and booleans\nDefining a scalar value is done with:\n\nx <- 1 # attribute the value '1' to the variable 'x'\nx      # print the value of 'x'\n\n#> [1] 1\n\n\nYou can use the R console as an advanced calculator:\n\n1 + 3\n\n#> [1] 4\n\nx <- 1/2\ny <- exp(log(sin(cos(x*pi))))\nx - y\n\n#> [1] 0.5\n\n\n\n\n\n\n\n\nAttention\n\n\n\nTyping x + 1 will only print the result of x + 1, but not add 1 to x and save this as the new value of x.\nTo actually modify the value stored in x, type x <- x + 1.\n\n\n\n4.1.1 Special values\nR handles infinity, NaN (Not a Number), and has \\(\\pi\\) defined. Missing numbers are handled through the NA keyword (Not Attributed).\n\npi\n\n#> [1] 3.141593\n\n10/0\n\n#> [1] Inf\n\n0/0\n\n#> [1] NaN\n\nNA\n\n#> [1] NA\n\n\n\n4.1.2 Booleans and common tests on values\nBooleans are handled with the TRUE and FALSE keywords, and are usually obtained as the result of a test on values.\nOnce you’ve understood these tests, you can use them in conditional actions of the type if(test){then}else{then}, or to apply some filter on your data.\nLet’s create two variables x and y and test some assertions on these two variables (i.e. answer the following questions):\n\nx <- 1\ny <- 2\n\n\nIs x equal to y?\n\n\nx == y\n\n#> [1] FALSE\n\n\n\nIs x not equal to y?\n\n\nx != y\n\n#> [1] TRUE\n\n\n\nIs x smaller than y?\n\n\nx < y \n\n#> [1] TRUE\n\n\n\nIs x smaller or equal than y?\n\n\nx <= y\n\n#> [1] TRUE\n\n\n\nOperator “and”: &\n\n\n\nx == y & x < y \n\n#> [1] FALSE\n\n\n\nOperator “or”: |\n\n\n\nx == y | x < y \n\n#> [1] TRUE\n\n\n\nIs x a NaN?\n\n\nis.nan(x)\n\n#> [1] FALSE\n\n\n\nIs x a number?\n\n\nis.numeric(x)\n\n#> [1] TRUE\n\n\n\nIs x a string?\n\n\nis.character(x)\n\n#> [1] FALSE\n\n\n\nIs x a NA?\n\n\nis.na(x)\n\n#> [1] FALSE\n\n\n\nOperator “not” (inverse)\n\n\n!is.na(x)\n\n#> [1] TRUE\n\n\nA number can by converted to a boolean using as.logical(): any number non 0 is equivalent to TRUE, and 0 is FALSE.\n\nas.logical(1)\n\n#> [1] TRUE\n\nas.logical(0)\n\n#> [1] FALSE"
  },
  {
    "objectID": "05-variables.html#complex-values",
    "href": "05-variables.html#complex-values",
    "title": "\n4  Variables, booleans and strings\n",
    "section": "\n4.2 Complex values",
    "text": "4.2 Complex values\n\n\n\n\n\n\nClick to expand\n\n\n\n\n\nR also natively handles complex values:\n\n1+i   # not a valid complex value\n\n#> Error in eval(expr, envir, enclos): object 'i' not found\n\n1+1i  # valid complex value\n\n#> [1] 1+1i\n\nexp(1i*pi)\n\n#> [1] -1+0i\n\nsqrt(-1)\n\n#> [1] NaN\n\nsqrt(-1 + 0i)\n\n#> [1] 0+1i\n\nIm(exp(1i*pi))\n\n#> [1] 1.224647e-16\n\nRe(exp(1i*pi))\n\n#> [1] -1\n\nMod(exp(1i*pi))\n\n#> [1] 1"
  },
  {
    "objectID": "05-variables.html#strings",
    "href": "05-variables.html#strings",
    "title": "\n4  Variables, booleans and strings\n",
    "section": "\n4.3 Strings",
    "text": "4.3 Strings\nA string is defined between quotation marks such as: \"string\". Thus \"1\" is not the number 1, but rather the character “1”. Here are some operations on strings :\n\nDefinition of a string:\n\n\nphrase <- \"Hello World \" \n\n\nConcatenation of strings using paste():\n\n\npaste(\"phrase\", phrase, sep=\" = \")\n\n#> [1] \"phrase = Hello World \"\n\n\n\nConcatenation of strings using glue::glue():\n\n\nlibrary(glue)\nglue(\"phrase = {phrase}\")\n\n#> phrase = Hello World\n\n\n\nAcessing a sub-string:\n\n\nsubstr(phrase, 1, 4)\n\n#> [1] \"Hell\"\n\n\n\nChange case of string with tolower() and toupper():\n\n\ntolower(phrase); toupper(phrase)\n\n#> [1] \"hello world \"\n\n\n#> [1] \"HELLO WORLD \"\n\n\n\nChange the first occurrence of “o” in “a” with sub():\n\n\nsub(\"o\", \"a\", phrase)\n\n#> [1] \"Hella World \"\n\n\n\nChange all occurrences of “o” in “a” with gsub():\n\n\ngsub(\"o\", \"a\", phrase)\n\n#> [1] \"Hella Warld \"\n\n\n\nTrim white spaces with trimws():\n\n\ntrimws(phrase)\n\n#> [1] \"Hello World\"\n\n\n\nGet a vector from string separation based on a character with strsplit():\n\n\nstrsplit(phrase, \" \")         # returns a list\n\n#> [[1]]\n#> [1] \"Hello\" \"World\"\n\nunlist(strsplit(phrase, \" \")) # returns a vector\n\n#> [1] \"Hello\" \"World\"\n\n\n\n\n\n\n\n\nAttention!!\n\n\n\n\nphrase2 <- \"1234\"\nphrase2 - 4321             # won't work: string - double\n\n#> Error in phrase2 - 4321: non-numeric argument to binary operator\n\nas.numeric(phrase2) - 4321 # conversion of string to double\n\n#> [1] -3087\n\n\n\n\nFor more complex operations, see the stringr package and its cheatsheet."
  },
  {
    "objectID": "05-variables.html#exo-variables",
    "href": "05-variables.html#exo-variables",
    "title": "\n4  Variables, booleans and strings\n",
    "section": "\n4.4 Exercises",
    "text": "4.4 Exercises\nInteractive exercises can be found in the tutor package. For this, simply run:\nlibrary(tutor)\ntuto(\"variables\")\nAlternatively, you can just download the archive with all the exercises files, unzip it in your R class RStudio project, and edit the R files."
  },
  {
    "objectID": "06-vectors.html#different-ways-of-defining-a-vector",
    "href": "06-vectors.html#different-ways-of-defining-a-vector",
    "title": "\n5  Vectors\n",
    "section": "\n5.1 Different ways of defining a vector",
    "text": "5.1 Different ways of defining a vector\nIf you want to define a vector by specifying the values, use the function c(), like so:\n\nHere, x is a vector of doubles:\n\n\nx <- c(1, 5, 3, 12, 4.2)\nx\n\n#> [1]  1.0  5.0  3.0 12.0  4.2\n\n\n\nBut here, x is converted to a vector of strings because it contains a string:\n\n\nx <- c(1, 5, 3, \"hello\")\nx\n\n#> [1] \"1\"     \"5\"     \"3\"     \"hello\"\n\n\n\nTo define a sequence of increasing numbers, either use the notation start:end for a sequence going from start to end by step of 1, or use the seq() function that is more versatile:\n\n\n1:10\n\n#>  [1]  1  2  3  4  5  6  7  8  9 10\n\nseq(-10, 10, by = .5)\n\n#>  [1] -10.0  -9.5  -9.0  -8.5  -8.0  -7.5  -7.0  -6.5  -6.0  -5.5  -5.0  -4.5\n#> [13]  -4.0  -3.5  -3.0  -2.5  -2.0  -1.5  -1.0  -0.5   0.0   0.5   1.0   1.5\n#> [25]   2.0   2.5   3.0   3.5   4.0   4.5   5.0   5.5   6.0   6.5   7.0   7.5\n#> [37]   8.0   8.5   9.0   9.5  10.0\n\nseq(-10, 10, length = 6)\n\n#> [1] -10  -6  -2   2   6  10\n\nseq(-10, 10, along = x)\n\n#> [1] -10.000000  -3.333333   3.333333  10.000000\n\n\n\nTo repeat values, use the rep() function:\n\n\nrep(0, 10)\n\n#>  [1] 0 0 0 0 0 0 0 0 0 0\n\nrep(c(0, 2), 5)\n\n#>  [1] 0 2 0 2 0 2 0 2 0 2\n\nrep(c(0, 2), each = 5)\n\n#>  [1] 0 0 0 0 0 2 2 2 2 2\n\n\n\nTo create vectors of random numbers, use rnorm() or runif() for normally or uniformly distributed numbers, respectively:\n\n\n# vector with 10 random values normally distributed around mean \n# with given standard deviation `sd`\nrnorm(10, mean=3, sd=1)\n\n#>  [1] 2.363420 2.761449 2.460023 4.199140 1.736984 3.536408 3.159021 4.677559\n#>  [9] 3.323875 3.667504\n\n# vector with 10 random values uniformly distributed between min and max\nrunif(10, min = 0, max = 1)\n\n#>  [1] 0.1579355 0.5273384 0.2034804 0.8531060 0.6262420 0.3668108 0.6957176\n#>  [8] 0.1347311 0.9240781 0.8583217"
  },
  {
    "objectID": "06-vectors.html#numerical-and-categorical-data-types",
    "href": "06-vectors.html#numerical-and-categorical-data-types",
    "title": "\n5  Vectors\n",
    "section": "\n5.2 Numerical and categorical data types",
    "text": "5.2 Numerical and categorical data types\nData can be of two different types: numerical, or categorical. Let’s say you are measuring a the temperature in a room and recording its value over time:\n\nT1 <- c(22.3, 23.5, 26.0, 30.2)\n\nT1 is a vector containing numerical data.\nLet’s say that now you are recording the temperature level, which can be low, high or medium\n\nT2 <- c(\"low\", \"low\", \"medium\", \"high\")\n\nT2 is a vector containing categorical data, i.e. the data in this example can fall into either of 3 categories. For now, T2 is however a vector of strings, and we need to tell R that it contains categorical data by using the function factor():\n\nT2 <- factor(T2)\nT2\n\n#> [1] low    low    medium high  \n#> Levels: high low medium\n\n\nWe see here that we now have 3 levels, and a numerization of T2 leads to obtaining the numbers 1, 2 and 3 according to the levels in T2:\n\nas.numeric(T2)\n\n#> [1] 2 2 3 1\n\n\nNumerical data can be converted to factors in the same way – this can be useful sometimes, e.g. when plotting with ggplot as we will see later:\n\nfactor(T1)\n\n#> [1] 22.3 23.5 26   30.2\n#> Levels: 22.3 23.5 26 30.2"
  },
  {
    "objectID": "06-vectors.html#principal-operations-on-vectors",
    "href": "06-vectors.html#principal-operations-on-vectors",
    "title": "\n5  Vectors\n",
    "section": "\n5.3 Principal operations on vectors",
    "text": "5.3 Principal operations on vectors\n\n5.3.1 Mathematical operations\nBecause R is a vectorized language, you don’t need to loop on all elements of a vector to perform element-wise operations on it. Let’s say that x <- 1:6, then:\n\nAddition of a value to all elements:\n\n\nx + 2.5\n\n#> [1] 3.5 4.5 5.5 6.5 7.5 8.5\n\n\n\nMultiplication / division of all elements:\n\n\nx*2\n\n#> [1]  2  4  6  8 10 12\n\n\n\nInteger division:\n\n\nx %/% 3\n\n#> [1] 0 0 1 1 1 2\n\n\n\nMath functions apply on all elements:\n\n\nsqrt(abs(cos(x)))\n\n#> [1] 0.7350526 0.6450944 0.9949837 0.8084823 0.5325995 0.9798828\n\n\n\nPower:\n\n\nx^2.5\n\n#> [1]  1.000000  5.656854 15.588457 32.000000 55.901699 88.181631\n\n\n\nMultiplication of vectors of the same size is performed element by element:\n\n\ny <- c(2.3, 5, 7, 10, 12, 20)\nx*y\n\n#> [1]   2.3  10.0  21.0  40.0  60.0 120.0\n\n\n\nMultiplication of vectors of different sizes: the smaller vector is automatically repeated the number of times needed to get a vector of the size of the larger one. It will work also if the longer object length is not a multiple of shorter object length, but the shorter object will be truncated and you’ll get an error:\n\n\nx*1:2\n\n#> [1]  1  4  3  8  5 12\n\nx*1:4\n\n#> [1]  1  4  9 16  5 12\n\n\n\nModulo:\n\n\nx %% 2\n\n#> [1] 1 0 1 0 1 0\n\n\n\nOuter product of vectors (the result is a matrix):\n\n\nx %o% y\n\n#>      [,1] [,2] [,3] [,4] [,5] [,6]\n#> [1,]  2.3    5    7   10   12   20\n#> [2,]  4.6   10   14   20   24   40\n#> [3,]  6.9   15   21   30   36   60\n#> [4,]  9.2   20   28   40   48   80\n#> [5,] 11.5   25   35   50   60  100\n#> [6,] 13.8   30   42   60   72  120\n\n\n\n5.3.2 Accessing values\nLet’s work on this vector x:\n\nx <- c(5, 3, 4, 9, 3)\n\n\n5.3.2.1 Accessing values by index\n\nTo access values of a vector x, use the x[i] notation, where i is the index you want to access. i can be (and in fact, is always) a vector.\n\n\n\n\n\n\n\nAttention\n\n\n\nIn R, indexes numbering start at 1 !!!\n\n\n\n# accessing by indexes\nx[3]\n\n#> [1] 4\n\n# access index 1, 5 and 2\nx[c(1,5,2)] \n\n#> [1] 5 3 3\n\n\n\nTo remove elements j from a vector x, use the notation x[-j]:\n\n\n# remove elements 1 and 3\nx[-c(1,3)]\n\n#> [1] 3 9 3\n\n\n\n\n\n\n\n\nAttention\n\n\n\nWriting x[-c(1,3)] will just print the result of x[-c(1,3)], but not actually modify x.\nTo really modify x, you’d need to write x <- x[-c(1,3)].\n\n\n\n5.3.2.2 Filtering values with tests\nYou can access values with booleans. Values getting a TRUE will be kept, while values with a FALSE will be discarded – or return a NA if a TRUE is given to a non existing value (i.e. to an index larger than the size of the vector):\n\nx\n\n#> [1] 5 3 4 9 3\n\nx[c(TRUE,TRUE,TRUE,FALSE,TRUE,TRUE)]\n\n#> [1]  5  3  4  3 NA\n\n\nTherefore, you can apply tests on your values and filter them very easily:\n\nx > 4 # is a vector of booleans\n\n#> [1]  TRUE FALSE FALSE  TRUE FALSE\n\nx[x > 4] # is a filtered vector\n\n#> [1] 5 9\n\n\n\n5.3.2.3 Accessing values by name\nFinally, values in vectors can be named, and thus can be accessed by their name:\n\ny <- c(age=32, name=\"John\", pet=\"Cat\")\ny\n\n#>    age   name    pet \n#>   \"32\" \"John\"  \"Cat\"\n\ny[c('age','pet')] # prints a named vector\n\n#>   age   pet \n#>  \"32\" \"Cat\"\n\ny[['name']] # prints an un-named vector\n\n#> [1] \"John\"\n\n\nAnd you can access the names of the vector using names():\n\nnames(y)\n\n#> [1] \"age\"  \"name\" \"pet\"\n\n\n\n5.3.3 Sorting, removing duplicates, sampling\n\nSorting by ascending number:\n\n\nsort(x)\n\n#> [1] 3 3 4 5 9\n\n\n\nIt works with strings too, but stringr::str_sort() might be needed for strings mixing letters and numbers:\n\n\nsort(c(\"c\",\"a\",\"d\",\"ab\")) \n\n#> [1] \"a\"  \"ab\" \"c\"  \"d\"\n\nsort(c(\"c\", \"a10\", \"a2\", \"d\", \"ab\"))\n\n#> [1] \"a10\" \"a2\"  \"ab\"  \"c\"   \"d\"\n\nstringr::str_sort(c(\"c\", \"a10\", \"a2\", \"d\", \"ab\"), numeric = TRUE)\n\n#> [1] \"a2\"  \"a10\" \"ab\"  \"c\"   \"d\"\n\n\n\nSorting by descending number:\n\n\nsort(x, decreasing = TRUE) \n\n#> [1] 9 5 4 3 3\n\n\n\nInverting the order of the vector:\n\n\nrev(x)\n\n#> [1] 3 9 4 3 5\n\n\n\nFind the order of the indexes of the sorting:\n\n\norder(x)\n\n#> [1] 2 5 3 1 4\n\nx[order(x)] # is thus equivalent to `sort(x)`\n\n#> [1] 3 3 4 5 9\n\n\n\nFind duplicates:\n\n\nduplicated(x)\n\n#> [1] FALSE FALSE FALSE FALSE  TRUE\n\n\n\nRemove duplicates:\n\n\nunique(x)\n\n#> [1] 5 3 4 9\n\n\n\nChoose 3 random values:\n\n\nsample(x, 3)\n\n#> [1] 9 3 3\n\n\n\n5.3.4 Maximum and minimum\nThis is quite straightforward:\n\n# maximum of x and its index\nx; max(x); which.max(x) \n\n#> [1] 5 3 4 9 3\n\n\n#> [1] 9\n\n\n#> [1] 4\n\n# minimum of x and its index\nx; min(x); which.min(x)\n\n#> [1] 5 3 4 9 3\n\n\n#> [1] 3\n\n\n#> [1] 2\n\n# range of a vector\nrange(x)\n\n#> [1] 3 9\n\n\n\n5.3.5 Characteristics of vectors\n\nSize of a vector:\n\n\nlength(x)\n\n#> [1] 5\n\n\n\nStatistics on vector:\n\n\nsummary(x)\n\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>     3.0     3.0     4.0     4.8     5.0     9.0\n\n\n\nSum of all terms:\n\n\nsum(x)\n\n#> [1] 24\n\n\n\nAverage value:\n\n\nmean(x)\n\n#> [1] 4.8\n\n\n\nMedian value:\n\n\nmedian(x)\n\n#> [1] 4\n\n\n\nStandard deviation:\n\n\nsd(x)\n\n#> [1] 2.48998\n\n\n\nCount occurrence of values:\n\n\ntable(x)\n\n#> x\n#> 3 4 5 9 \n#> 2 1 1 1\n\n\n\nCumulative sum:\n\n\ncumsum(x)\n\n#> [1]  5  8 12 21 24\n\n\n\nTerm-by-term difference\n\n\ndiff(x)\n\n#> [1] -2  1  5 -6\n\n\n\n5.3.6 Concatenation of vectors\nThis is done using the c() notation: you basically create a vector of vectors, the result being a new vector:\n\n# concatenate vectors\nz <- c(-1:4, NA, -x); z\n\n#>  [1] -1  0  1  2  3  4 NA -5 -3 -4 -9 -3\n\n\nAnother option is to use the append() function that allows for more options:\n\n# append values\nappend(x, 4)    # at the end\n\n#> [1] 5 3 4 9 3 4\n\nappend(x, 4:1, 3) # or after a given index\n\n#> [1] 5 3 4 4 3 2 1 9 3"
  },
  {
    "objectID": "06-vectors.html#exo-vectors",
    "href": "06-vectors.html#exo-vectors",
    "title": "\n5  Vectors\n",
    "section": "\n5.4 Exercises",
    "text": "5.4 Exercises\nInteractive exercises can be found in the tutor package. For this, simply run:\nlibrary(tutor)\ntuto(\"vectors\")\nAlternatively, you can just download the archive with all the exercises files, unzip it in your R class RStudio project, and edit the R files.\n\nExercise 1\nLet’s create a named vector containing age of the students in the class, the names of each value being the first name of the students. Then:\n\nCompute the average age and its standard deviation\nCompute the median age\nWhat is the maximum, minimum and range of the ages in the class?\nWhat are all the student names in the class?\nPrint the sorted ages by increasing and decreasing order\nPrint the ages sorted by alphabetically ordered names (increasing and decreasing)\nShow a histogram of the ages distribution using hist() and play with the parameter breaks to modify the histogram\nShow a boxplot of the ages distribution using boxplot()\n\nExercise 2\nThis exercise is adapted from here.\nOpen Rstudio and create a new R script, save it as population.R in your wanted directory, say Rcourse/.\nDownload the population.csv file and save it in your working directory.\nA csv file contains raw data stored as plain text and separated by a comma (Comma Separated Values). Open it with Rstudio.\nWe can of course directly load such file with R and store its data in an appropriate format (i.e. a data.frame), but this is for the next chapter. For now, just copy-paste the text in the Rstudio script area to:\n\nCreate a cities vector containing all the cities listed in population.csv\n\nCreate a pop_1962 and pop_2012 vectors containing the populations of each city at these years. Print the 2 vectors.\nUse names() to name values of pop_1962 and pop_2012. Print the 2 vectors again. Are there any change?\nWhat are the cities with more than 200000 people in 1962? For these, how many residents in 2012?\nWhat is the population evolution of Montpellier and Nantes?\nCreate a pop_diff vector to store population change between 1962 and 2012\nPrint cities with a negative change\nPrint cities which broke the 300000 people barrier between 1962 and 2012\nCompute the total change in population of the 10 largest cities (as of 1962) between 1962 and 2012.\nCompute the population mean for year 1962\nCompute the population mean of Paris over these two years\nSort the cities by decreasing order of population for 1962\n\n\nSolution\n\n# Create a `cities` vector containing all the cities listed in `population.csv`\ncities <- c(\"Angers\", \"Bordeaux\", \"Brest\", \"Dijon\", \"Grenoble\", \"Le Havre\", \n            \"Le Mans\", \"Lille\", \"Lyon\", \"Marseille\", \"Montpellier\", \"Nantes\", \n            \"Nice\", \"Paris\", \"Reims\", \"Rennes\", \"Saint-Etienne\", \"Strasbourg\", \n            \"Toulon\", \"Toulouse\")\n# Create a `pop_1962` and `pop_2012` vectors containing the populations \n# of each city at these years. Print the 2 vectors. \npop_1962 <- c(115273,278403,136104,135694,156707,187845,132181,239955,\n              535746,778071,118864,240048,292958,2790091,134856,151948,\n              210311,228971,161797,323724)\npop_2012 <- c(149017,241287,139676,152071,158346,173142,143599,228652,\n              496343,852516,268456,291604,343629,2240621,181893,209860,\n              171483,274394,164899,453317)\npop_1962; pop_2012\n\n#>  [1]  115273  278403  136104  135694  156707  187845  132181  239955  535746\n#> [10]  778071  118864  240048  292958 2790091  134856  151948  210311  228971\n#> [19]  161797  323724\n\n\n#>  [1]  149017  241287  139676  152071  158346  173142  143599  228652  496343\n#> [10]  852516  268456  291604  343629 2240621  181893  209860  171483  274394\n#> [19]  164899  453317\n\n# Use names() to name values of `pop_1962` and `pop_2012`. \n# Print the 2 vectors again. Are there any change?\nnames(pop_2012) <- names(pop_1962) <- cities\npop_1962; pop_2012\n\n#>        Angers      Bordeaux         Brest         Dijon      Grenoble \n#>        115273        278403        136104        135694        156707 \n#>      Le Havre       Le Mans         Lille          Lyon     Marseille \n#>        187845        132181        239955        535746        778071 \n#>   Montpellier        Nantes          Nice         Paris         Reims \n#>        118864        240048        292958       2790091        134856 \n#>        Rennes Saint-Etienne    Strasbourg        Toulon      Toulouse \n#>        151948        210311        228971        161797        323724\n\n\n#>        Angers      Bordeaux         Brest         Dijon      Grenoble \n#>        149017        241287        139676        152071        158346 \n#>      Le Havre       Le Mans         Lille          Lyon     Marseille \n#>        173142        143599        228652        496343        852516 \n#>   Montpellier        Nantes          Nice         Paris         Reims \n#>        268456        291604        343629       2240621        181893 \n#>        Rennes Saint-Etienne    Strasbourg        Toulon      Toulouse \n#>        209860        171483        274394        164899        453317\n\n# What are the cities with more than 200000 people in 1962? \n# For these, how many residents in 2012?\ncities200k <- cities[pop_1962>200000]\ncities200k; pop_2012[cities200k]\n\n#>  [1] \"Bordeaux\"      \"Lille\"         \"Lyon\"          \"Marseille\"    \n#>  [5] \"Nantes\"        \"Nice\"          \"Paris\"         \"Saint-Etienne\"\n#>  [9] \"Strasbourg\"    \"Toulouse\"\n\n\n#>      Bordeaux         Lille          Lyon     Marseille        Nantes \n#>        241287        228652        496343        852516        291604 \n#>          Nice         Paris Saint-Etienne    Strasbourg      Toulouse \n#>        343629       2240621        171483        274394        453317\n\n# What is the population evolution of Montpellier and Nantes?\npop_2012['Montpellier'] - pop_1962['Montpellier']; pop_2012['Nantes'] - pop_1962['Nantes']\n\n#> Montpellier \n#>      149592\n\n\n#> Nantes \n#>  51556\n\n# Create a `pop_diff` vector to store population change between 1962 and 2012\npop_diff <- pop_2012 - pop_1962\n# Print cities with a negative change\ncities[pop_diff<0]\n\n#> [1] \"Bordeaux\"      \"Le Havre\"      \"Lille\"         \"Lyon\"         \n#> [5] \"Paris\"         \"Saint-Etienne\"\n\n# Print cities which broke the 300000 people barrier between 1962 and 2012\ncities[pop_2012>300000 & pop_1962<300000]\n\n#> [1] \"Nice\"\n\n# Compute the total change in population of the 10 largest cities\n# (as of 1962) between 1962 and 2012.\nten_largest <- cities[order(pop_1962, decreasing = TRUE)[1:10]]\nsum(pop_2012[ten_largest] - pop_1962[ten_largest])\n\n#> [1] -324432\n\n# Compute the population mean for year 1962\nmean(pop_1962)\n\n#> [1] 367477.3\n\n# Compute the population mean of Paris\nmean(c(pop_1962['Paris'], pop_2012['Paris']))\n\n#> [1] 2515356\n\n# Sort the cities by decreasing order of population for 1962\n(pop_1962_sorted <- sort(pop_1962, decreasing = TRUE))\n\n#>         Paris     Marseille          Lyon      Toulouse          Nice \n#>       2790091        778071        535746        323724        292958 \n#>      Bordeaux        Nantes         Lille    Strasbourg Saint-Etienne \n#>        278403        240048        239955        228971        210311 \n#>      Le Havre        Toulon      Grenoble        Rennes         Brest \n#>        187845        161797        156707        151948        136104 \n#>         Dijon         Reims       Le Mans   Montpellier        Angers \n#>        135694        134856        132181        118864        115273"
  },
  {
    "objectID": "07-dataframes.html#defining-a-data.frame",
    "href": "07-dataframes.html#defining-a-data.frame",
    "title": "\n6  Data frames\n",
    "section": "\n6.1 Defining a data.frame",
    "text": "6.1 Defining a data.frame\n\n6.1.1 Defining a data.frame from vectors\nIn R, the principal object is the data. Hence the data.frame object, which is basically a table of vectors. A data.frame is a list presented under the form of a table – i.e. a spreadsheet. On a day-to-day basis, you will either define data.frame from existing vectors or other data.frame, or define a data.frame from a file (text, Excel…). In this example, we use test.dat and test.xlsx.\nTo define a data.frame from known vectors, we just have to do:\n\nx  <- seq(-pi, pi, length = 6) \ny  <- sin(x)\ndf <- data.frame(x, y) # df is a data.frame (a table)\ndf\n\n#>            x             y\n#> 1 -3.1415927 -1.224647e-16\n#> 2 -1.8849556 -9.510565e-01\n#> 3 -0.6283185 -5.877853e-01\n#> 4  0.6283185  5.877853e-01\n#> 5  1.8849556  9.510565e-01\n#> 6  3.1415927  1.224647e-16\n\n\nThen some information about our table are readily accessible, like:\n\nThe dimension, the number of rows and columns:\n\n\ndim(df); nrow(df); ncol(df)\n\n#> [1] 6 2\n\n\n#> [1] 6\n\n\n#> [1] 2\n\n\n\nPrint the first and last 3 values\n\n\nhead(df, 3); tail(df, 3)\n\n#>            x             y\n#> 1 -3.1415927 -1.224647e-16\n#> 2 -1.8849556 -9.510565e-01\n#> 3 -0.6283185 -5.877853e-01\n\n\n#>           x            y\n#> 4 0.6283185 5.877853e-01\n#> 5 1.8849556 9.510565e-01\n#> 6 3.1415927 1.224647e-16\n\n\n\nPrint some information on df\n\n\n\nstr(df)\n\n#> 'data.frame':    6 obs. of  2 variables:\n#>  $ x: num  -3.142 -1.885 -0.628 0.628 1.885 ...\n#>  $ y: num  -1.22e-16 -9.51e-01 -5.88e-01 5.88e-01 9.51e-01 ...\n\n\n\nPrint some statistics on df\n\n\n\nsummary(df)\n\n#>        x                y          \n#>  Min.   :-3.142   Min.   :-0.9511  \n#>  1st Qu.:-1.571   1st Qu.:-0.4408  \n#>  Median : 0.000   Median : 0.0000  \n#>  Mean   : 0.000   Mean   : 0.0000  \n#>  3rd Qu.: 1.571   3rd Qu.: 0.4408  \n#>  Max.   : 3.142   Max.   : 0.9511\n\n\nIf not defined when creating the data.frame, the column names will be by default the vector names. To specify your own column names, do it when creating the data.frame:\n\ndf <- data.frame(xxx = x, yyy = y)\nhead(df, 2)\n\n#>         xxx           yyy\n#> 1 -3.141593 -1.224647e-16\n#> 2 -1.884956 -9.510565e-01\n\n\nOr, once it’s created, do it using names()\n\nnames(df)\n\n#> [1] \"xxx\" \"yyy\"\n\nnames(df) <- c(\"X\", \"Y\")\nhead(df, 2)\n\n#>           X             Y\n#> 1 -3.141593 -1.224647e-16\n#> 2 -1.884956 -9.510565e-01\n\n\n\n6.1.2 Defining a data.frame from a file\n\n6.1.2.1 A text file\nLet’s say we have test.dat that looks like this:\n\n# Bash code:\nhead Data/test.dat\n\n#> x   y\n#> 1   2\n#> 2   3\n\n\n\nThen, to read this file into a data.frame, we will use read.table(). If you don’t specify that the file contains a header, read.table() will default to attributing column names that will be V1, V2, V3, etc:\n\n\nread.table(\"Data/test.dat\")\n\n#>   V1 V2\n#> 1  x  y\n#> 2  1  2\n#> 3  2  3\n\n\n\nIf your file contains column names, you can use the first line as column names, like so:\n\n\nread.table(\"Data/test.dat\", header=TRUE)\n\n#>   x y\n#> 1 1 2\n#> 2 2 3\n\n\n\nIf you want to skip some lines before starting the reading, use skip:\n\n\nread.table(\"Data/test.dat\", skip=1)\n\n#>   V1 V2\n#> 1  1  2\n#> 2  2  3\n\n\n\nYou can specify your own column names using col.names:\n\n\nread.table(\"Data/test.dat\", skip=1, col.names = c(\"A\",\"B\"))\n\n#>   A B\n#> 1 1 2\n#> 2 2 3\n\n\n\nYou can type ?read.table for more options.\n\n6.1.2.2 An Excel file\nNow, to read an Excel file, use the readxl library:\n\nlibrary(readxl) # load readxl from tidyverse to read Excel files\nread_excel(\"Data/test.xlsx\", sheet=1)\n\n#> # A tibble: 10 × 2\n#>        x      y\n#>    <dbl>  <dbl>\n#>  1     1  5.21 \n#>  2     2  6.55 \n#>  3     3  3.71 \n#>  4     4  0.216\n#>  5     5  0.205\n#>  6     6  4.60 \n#>  7     7 10.3  \n#>  8     8 12.9  \n#>  9     9 11.1  \n#> 10    10  7.28\n\nread_excel(\"Data/test.xlsx\", sheet=2)\n\n#> # A tibble: 4 × 2\n#>   hello  world   \n#>   <chr>  <chr>   \n#> 1 ac     th      \n#> 2 asc    thh     \n#> 3 ascsa  dthdh   \n#> 4 ascacs dthtdhdh"
  },
  {
    "objectID": "07-dataframes.html#accessing-values",
    "href": "07-dataframes.html#accessing-values",
    "title": "\n6  Data frames\n",
    "section": "\n6.2 Accessing values",
    "text": "6.2 Accessing values\n\nLike with vectors, accessing values is done using the [] notation, except that here there are two indexes: df[row, column]:\n\n\ndf[3,1]\n\n#> [1] -0.6283185\n\n\n\nIn general however, what you want is to access a given column, by its index:\n\n\ndf[,1]\n\n#> [1] -3.1415927 -1.8849556 -0.6283185  0.6283185  1.8849556  3.1415927\n\ndf[[1]]# this is a vector too\n\n#> [1] -3.1415927 -1.8849556 -0.6283185  0.6283185  1.8849556  3.1415927\n\n\n\nOr, preferably, by its name using the $ notation:\n\n\ndf$X\n\n#> [1] -3.1415927 -1.8849556 -0.6283185  0.6283185  1.8849556  3.1415927\n\n\n\nFinally, you may want to apply filters on your table:\n\n\ndf[df$X < 0, ]\n\n#>            X             Y\n#> 1 -3.1415927 -1.224647e-16\n#> 2 -1.8849556 -9.510565e-01\n#> 3 -0.6283185 -5.877853e-01\n\n\n\nUsing the function subset(), the conditions are applied on column names (no need for df$col_name here, while you need it in the above expression):\n\n\nsubset(df, X>1)\n\n#>          X            Y\n#> 5 1.884956 9.510565e-01\n#> 6 3.141593 1.224647e-16\n\nsubset(df, X>1, select = c(X))\n\n#>          X\n#> 5 1.884956\n#> 6 3.141593"
  },
  {
    "objectID": "07-dataframes.html#adding-columns-or-rows",
    "href": "07-dataframes.html#adding-columns-or-rows",
    "title": "\n6  Data frames\n",
    "section": "\n6.3 Adding columns or rows",
    "text": "6.3 Adding columns or rows\n\n6.3.1 Adding columns\n\nTo add a column, just attribute a value to an column that do not exist yet, it will be created:\n\n\n# Adding columns\ndf   <- data.frame(x,y)\ndf$z <- df$x^2\ndf\n\n#>            x             y         z\n#> 1 -3.1415927 -1.224647e-16 9.8696044\n#> 2 -1.8849556 -9.510565e-01 3.5530576\n#> 3 -0.6283185 -5.877853e-01 0.3947842\n#> 4  0.6283185  5.877853e-01 0.3947842\n#> 5  1.8849556  9.510565e-01 3.5530576\n#> 6  3.1415927  1.224647e-16 9.8696044\n\n\n\nYou can also create a data.frame of a data.frame:\n\n\ndata.frame(df, z=df$x^2, u=cos(df$x))\n\n#>            x             y         z       z.1         u\n#> 1 -3.1415927 -1.224647e-16 9.8696044 9.8696044 -1.000000\n#> 2 -1.8849556 -9.510565e-01 3.5530576 3.5530576 -0.309017\n#> 3 -0.6283185 -5.877853e-01 0.3947842 0.3947842  0.809017\n#> 4  0.6283185  5.877853e-01 0.3947842 0.3947842  0.809017\n#> 5  1.8849556  9.510565e-01 3.5530576 3.5530576 -0.309017\n#> 6  3.1415927  1.224647e-16 9.8696044 9.8696044 -1.000000\n\n\n\nFinally, you can use the cbind() function to bind two data.frame column-wise:\n\n\ndf2 <- data.frame(a = 1:length(x), b = 1:length(x))\ncbind(df, df2)\n\n#>            x             y         z a b\n#> 1 -3.1415927 -1.224647e-16 9.8696044 1 1\n#> 2 -1.8849556 -9.510565e-01 3.5530576 2 2\n#> 3 -0.6283185 -5.877853e-01 0.3947842 3 3\n#> 4  0.6283185  5.877853e-01 0.3947842 4 4\n#> 5  1.8849556  9.510565e-01 3.5530576 5 5\n#> 6  3.1415927  1.224647e-16 9.8696044 6 6\n\n\n\n6.3.2 Adding rows\nFor this, use the rbind() function.\n\n\n\n\n\n\nAttention\n\n\n\nThe two data.frame must have the same number of columns and the same column names.\n\n\n\nrbind(df, df)\n\n#>             x             y         z\n#> 1  -3.1415927 -1.224647e-16 9.8696044\n#> 2  -1.8849556 -9.510565e-01 3.5530576\n#> 3  -0.6283185 -5.877853e-01 0.3947842\n#> 4   0.6283185  5.877853e-01 0.3947842\n#> 5   1.8849556  9.510565e-01 3.5530576\n#> 6   3.1415927  1.224647e-16 9.8696044\n#> 7  -3.1415927 -1.224647e-16 9.8696044\n#> 8  -1.8849556 -9.510565e-01 3.5530576\n#> 9  -0.6283185 -5.877853e-01 0.3947842\n#> 10  0.6283185  5.877853e-01 0.3947842\n#> 11  1.8849556  9.510565e-01 3.5530576\n#> 12  3.1415927  1.224647e-16 9.8696044\n\n\n\n6.3.3 Deleting rows/columns\nThis works like with vectors:\n\ndf[-1,]\n\n#>            x             y         z\n#> 2 -1.8849556 -9.510565e-01 3.5530576\n#> 3 -0.6283185 -5.877853e-01 0.3947842\n#> 4  0.6283185  5.877853e-01 0.3947842\n#> 5  1.8849556  9.510565e-01 3.5530576\n#> 6  3.1415927  1.224647e-16 9.8696044\n\ndf[,-1]\n\n#>               y         z\n#> 1 -1.224647e-16 9.8696044\n#> 2 -9.510565e-01 3.5530576\n#> 3 -5.877853e-01 0.3947842\n#> 4  5.877853e-01 0.3947842\n#> 5  9.510565e-01 3.5530576\n#> 6  1.224647e-16 9.8696044"
  },
  {
    "objectID": "07-dataframes.html#tidy-up",
    "href": "07-dataframes.html#tidy-up",
    "title": "\n6  Data frames\n",
    "section": "\n6.4 Tidy up!",
    "text": "6.4 Tidy up!\n\n6.4.1 What is tidy data?\nA good practice in R is to tidy your data. R follows a set of conventions that makes one layout of tabular data much easier to work with than others. Your data will be easier to work with in R if it follows three rules:\n\nEach variable in the data set is placed in its own column\nEach observation is placed in its own row\nEach value is placed in its own cell\n\n\n\n\n\nIllustration of tidy data.\n\n\n\n\nData that satisfies these rules is known as tidy data: you see that thanks to this representation, a 2D table can handle an arbitrary number of variables – this avoids using multi-dimensional arrays or multi-tab Excel documents. Note that it does’t matter if a value is repeated in a column.\nHere is an example:\ndf <- read.csv(\"Data/population.csv\")\ndf # is not tidy\n\nShow output\n\n\n#>   year Angers Bordeaux  Brest  Dijon Grenoble LeHavre LeMans  Lille   Lyon\n#> 1 1962 115273   278403 136104 135694   156707  187845 132181 239955 535746\n#> 2 1968 128557   266662 154023 145357   161616  207150 143246 238554 527800\n#> 3 1975 137591   223131 166826 151705   166037  217882 152285 219204 456716\n#> 4 1982 136038   208159 156060 140942   156637  199388 147697 196705 413095\n#> 5 1990 141404   210336 147956 146703   150758  195854 145502 198691 415487\n#> 6 1999 151279   215363 149634 149867   153317  190905 146105 212597 445452\n#> 7 2007 151108   235178 142722 151543   156793  179751 144164 225789 472330\n#> 8 2012 149017   241287 139676 152071   158346  173142 143599 228652 496343\n#>   Marseille Montpellier Nantes   Nice   Paris  Reims Rennes Saint.Etienne\n#> 1    778071      118864 240048 292958 2790091 134856 151948        210311\n#> 2    889029      161910 260244 322442 2590771 154534 180943        223223\n#> 3    908600      191354 256693 344481 2299830 178381 198305        220181\n#> 4    874436      197231 240539 337085 2176243 177234 194656        204955\n#> 5    800550      207996 244995 342439 2152423 180620 197536        199396\n#> 6    798430      225392 270251 342738 2125246 187206 206229        180210\n#> 7    852395      253712 283025 348721 2193030 183500 207922        175318\n#> 8    852516      268456 291604 343629 2240621 181893 209860        171483\n#>   Strasbourg Toulon Toulouse\n#> 1     228971 161797   323724\n#> 2     249396 174746   370796\n#> 3     253384 181801   373796\n#> 4     248712 179423   347995\n#> 5     252338 167619   358688\n#> 6     264115 160639   390350\n#> 7     272123 166537   439453\n#> 8     274394 164899   453317\n\n\n\nlibrary(tidyr)\ndf <- pivot_longer(df, cols=-year, names_to=\"city\", values_to=\"pop\")\ndf #is tidy\n\nShow output\n\n\n#> # A tibble: 160 × 3\n#>     year city         pop\n#>    <int> <chr>      <int>\n#>  1  1962 Angers    115273\n#>  2  1962 Bordeaux  278403\n#>  3  1962 Brest     136104\n#>  4  1962 Dijon     135694\n#>  5  1962 Grenoble  156707\n#>  6  1962 LeHavre   187845\n#>  7  1962 LeMans    132181\n#>  8  1962 Lille     239955\n#>  9  1962 Lyon      535746\n#> 10  1962 Marseille 778071\n#> # … with 150 more rows\n\n\n\n# is not tidy\npivot_wider(df, names_from=\"city\", values_from=\"pop\")\n\nShow output\n\n\n#> # A tibble: 8 × 21\n#>    year Angers Bordeaux  Brest  Dijon Grenoble LeHavre LeMans  Lille   Lyon\n#>   <int>  <int>    <int>  <int>  <int>    <int>   <int>  <int>  <int>  <int>\n#> 1  1962 115273   278403 136104 135694   156707  187845 132181 239955 535746\n#> 2  1968 128557   266662 154023 145357   161616  207150 143246 238554 527800\n#> 3  1975 137591   223131 166826 151705   166037  217882 152285 219204 456716\n#> 4  1982 136038   208159 156060 140942   156637  199388 147697 196705 413095\n#> 5  1990 141404   210336 147956 146703   150758  195854 145502 198691 415487\n#> 6  1999 151279   215363 149634 149867   153317  190905 146105 212597 445452\n#> 7  2007 151108   235178 142722 151543   156793  179751 144164 225789 472330\n#> 8  2012 149017   241287 139676 152071   158346  173142 143599 228652 496343\n#> # … with 11 more variables: Marseille <int>, Montpellier <int>, Nantes <int>,\n#> #   Nice <int>, Paris <int>, Reims <int>, Rennes <int>, Saint.Etienne <int>,\n#> #   Strasbourg <int>, Toulon <int>, Toulouse <int>\n\n\n\nYou can find more information on data import and tidyness on the data-import cheatsheet and on the tidyr package.\n\n\n\n\nUnderstanding long and wide data with an animation. Source: tidyexplain\n\n\n\n\n\n6.4.2 Tibbles\nA tibble is an enhanced version of the data.frame provided by the tibble package (which is part of the tidyverse). The main advantage of tibble is that it has easier initialization and nicer printing than data.frame.\nMoreover, the performance are also enhanced for the reading from files with read_csv(), read_tsv(), read_table() and read_delim() that do the same things as their read.xx() counterparts and return a tibble. Otherwise, the handling is basically the same.\nMore on tibbles here.\n\nNote that when initializing tibbles, the construction is iterative. It means that when creating a second column, one can refer to the first one that was created. This does’t work with data.frames.\n\n\n# won't work unless a `x` vector was created before\ndata.frame(x=runif(1e3), y=cumsum(x)) \n\n#> Error in data.frame(x = runif(1000), y = cumsum(x)): object 'x' not found\n\nlibrary(tidyverse)\ntib <- tibble(x=runif(1e3), y=cumsum(x))\ntib\n\n#> # A tibble: 1,000 × 2\n#>        x     y\n#>    <dbl> <dbl>\n#>  1 0.651 0.651\n#>  2 0.699 1.35 \n#>  3 0.844 2.19 \n#>  4 0.436 2.63 \n#>  5 0.267 2.90 \n#>  6 0.866 3.76 \n#>  7 0.416 4.18 \n#>  8 0.767 4.94 \n#>  9 0.773 5.72 \n#> 10 0.774 6.49 \n#> # … with 990 more rows\n\n\n\n\n\n\n\n\nAttention\n\n\n\n\n\n\nTibbles are quite strict about subsetting. [ always returns another tibble. Contrast this with a data frame: sometimes [ returns a data frame and sometimes it just returns a vector:\n\nhead(tib[[1]]) # is a vector\n\n#> [1] 0.6514851 0.6987136 0.8436557 0.4357583 0.2666696 0.8655372\n\nhead(tib[,1])  # is a tibble\n\n#> # A tibble: 6 × 1\n#>       x\n#>   <dbl>\n#> 1 0.651\n#> 2 0.699\n#> 3 0.844\n#> 4 0.436\n#> 5 0.267\n#> 6 0.866\n\n\nUnless you want to get a tibble, I recommend always using the $ notation when you want to get a column as a vector to avoid problems.\n\nAnother interesting feature of tibbles is that their columns can contain vectors, like usual, but also lists of any R objects like other tibbles, nls() objects, etc. This is called “nesting”, and you can nest and un-nest tibbles using these explicit functions:\n\n\ntib1 <- tibble(x=1:3, y=1:3)\ntib2 <- tibble(x=1:5, y=1:5)\ntib  <- tibble(number=1:2, data=list(tib1, tib2))\ntib\n\n#> # A tibble: 2 × 2\n#>   number data            \n#>    <int> <list>          \n#> 1      1 <tibble [3 × 2]>\n#> 2      2 <tibble [5 × 2]>\n\n\n\n\n\n\nFigure 6.1: Excel equivalent to a nested tibble.\n\n\n\n\n\ntib_unnested <- unnest(tib, data)\ntib_unnested\n\n#> # A tibble: 8 × 3\n#>   number     x     y\n#>    <int> <int> <int>\n#> 1      1     1     1\n#> 2      1     2     2\n#> 3      1     3     3\n#> 4      2     1     1\n#> 5      2     2     2\n#> 6      2     3     3\n#> 7      2     4     4\n#> 8      2     5     5\n\ntib_unnested_renested <- nest(tib_unnested, data = c(number, y))\ntib_unnested_renested\n\n#> # A tibble: 5 × 2\n#>       x data            \n#>   <int> <list>          \n#> 1     1 <tibble [2 × 2]>\n#> 2     2 <tibble [2 × 2]>\n#> 3     3 <tibble [2 × 2]>\n#> 4     4 <tibble [1 × 2]>\n#> 5     5 <tibble [1 × 2]>\n\ntib_unnested_renested$data # The `data` column is a list\n\n#> [[1]]\n#> # A tibble: 2 × 2\n#>   number     y\n#>    <int> <int>\n#> 1      1     1\n#> 2      2     1\n#> \n#> [[2]]\n#> # A tibble: 2 × 2\n#>   number     y\n#>    <int> <int>\n#> 1      1     2\n#> 2      2     2\n#> \n#> [[3]]\n#> # A tibble: 2 × 2\n#>   number     y\n#>    <int> <int>\n#> 1      1     3\n#> 2      2     3\n#> \n#> [[4]]\n#> # A tibble: 1 × 2\n#>   number     y\n#>    <int> <int>\n#> 1      2     4\n#> \n#> [[5]]\n#> # A tibble: 1 × 2\n#>   number     y\n#>    <int> <int>\n#> 1      2     5"
  },
  {
    "objectID": "07-dataframes.html#operations-in-the-tidyverse",
    "href": "07-dataframes.html#operations-in-the-tidyverse",
    "title": "\n6  Data frames\n",
    "section": "\n6.5 Operations in the tidyverse",
    "text": "6.5 Operations in the tidyverse\nIn the end, base R and the tidyverse package provide many efficient functions to perform most of the tasks you would want to perform recursively, thus allowing avoiding explicit for loops (that are slow).\nHere are some examples, and you will find much more here. Take a look at the cheatsheets on tidyr and on dplyr, it’s really helpful.\nLet’s work on this tibble:\n\n# Let's create a random tibble\nlibrary(tidyverse)\nN <- 500\ndt <- tibble(x     = rep(runif(N, -1, 1), 3), \n             y     = runif(N*3, -1, 1), \n             signx = ifelse(x>0, \"positive\", \"negative\"),\n             signy = ifelse(y>0, \"positive\", \"negative\")\n)\ndt\n\n#> # A tibble: 1,500 × 4\n#>          x       y signx    signy   \n#>      <dbl>   <dbl> <chr>    <chr>   \n#>  1  0.0630  0.0864 positive positive\n#>  2 -0.209  -0.184  negative negative\n#>  3 -0.0769  0.0254 negative positive\n#>  4  0.495   0.792  positive positive\n#>  5 -0.701  -0.874  negative negative\n#>  6 -0.143   0.184  negative positive\n#>  7 -0.131  -0.926  negative negative\n#>  8 -0.610   0.397  negative positive\n#>  9  0.324  -0.831  positive negative\n#> 10  0.500   0.265  positive positive\n#> # … with 1,490 more rows\n\n\n\n6.5.1 The pipe operator\nIn the following, we will introduce the pipe operator from the magrittr package: %>%. This operator allows a clear syntax for successive operations, as “what is on the left of the operator is given as first argument of what is on the right”. It is thus a good habit to write each operation on a separate line to facilitate the reading. This is particularly helpful when performing multiple nested operations. For example, summary(head(tail(dt),2)), which is hard to read, would translate to:\ndt %>% \n    tail() %>% \n    head(2) %>% \n    summary()\nNote that since the 4.1 version of R, R has a native pipe operator: |>. However, for backward compatibility with all my previous code I still stick to the magrittr version %>%. In magrittr’s pipe, retrieving the piped object is done with the operator ., while it is done with the operator _ in base R’s.\nSilly example:\n\n\"Hello\" %>% gsub(\"o\", \"e\") # replace the substring \"Hello\" by \"o\" in string \"e\"\n\n#> [1] \"e\"\n\n\"Hello\" %>% gsub(\"o\", \"e\", .) # replace the substring \"o\" by \"e\" in string \"Hello\"\n\n#> [1] \"Helle\"\n\n\"Hello\" |> gsub(\"o\", \"e\") # replace the substring \"Hello\" by \"o\" in string \"e\"\n\n#> [1] \"e\"\n\n\"Hello\" |> gsub(\"o\", \"e\", x=_) # replace the substring \"o\" by \"e\" in string \"Hello\"\n\n#> [1] \"Helle\"\n\n\n\n6.5.2 Sampling data\n\ndt %>% slice(1:3)  # by index\n\n#> # A tibble: 3 × 4\n#>         x       y signx    signy   \n#>     <dbl>   <dbl> <chr>    <chr>   \n#> 1  0.0630  0.0864 positive positive\n#> 2 -0.209  -0.184  negative negative\n#> 3 -0.0769  0.0254 negative positive\n\ndt %>% sample_n(3) # randomly\n\n#> # A tibble: 3 × 4\n#>          x      y signx    signy   \n#>      <dbl>  <dbl> <chr>    <chr>   \n#> 1 -0.411    0.911 negative positive\n#> 2 -0.00343 -0.800 negative negative\n#> 3  0.760   -0.628 positive negative\n\n\n\n6.5.3 Operations on groups of a variable\ngroup_by(column) groups by similar values of the wanted column(s) and performs the next operations on each element of the group successively.\n\ndt %>% \n    group_by(signx)\n\n#> # A tibble: 1,500 × 4\n#> # Groups:   signx [2]\n#>          x       y signx    signy   \n#>      <dbl>   <dbl> <chr>    <chr>   \n#>  1  0.0630  0.0864 positive positive\n#>  2 -0.209  -0.184  negative negative\n#>  3 -0.0769  0.0254 negative positive\n#>  4  0.495   0.792  positive positive\n#>  5 -0.701  -0.874  negative negative\n#>  6 -0.143   0.184  negative positive\n#>  7 -0.131  -0.926  negative negative\n#>  8 -0.610   0.397  negative positive\n#>  9  0.324  -0.831  positive negative\n#> 10  0.500   0.265  positive positive\n#> # … with 1,490 more rows\n\ndt %>% \n    group_by(signx) %>% \n    sample_n(3)\n\n#> # A tibble: 6 × 4\n#> # Groups:   signx [2]\n#>         x      y signx    signy   \n#>     <dbl>  <dbl> <chr>    <chr>   \n#> 1 -0.0782  0.308 negative positive\n#> 2 -0.402  -0.627 negative negative\n#> 3 -0.883   0.218 negative positive\n#> 4  0.225   0.433 positive positive\n#> 5  0.126  -0.973 positive negative\n#> 6  0.513   0.592 positive positive\n\ndt %>% \n    group_by(signx, signy) %>% \n    sample_n(3)\n\n#> # A tibble: 12 × 4\n#> # Groups:   signx, signy [4]\n#>           x      y signx    signy   \n#>       <dbl>  <dbl> <chr>    <chr>   \n#>  1 -0.736   -0.282 negative negative\n#>  2 -0.720   -0.312 negative negative\n#>  3 -0.543   -0.500 negative negative\n#>  4 -0.647    0.948 negative positive\n#>  5 -0.276    0.731 negative positive\n#>  6 -0.140    0.433 negative positive\n#>  7  0.00281 -0.290 positive negative\n#>  8  0.468   -0.284 positive negative\n#>  9  0.817   -0.694 positive negative\n#> 10  0.321    0.223 positive positive\n#> 11  0.332    0.546 positive positive\n#> 12  0.239    0.581 positive positive\n\n\n\n6.5.4 Summary by groups of a variable\nsummarise() returns a single value for each element of the groups.\n\ndt %>% \n    group_by(signx) %>% \n    summarise(count  = n(),\n              mean_x = mean(x), \n              sd_x   = sd(x))\n\n#> # A tibble: 2 × 4\n#>   signx    count mean_x  sd_x\n#>   <chr>    <int>  <dbl> <dbl>\n#> 1 negative   735 -0.491 0.285\n#> 2 positive   765  0.483 0.285\n\ndt %>% \n    group_by(signx, signy) %>% \n    summarise(count  = n(),\n              mean_x = mean(x), \n              mean_y = mean(y))\n\n#> # A tibble: 4 × 5\n#> # Groups:   signx [2]\n#>   signx    signy    count mean_x mean_y\n#>   <chr>    <chr>    <int>  <dbl>  <dbl>\n#> 1 negative negative   365 -0.499 -0.480\n#> 2 negative positive   370 -0.483  0.499\n#> 3 positive negative   392  0.483 -0.484\n#> 4 positive positive   373  0.483  0.484\n\n\n\n6.5.5 Sorting\n\ndt %>% arrange(x)\n\n#> # A tibble: 1,500 × 4\n#>         x      y signx    signy   \n#>     <dbl>  <dbl> <chr>    <chr>   \n#>  1 -0.984  0.450 negative positive\n#>  2 -0.984 -0.404 negative negative\n#>  3 -0.984  0.815 negative positive\n#>  4 -0.983  0.926 negative positive\n#>  5 -0.983 -0.990 negative negative\n#>  6 -0.983 -0.869 negative negative\n#>  7 -0.983  0.357 negative positive\n#>  8 -0.983  0.295 negative positive\n#>  9 -0.983 -0.446 negative negative\n#> 10 -0.979 -0.158 negative negative\n#> # … with 1,490 more rows\n\ndt %>% arrange(x, desc(y))\n\n#> # A tibble: 1,500 × 4\n#>         x      y signx    signy   \n#>     <dbl>  <dbl> <chr>    <chr>   \n#>  1 -0.984  0.815 negative positive\n#>  2 -0.984  0.450 negative positive\n#>  3 -0.984 -0.404 negative negative\n#>  4 -0.983  0.926 negative positive\n#>  5 -0.983 -0.869 negative negative\n#>  6 -0.983 -0.990 negative negative\n#>  7 -0.983  0.357 negative positive\n#>  8 -0.983  0.295 negative positive\n#>  9 -0.983 -0.446 negative negative\n#> 10 -0.979  0.691 negative positive\n#> # … with 1,490 more rows\n\n\n\n6.5.6 Merge tables column-wise\nAt least one column with the exact same name must be present in each table to use the xx_join() functions. There are more possibilities than inner_join() that I show here, see the help for more information.\n\ndt2 <- tibble(signx=c(\"positive\",\"positive\",\"negative\",\"negative\"), \n              signy=c(\"positive\",\"negative\",\"positive\",\"negative\"), \n              value=c(TRUE, FALSE, FALSE, TRUE))\ndt2\n\n#> # A tibble: 4 × 3\n#>   signx    signy    value\n#>   <chr>    <chr>    <lgl>\n#> 1 positive positive TRUE \n#> 2 positive negative FALSE\n#> 3 negative positive FALSE\n#> 4 negative negative TRUE\n\ninner_join(dt, dt2)\n\n#> # A tibble: 1,500 × 5\n#>          x       y signx    signy    value\n#>      <dbl>   <dbl> <chr>    <chr>    <lgl>\n#>  1  0.0630  0.0864 positive positive TRUE \n#>  2 -0.209  -0.184  negative negative TRUE \n#>  3 -0.0769  0.0254 negative positive FALSE\n#>  4  0.495   0.792  positive positive TRUE \n#>  5 -0.701  -0.874  negative negative TRUE \n#>  6 -0.143   0.184  negative positive FALSE\n#>  7 -0.131  -0.926  negative negative TRUE \n#>  8 -0.610   0.397  negative positive FALSE\n#>  9  0.324  -0.831  positive negative FALSE\n#> 10  0.500   0.265  positive positive TRUE \n#> # … with 1,490 more rows\n\n\n\n6.5.7 Merge tables row-wise\nThis works even if there are missing rows.\n\ndt3 <- tibble(a=1:3, b=3:5, c=6:8)\ndt4 <- tibble(a=3:1, c=3:5)\nbind_rows(dt3, dt4)\n\n#> # A tibble: 6 × 3\n#>       a     b     c\n#>   <int> <int> <int>\n#> 1     1     3     6\n#> 2     2     4     7\n#> 3     3     5     8\n#> 4     3    NA     3\n#> 5     2    NA     4\n#> 6     1    NA     5\n\n\n\n6.5.8 Add/modify a column\nmutate(), like $, adds a column if it doesn’t exist, and modifies it if it does.\n\ndt %>% mutate(w=seq_along(x), z=sin(x))\n\n#> # A tibble: 1,500 × 6\n#>          x       y signx    signy        w       z\n#>      <dbl>   <dbl> <chr>    <chr>    <int>   <dbl>\n#>  1  0.0630  0.0864 positive positive     1  0.0630\n#>  2 -0.209  -0.184  negative negative     2 -0.207 \n#>  3 -0.0769  0.0254 negative positive     3 -0.0769\n#>  4  0.495   0.792  positive positive     4  0.475 \n#>  5 -0.701  -0.874  negative negative     5 -0.645 \n#>  6 -0.143   0.184  negative positive     6 -0.143 \n#>  7 -0.131  -0.926  negative negative     7 -0.131 \n#>  8 -0.610   0.397  negative positive     8 -0.573 \n#>  9  0.324  -0.831  positive negative     9  0.318 \n#> 10  0.500   0.265  positive positive    10  0.479 \n#> # … with 1,490 more rows\n\ndt %>% mutate(x=seq_along(x))\n\n#> # A tibble: 1,500 × 4\n#>        x       y signx    signy   \n#>    <int>   <dbl> <chr>    <chr>   \n#>  1     1  0.0864 positive positive\n#>  2     2 -0.184  negative negative\n#>  3     3  0.0254 negative positive\n#>  4     4  0.792  positive positive\n#>  5     5 -0.874  negative negative\n#>  6     6  0.184  negative positive\n#>  7     7 -0.926  negative negative\n#>  8     8  0.397  negative positive\n#>  9     9 -0.831  positive negative\n#> 10    10  0.265  positive positive\n#> # … with 1,490 more rows\n\n\n\n6.5.9 Selecting columns\n\ndt %>% select(x)  # only x\n\n#> # A tibble: 1,500 × 1\n#>          x\n#>      <dbl>\n#>  1  0.0630\n#>  2 -0.209 \n#>  3 -0.0769\n#>  4  0.495 \n#>  5 -0.701 \n#>  6 -0.143 \n#>  7 -0.131 \n#>  8 -0.610 \n#>  9  0.324 \n#> 10  0.500 \n#> # … with 1,490 more rows\n\ndt %>% select(-x) # all but x\n\n#> # A tibble: 1,500 × 3\n#>          y signx    signy   \n#>      <dbl> <chr>    <chr>   \n#>  1  0.0864 positive positive\n#>  2 -0.184  negative negative\n#>  3  0.0254 negative positive\n#>  4  0.792  positive positive\n#>  5 -0.874  negative negative\n#>  6  0.184  negative positive\n#>  7 -0.926  negative negative\n#>  8  0.397  negative positive\n#>  9 -0.831  positive negative\n#> 10  0.265  positive positive\n#> # … with 1,490 more rows\n\ndt %>% select(starts_with(\"sign\"))\n\n#> # A tibble: 1,500 × 2\n#>    signx    signy   \n#>    <chr>    <chr>   \n#>  1 positive positive\n#>  2 negative negative\n#>  3 negative positive\n#>  4 positive positive\n#>  5 negative negative\n#>  6 negative positive\n#>  7 negative negative\n#>  8 negative positive\n#>  9 positive negative\n#> 10 positive positive\n#> # … with 1,490 more rows\n\ndt %>% select(contains(\"x\"))\n\n#> # A tibble: 1,500 × 2\n#>          x signx   \n#>      <dbl> <chr>   \n#>  1  0.0630 positive\n#>  2 -0.209  negative\n#>  3 -0.0769 negative\n#>  4  0.495  positive\n#>  5 -0.701  negative\n#>  6 -0.143  negative\n#>  7 -0.131  negative\n#>  8 -0.610  negative\n#>  9  0.324  positive\n#> 10  0.500  positive\n#> # … with 1,490 more rows\n\n\n\n6.5.10 Filtering columns\n\ndt %>% filter(signx==\"positive\")\n\n#> # A tibble: 765 × 4\n#>         x       y signx    signy   \n#>     <dbl>   <dbl> <chr>    <chr>   \n#>  1 0.0630  0.0864 positive positive\n#>  2 0.495   0.792  positive positive\n#>  3 0.324  -0.831  positive negative\n#>  4 0.500   0.265  positive positive\n#>  5 0.923  -0.524  positive negative\n#>  6 0.412   0.801  positive positive\n#>  7 0.436  -0.606  positive negative\n#>  8 0.168  -0.731  positive negative\n#>  9 0.974   0.225  positive positive\n#> 10 0.234   0.638  positive positive\n#> # … with 755 more rows\n\ndt %>% filter(x<0, y>.1) # multiple filters can be applied at once\n\n#> # A tibble: 332 × 4\n#>          x     y signx    signy   \n#>      <dbl> <dbl> <chr>    <chr>   \n#>  1 -0.143  0.184 negative positive\n#>  2 -0.610  0.397 negative positive\n#>  3 -0.489  0.409 negative positive\n#>  4 -0.0242 0.104 negative positive\n#>  5 -0.276  0.731 negative positive\n#>  6 -0.983  0.926 negative positive\n#>  7 -0.372  0.302 negative positive\n#>  8 -0.883  0.839 negative positive\n#>  9 -0.125  0.148 negative positive\n#> 10 -0.212  0.802 negative positive\n#> # … with 322 more rows\n\n\n\n6.5.11 Reorder columns\n\ndt %>% relocate(y, .after = signy)\n\n#> # A tibble: 1,500 × 4\n#>          x signx    signy          y\n#>      <dbl> <chr>    <chr>      <dbl>\n#>  1  0.0630 positive positive  0.0864\n#>  2 -0.209  negative negative -0.184 \n#>  3 -0.0769 negative positive  0.0254\n#>  4  0.495  positive positive  0.792 \n#>  5 -0.701  negative negative -0.874 \n#>  6 -0.143  negative positive  0.184 \n#>  7 -0.131  negative negative -0.926 \n#>  8 -0.610  negative positive  0.397 \n#>  9  0.324  positive negative -0.831 \n#> 10  0.500  positive positive  0.265 \n#> # … with 1,490 more rows\n\n\n\n6.5.12 Separate columns\nThe separation is based on standard separators such as “-”, “_”, “.”, ” “, etc. A single separator can be specified with the argument sep, otherwise all separators are used. One must provide the resulting vector of new column names: if one value is NA, this column will be discarded. Examples:\n\ndt5 <- tibble(file=list.files(path=\"Exo/FTIR/Data/\", pattern=\".xls\"))\ndt5\n\n#> # A tibble: 10 × 1\n#>    file             \n#>    <chr>            \n#>  1 sample_0_25C.xls \n#>  2 sample_0_300C.xls\n#>  3 sample_1_25C.xls \n#>  4 sample_1_300C.xls\n#>  5 sample_2_25C.xls \n#>  6 sample_2_300C.xls\n#>  7 sample_3_25C.xls \n#>  8 sample_3_300C.xls\n#>  9 sample_4_25C.xls \n#> 10 sample_4_300C.xls\n\ndt5 %>% separate(file, c(NA, \"sample\", \"temperature\", NA), convert = TRUE)\n\n#> # A tibble: 10 × 2\n#>    sample temperature\n#>     <int> <chr>      \n#>  1      0 25C        \n#>  2      0 300C       \n#>  3      1 25C        \n#>  4      1 300C       \n#>  5      2 25C        \n#>  6      2 300C       \n#>  7      3 25C        \n#>  8      3 300C       \n#>  9      4 25C        \n#> 10      4 300C\n\ndt5 %>% separate(file, \n                 c(\"name\", \"extension\"), \n                 sep = \"\\\\.\"\n                 )\n\n#> # A tibble: 10 × 2\n#>    name          extension\n#>    <chr>         <chr>    \n#>  1 sample_0_25C  xls      \n#>  2 sample_0_300C xls      \n#>  3 sample_1_25C  xls      \n#>  4 sample_1_300C xls      \n#>  5 sample_2_25C  xls      \n#>  6 sample_2_300C xls      \n#>  7 sample_3_25C  xls      \n#>  8 sample_3_300C xls      \n#>  9 sample_4_25C  xls      \n#> 10 sample_4_300C xls\n\n\n\n6.5.13 Apply a function recursively on each element of a column\nTake a look at the cheatsheet on the purrr package for more options and a visual help on the map() family. I show here a use of purrr::map(vector, function) that returns a list. map(x, f) applies the function f() to each element of the vector x, putting the result in a separate element of a list: map(x, f) -> list(f(x1), f(x2), ... f(xn)). In case f(xi) returns a single value, you might want to use map_dbl() or map_chr(), for example, that will return a vector of doubles or of characters, respectively.\n\nx <- c(pi, pi/3, pi/2)\nmap(x, sin)     # returns a list\n\n#> [[1]]\n#> [1] 1.224647e-16\n#> \n#> [[2]]\n#> [1] 0.8660254\n#> \n#> [[3]]\n#> [1] 1\n\nx %>% map_dbl(sin) # returns a vector\n\n#> [1] 1.224647e-16 8.660254e-01 1.000000e+00\n\n\nOf course, in the above case, it’s a stupid use of the power of map(). A typical use case is when you want to read multiple files, for example:\n\ndt6 <- tibble(file=list.files(path=\"Exo/spectro/Data\", \n                              pattern = \".txt\", \n                              full.names = TRUE)) %>% \n    slice(1:5) %>% \n    mutate(data = map(file, read_table))\ndt6\n\n#> # A tibble: 5 × 2\n#>   file                          data                  \n#>   <chr>                         <list>                \n#> 1 Exo/spectro/Data/rubis_01.txt <spc_tbl_ [1,014 × 2]>\n#> 2 Exo/spectro/Data/rubis_02.txt <spc_tbl_ [1,014 × 2]>\n#> 3 Exo/spectro/Data/rubis_03.txt <spc_tbl_ [1,014 × 2]>\n#> 4 Exo/spectro/Data/rubis_04.txt <spc_tbl_ [1,014 × 2]>\n#> 5 Exo/spectro/Data/rubis_05.txt <spc_tbl_ [1,014 × 2]>\n\n\nThis is (almost) equivalent to:\n\ndt6 <- tibble(file=list.files(path=\"Exo/spectro/Data\", \n                              pattern = \".txt\", \n                              full.names = TRUE)) %>% \n    slice(1:5) %>%\n    mutate(data = map(file, ~ read_table(., col_names = c(\"w\", \"Int\"))))\n\nYou see that you can create the function directly within the call to map using the shortcut map(vector, ~ function(.)). This is useful to provide more arguments to the function – another solution is to write your own function before the call to map() and then call this function in map().\nNote that in case you need more parameters, you can use purrr::map2(vector1, vector2, ~function(.x, .y)), where .x and .y refer to vector1 and vector2, respectively (it’s always .x and .y whatever the name of vector1 and vector2).\n\ntibble(x=1:3, y=5:7) %>% \n    mutate(sum = map2_dbl(x, y, sum))\n\n#> # A tibble: 3 × 3\n#>       x     y   sum\n#>   <int> <int> <dbl>\n#> 1     1     5     6\n#> 2     2     6     8\n#> 3     3     7    10\n\ntibble(a=list(tibble(x=1:3, y=5:7), \n              tibble(x=0:3, y=4:7)), \n       b=list(tibble(x=10:13, y=15:18), \n              tibble(x=-1:2,  y=-14:-17))) %>% \n    mutate(sumx = map2_dbl(a, b, ~sum(.x$x, .y$x)),\n           sumy = map2_dbl(a, b, ~sum(.x$y, .y$y)))\n\n#> # A tibble: 2 × 4\n#>   a                b                 sumx  sumy\n#>   <list>           <list>           <dbl> <dbl>\n#> 1 <tibble [3 × 2]> <tibble [4 × 2]>    52    84\n#> 2 <tibble [4 × 2]> <tibble [4 × 2]>     8   -40\n\n\n\n6.5.14 Nesting and un-nesting data\n\ndt7 <- dt6 %>% \n    mutate(file = basename(file)) %>% \n    unnest(data)\ndt7\n\n#> # A tibble: 5,075 × 3\n#>    file             w   Int\n#>    <chr>        <dbl> <dbl>\n#>  1 rubis_01.txt 3064.  43.9\n#>  2 rubis_01.txt 3064.  47.9\n#>  3 rubis_01.txt 3064.  44.5\n#>  4 rubis_01.txt 3065.  50.5\n#>  5 rubis_01.txt 3065.  50.5\n#>  6 rubis_01.txt 3065.  44.5\n#>  7 rubis_01.txt 3065.  44.9\n#>  8 rubis_01.txt 3066.  39.9\n#>  9 rubis_01.txt 3066.  49.5\n#> 10 rubis_01.txt 3066.  48.9\n#> # … with 5,065 more rows\n\n# Nesting data per repeated values in a column (~equivalent to grouping)\ndt7 %>% nest(data=-file)\n\n#> # A tibble: 5 × 2\n#>   file         data                \n#>   <chr>        <list>              \n#> 1 rubis_01.txt <tibble [1,015 × 2]>\n#> 2 rubis_02.txt <tibble [1,015 × 2]>\n#> 3 rubis_03.txt <tibble [1,015 × 2]>\n#> 4 rubis_04.txt <tibble [1,015 × 2]>\n#> 5 rubis_05.txt <tibble [1,015 × 2]>\n\n\n\n6.5.15 Providing data to ggplot\n\nlibrary(ggplot2)\ndt %>% filter(abs(y) > 0.1) %>% \n    ggplot(aes(x=x, y=y, color=signy))+\n        geom_point()"
  },
  {
    "objectID": "07-dataframes.html#exo-df",
    "href": "07-dataframes.html#exo-df",
    "title": "\n6  Data frames\n",
    "section": "\n6.6 Exercises",
    "text": "6.6 Exercises\nInteractive exercises can be found in the tutor package. For this, simply run:\nlibrary(tutor)\ntuto(\"dataframes\")\nAlternatively, you can just download the archive with all the exercises files, unzip it in your R class RStudio project, and edit the R files.\n\nExercise 1\n\nCreate a 3 column data.frame containing 10 random values, their sinus, and the sum of the two first columns.\nPrint the 4 first lines of the table\nPrint the second column\nPrint the average of the third column\nUsing plot(x,y) where x and y are vectors, plot the 2nd column as a function of the first\nLook into the function write.table() to write a text file containing this data.frame\n\nDo the all the same things with a tibble\n\n\n\nSolution\n\n# Create a 3 column `data.frame`{.R} containing 10 random values, their sinus, \n# and the sum of the two first columns.\nx <- runif(10)\ny <- sin(x)\nz <- x + y\ndf <- data.frame(x=x, y=y, z=z)\n# Print the 4 first lines of the table\nhead(df, 4)\n\n#>           x         y         z\n#> 1 0.3687327 0.3604336 0.7291662\n#> 2 0.7564410 0.6863374 1.4427784\n#> 3 0.8059449 0.7214852 1.5274301\n#> 4 0.9018675 0.7844864 1.6863540\n\n# Print the second column\ndf[,2]\n\n#>  [1] 0.36043357 0.68633741 0.72148524 0.78448642 0.12695546 0.80907353\n#>  [7] 0.60398593 0.76937501 0.06252437 0.79503919\n\n# Print the average of the third column\nmean(df$z); mean(df[3]); mean(df[,3])\n\n#> [1] 1.213055\n\n\n#> [1] NA\n\n\n#> [1] 1.213055\n\n# Using `plot(x,y)`{.R} where `x` and `y` are vectors, \n# plot the 2nd column as a function of the first\nplot(df[,1], df[,2])\n\n\n\nplot(df$x, df$y)\n\n\n\n# Look into the function `write.table()`{.R} to write a text file \n# containing this `data.frame`{.R}\nwrite.table(df, \"Data/some_data.dat\", quote = FALSE, row.names = FALSE)\n# # # # # # # # # # # # # # # # # \n# Tibble version\nlibrary(tidyverse)\ndf_tib <- tibble(a = runif(10), b = sin(a), c = a + b)\nhead(df_tib, 4)\n\n#> # A tibble: 4 × 3\n#>       a     b     c\n#>   <dbl> <dbl> <dbl>\n#> 1 0.748 0.680 1.43 \n#> 2 0.586 0.553 1.14 \n#> 3 0.211 0.210 0.421\n#> 4 0.504 0.483 0.988\n\ndf_tib[,2]; df_tib[[2]];\n\n#> # A tibble: 10 × 1\n#>         b\n#>     <dbl>\n#>  1 0.680 \n#>  2 0.553 \n#>  3 0.210 \n#>  4 0.483 \n#>  5 0.282 \n#>  6 0.765 \n#>  7 0.0972\n#>  8 0.642 \n#>  9 0.480 \n#> 10 0.332\n\n\n#>  [1] 0.67988485 0.55273852 0.20967041 0.48323747 0.28164016 0.76507506\n#>  [7] 0.09723754 0.64213306 0.48012999 0.33159930\n\nmean(df_tib$c); mean(df_tib[3]); mean(df_tib[,3]); mean(df_tib[[3]])\n\n#> [1] 0.9362317\n\n\n#> [1] NA\n\n\n#> [1] NA\n\n\n#> [1] 0.9362317\n\nwrite.table(df_tib, \"Data/some_data.dat\", quote = FALSE, row.names = FALSE)\nplot(df_tib$a, df_tib$b)\n\n\n\n\nExercise 2\n\nDownload the files:\n\nrubis_01.txt\npopulation.csv\nFTIR_rocks.xlsx\n\n\nLoad them into separate data.frames. Look into the options of read.table(), read.csv(), readxl::read_excel(), to get the proper data fields.\nAdd column names to the data.frame containing rubis_01.txt.\nPrint their dimensions.\nDo the same things with tibbles.\n\n\nSolution\n\nrubis_01   <- read.table(\"Data/rubis_01.txt\", col.names = c(\"w\", \"intensity\"))\npopulation <- read.csv(\"Data/population.csv\")\nFTIR_rocks <- readxl::read_excel(\"Data/FTIR_rocks.xlsx\")\ndim(rubis_01); names(rubis_01)\n\n#> [1] 1015    2\n\n\n#> [1] \"w\"         \"intensity\"\n\ndim(population); names(population)\n\n#> [1]  8 21\n\n\n#>  [1] \"year\"          \"Angers\"        \"Bordeaux\"      \"Brest\"        \n#>  [5] \"Dijon\"         \"Grenoble\"      \"LeHavre\"       \"LeMans\"       \n#>  [9] \"Lille\"         \"Lyon\"          \"Marseille\"     \"Montpellier\"  \n#> [13] \"Nantes\"        \"Nice\"          \"Paris\"         \"Reims\"        \n#> [17] \"Rennes\"        \"Saint.Etienne\" \"Strasbourg\"    \"Toulon\"       \n#> [21] \"Toulouse\"\n\ndim(FTIR_rocks); names(FTIR_rocks)\n\n#> [1] 4718    4\n\n\n#> [1] \"wavenumber, cm-1\" \"rock 1\"           \"rock 2\"           \"rock 3\"\n\nlibrary(tidyverse)\nrubis_01 <- read_table(\"Data/rubis_01.txt\", col_names = c(\"w\", \"intensity\"))\npopulation <- read_csv(\"Data/population.csv\")\n\nExercise 3\n\nDownload the TGA data file ATG.txt\n\nLoad it into a data.frame. Look into the options of read.table() to get the proper data fields.\nDo the same with a tibble\n\n\nSolution\n\nd <- read.table(\"Data/ATG.txt\", \n                skip=12,\n                header=FALSE, \n                nrows=4088)\nnames(d) <- c(\"Index\", \"t\", \"Ts\", \"Tr\", \"Value\")\nhead(d)\n\n#>   Index  t      Ts Tr   Value\n#> 1     0  0 32.3769 25 32.9680\n#> 2     3  3 32.4051 25 32.9655\n#> 3     6  6 32.4332 25 32.9619\n#> 4     9  9 32.4726 25 32.9582\n#> 5    12 12 32.5066 25 32.9544\n#> 6    15 15 32.5221 25 32.9504\n\nd <- read.table(\"Data/ATG.txt\", \n                skip=10,\n                comment.char=\"[\",\n                header=TRUE, \n                nrows=4088)\nhead(d)\n\n#>   Index  t      Ts Tr   Value\n#> 1     0  0 32.3769 25 32.9680\n#> 2     3  3 32.4051 25 32.9655\n#> 3     6  6 32.4332 25 32.9619\n#> 4     9  9 32.4726 25 32.9582\n#> 5    12 12 32.5066 25 32.9544\n#> 6    15 15 32.5221 25 32.9504\n\nlibrary(tidyverse)\nd <- read_table(\"Data/ATG.txt\", \n                skip    = 10,\n                comment = \"[\") %>% \n        drop_na()\nd\n\n#> # A tibble: 4,088 × 5\n#>    Index     t    Ts    Tr Value\n#>    <dbl> <dbl> <dbl> <dbl> <dbl>\n#>  1     0     0  32.4    25  33.0\n#>  2     3     3  32.4    25  33.0\n#>  3     6     6  32.4    25  33.0\n#>  4     9     9  32.5    25  33.0\n#>  5    12    12  32.5    25  33.0\n#>  6    15    15  32.5    25  33.0\n#>  7    18    18  32.6    25  32.9\n#>  8    21    21  32.6    25  32.9\n#>  9    24    24  32.6    25  32.9\n#> 10    27    27  32.7    25  32.9\n#> # … with 4,078 more rows\n\n\nExercise 4\nDownload population.csv and load it into a tibble.\n\nWhat are the names of the columns?\nAre the data tidy? make the table tidy if needed\nCreate a subset containing the data for Montpellier\n\nWhat is the max and min of population in this city?\nThe average population over time?\n\n\nWhat is the total population in 2012?\nWhat is the total population per year?\nWhat is the average population per city over the years?\n\n\nSolution\n\n# Download population.txt and load it into a `data.frame`{.R}.\nlibrary(tidyverse)\npopul <- read_csv(\"Data/population.csv\")\n# What are the names of the columns and the dimension of the table?\nnames(popul); dim(popul)\n\n#>  [1] \"year\"          \"Angers\"        \"Bordeaux\"      \"Brest\"        \n#>  [5] \"Dijon\"         \"Grenoble\"      \"LeHavre\"       \"LeMans\"       \n#>  [9] \"Lille\"         \"Lyon\"          \"Marseille\"     \"Montpellier\"  \n#> [13] \"Nantes\"        \"Nice\"          \"Paris\"         \"Reims\"        \n#> [17] \"Rennes\"        \"Saint-Etienne\" \"Strasbourg\"    \"Toulon\"       \n#> [21] \"Toulouse\"\n\n\n#> [1]  8 21\n\n# Are the data tidy?\nhead(popul) # no\n\n#> # A tibble: 6 × 21\n#>    year Angers Bordeaux  Brest  Dijon Grenoble LeHavre LeMans  Lille   Lyon\n#>   <dbl>  <dbl>    <dbl>  <dbl>  <dbl>    <dbl>   <dbl>  <dbl>  <dbl>  <dbl>\n#> 1  1962 115273   278403 136104 135694   156707  187845 132181 239955 535746\n#> 2  1968 128557   266662 154023 145357   161616  207150 143246 238554 527800\n#> 3  1975 137591   223131 166826 151705   166037  217882 152285 219204 456716\n#> 4  1982 136038   208159 156060 140942   156637  199388 147697 196705 413095\n#> 5  1990 141404   210336 147956 146703   150758  195854 145502 198691 415487\n#> 6  1999 151279   215363 149634 149867   153317  190905 146105 212597 445452\n#> # … with 11 more variables: Marseille <dbl>, Montpellier <dbl>, Nantes <dbl>,\n#> #   Nice <dbl>, Paris <dbl>, Reims <dbl>, Rennes <dbl>, `Saint-Etienne` <dbl>,\n#> #   Strasbourg <dbl>, Toulon <dbl>, Toulouse <dbl>\n\npopul.tidy <- popul %>% \n    pivot_longer(cols=-year,\n                 names_to = \"city\",\n                 values_to = \"pop\"\n                )\npopul.tidy\n\n#> # A tibble: 160 × 3\n#>     year city         pop\n#>    <dbl> <chr>      <dbl>\n#>  1  1962 Angers    115273\n#>  2  1962 Bordeaux  278403\n#>  3  1962 Brest     136104\n#>  4  1962 Dijon     135694\n#>  5  1962 Grenoble  156707\n#>  6  1962 LeHavre   187845\n#>  7  1962 LeMans    132181\n#>  8  1962 Lille     239955\n#>  9  1962 Lyon      535746\n#> 10  1962 Marseille 778071\n#> # … with 150 more rows\n\n# Create a subset containing the data for Montpellier\nmtp <- subset(popul.tidy, city == \"Montpellier\")\n# I prefer the tidyverse version\nmtp <- popul.tidy %>% filter(city == \"Montpellier\")\n# What is the max and min of population in this city?\nmax(mtp$pop)\n\n#> [1] 268456\n\nmin(mtp$pop)\n\n#> [1] 118864\n\nrange(mtp$pop)\n\n#> [1] 118864 268456\n\n# The average population over time?\nmean(mtp$pop)\n\n#> [1] 203114.4\n\n# What is the total population in 2012?\nsum(popul.tidy[popul.tidy$year == 2012, \"pop\"])\n\n#> [1] 7334805\n\npopul.tidy %>% \n    filter(year==2012) %>% \n    select(pop) %>% \n    sum()\n\n#> [1] 7334805\n\n# What is the total population per year?\npopul.tidy %>% \n    group_by(year) %>% \n    summarise(pop_tot=sum(pop))\n\n#> # A tibble: 8 × 2\n#>    year pop_tot\n#>   <dbl>   <dbl>\n#> 1  1962 7349547\n#> 2  1968 7550999\n#> 3  1975 7298183\n#> 4  1982 6933230\n#> 5  1990 6857291\n#> 6  1999 6965325\n#> 7  2007 7235114\n#> 8  2012 7334805\n\n# What is the average population per city over the years?\npopul.tidy %>% \n    group_by(city) %>% \n    summarise(pop_ave=mean(pop))\n\n#> # A tibble: 20 × 2\n#>    city           pop_ave\n#>    <chr>            <dbl>\n#>  1 Angers         138783.\n#>  2 Bordeaux       234815.\n#>  3 Brest          149125.\n#>  4 Dijon          146735.\n#>  5 Grenoble       157526.\n#>  6 LeHavre        193990.\n#>  7 LeMans         144347.\n#>  8 Lille          220018.\n#>  9 Lyon           470371.\n#> 10 Marseille      844253.\n#> 11 Montpellier    203114.\n#> 12 Nantes         260925.\n#> 13 Nice           334312.\n#> 14 Paris         2321032.\n#> 15 Reims          172278 \n#> 16 Rennes         193425.\n#> 17 Saint-Etienne  198135.\n#> 18 Strasbourg     255429.\n#> 19 Toulon         169683.\n#> 20 Toulouse       382265.\n\n\nExercise 5\n\nFirst, load the tidyverse and lubridate package\nLoad people1.csv and people2.csv into pp1 and pp2\n\nCreate a new tibble pp by using the pipe operator (%>%) and successively:\n\njoining the two tibbles into one using inner_join()\n\nadding a column age containing the age in years (use lubridate::time_length(x, 'years') with x a time difference in days) by using mutate()\n\n\n\nDisplay a summary of the table using str()\n\nUsing groupe_by() and summarize():\n\nShow the number of males and females in the table (use the counter n())\nShow the average age per gender\nShow the average size per gender and institution\nShow the number of people from each country, sorted by descending population (arrange())\n\n\nUsing select(), display:\n\nonly the name and age columns\nall but the name column\n\n\nUsing filter(), show data only for\n\nChinese people\nFrom institution ECL and UCBL\nPeople older than 22\nPeople with a e in their name\n\n\n\n\nSolution\n\n# First, load the `tidyverse` and `lubridate` package\nlibrary(tidyverse)\nlibrary(lubridate)\n# Load people1.csv and people2.csv\npp1  <- read_csv(\"Data/people1.csv\")\npp2  <- read_csv(\"Data/people2.csv\")\n# Create a new tibble `pp` by using the pipe operator (`%>%`)\n# and successively:\n# - joining the two tibbles into one using `inner_join()`\n# - adding a column `age` containing the age in years \n#   (use lubridate's `time_length(x, 'years')` with x a time\n#   difference in days) by using `mutate()`\npp <- pp1 %>% \n        inner_join(pp2) %>% \n        mutate(age=time_length(today()-dateofbirth,'years'))\n# Display a summary of the table using `str()`\nstr(pp)\n\n#> tibble [20 × 7] (S3: tbl_df/tbl/data.frame)\n#>  $ name       : chr [1:20] \"Salem\" \"Dilruwan-Shanaka-Perera\" \"Hanna\" \"Sabin\" ...\n#>  $ gender     : chr [1:20] \"Male\" \"Male\" \"Female\" \"Male\" ...\n#>  $ origin     : chr [1:20] \"Yemen\" \"Sri Lanka\" \"Ukraine\" \"India\" ...\n#>  $ institution: chr [1:20] \"UCBL\" \"INSA\" \"ECL\" \"INSA\" ...\n#>  $ dateofbirth: Date[1:20], format: \"1997-12-26\" \"1997-03-28\" ...\n#>  $ size       : num [1:20] 161 172 165 186 176 ...\n#>  $ age        : num [1:20] 25.1 25.8 25.1 27.5 27.8 ...\n\n# Using `groupe_by()` and `summarize()`:\n# - Show the number of males and females in the table \n#   (use the counter `n()`)\npp %>% group_by(gender) %>% summarize(count=n())\n\n#> # A tibble: 2 × 2\n#>   gender count\n#>   <chr>  <int>\n#> 1 Female     4\n#> 2 Male      16\n\n# - Show the average age per gender\npp %>% group_by(gender) %>% summarize(age=mean(age))\n\n#> # A tibble: 2 × 2\n#>   gender   age\n#>   <chr>  <dbl>\n#> 1 Female  27.1\n#> 2 Male    26.8\n\n# - Show the average size per gender and institution\npp %>% group_by(gender, institution) %>% summarize(size=mean(size))\n\n#> # A tibble: 4 × 3\n#> # Groups:   gender [2]\n#>   gender institution  size\n#>   <chr>  <chr>       <dbl>\n#> 1 Female ECL          178.\n#> 2 Male   ECL          168.\n#> 3 Male   INSA         174.\n#> 4 Male   UCBL         174.\n\n# - Show the number of people from each country, \n#   sorted by descending population\npp %>% group_by(origin) %>% \n        summarize(count=n()) %>% \n        arrange(desc(count))\n\n#> # A tibble: 13 × 2\n#>    origin      count\n#>    <chr>       <int>\n#>  1 China           4\n#>  2 Ukraine         4\n#>  3 USA             2\n#>  4 Afghanistan     1\n#>  5 Austria         1\n#>  6 Brazil          1\n#>  7 Colombia        1\n#>  8 Cyprus          1\n#>  9 India           1\n#> 10 Iran            1\n#> 11 Sri Lanka       1\n#> 12 Tunisia         1\n#> 13 Yemen           1\n\n# Using `select()`, display:\n# - only the name and age columns\npp %>% select(c(name, age))\n\n#> # A tibble: 20 × 2\n#>    name                      age\n#>    <chr>                   <dbl>\n#>  1 Salem                    25.1\n#>  2 Dilruwan-Shanaka-Perera  25.8\n#>  3 Hanna                    25.1\n#>  4 Sabin                    27.5\n#>  5 Benedikt                 27.8\n#>  6 Jordyn                   25.9\n#>  7 Jennifer                 27.7\n#>  8 Yiran                    28.0\n#>  9 Leran                    30.1\n#> 10 Aymen                    32.9\n#> 11 Pavlo                    25.8\n#> 12 Saulo                    28.3\n#> 13 Nicolas-Estevan          29.2\n#> 14 Farzad                   26.1\n#> 15 Roein                    23.5\n#> 16 Paraskevas               23.6\n#> 17 Ihor                     23.3\n#> 18 Iryna                    29.9\n#> 19 Peng                     26.1\n#> 20 Mingyuan                 25.4\n\n# - all but the name column\npp %>% select(-name)\n\n#> # A tibble: 20 × 6\n#>    gender origin      institution dateofbirth  size   age\n#>    <chr>  <chr>       <chr>       <date>      <dbl> <dbl>\n#>  1 Male   Yemen       UCBL        1997-12-26   161.  25.1\n#>  2 Male   Sri Lanka   INSA        1997-03-28   172.  25.8\n#>  3 Female Ukraine     ECL         1997-12-30   165.  25.1\n#>  4 Male   India       INSA        1995-08-04   186.  27.5\n#>  5 Male   Austria     UCBL        1995-04-25   176.  27.8\n#>  6 Female USA         ECL         1997-02-19   176.  25.9\n#>  7 Female USA         ECL         1995-05-28   179   27.7\n#>  8 Male   China       UCBL        1995-02-04   188.  28.0\n#>  9 Male   China       UCBL        1992-12-30   186   30.1\n#> 10 Male   Tunisia     INSA        1990-03-03   160.  32.9\n#> 11 Male   Ukraine     ECL         1997-04-12   151.  25.8\n#> 12 Male   Brazil      ECL         1994-09-24   184.  28.3\n#> 13 Male   Colombia    INSA        1993-11-25   184.  29.2\n#> 14 Male   Iran        INSA        1996-12-27   183   26.1\n#> 15 Male   Afghanistan INSA        1999-07-11   155.  23.5\n#> 16 Male   Cyprus      INSA        1999-06-25   176.  23.6\n#> 17 Male   Ukraine     ECL         1999-10-03   170.  23.3\n#> 18 Female Ukraine     ECL         1993-02-27   192   29.9\n#> 19 Male   China       UCBL        1996-12-14   171   26.1\n#> 20 Male   China       UCBL        1997-08-21   164.  25.4\n\n# Using `filter()`, show data only for\n# - Chinese people\npp %>% filter(origin=='China')\n\n#> # A tibble: 4 × 7\n#>   name     gender origin institution dateofbirth  size   age\n#>   <chr>    <chr>  <chr>  <chr>       <date>      <dbl> <dbl>\n#> 1 Yiran    Male   China  UCBL        1995-02-04   188.  28.0\n#> 2 Leran    Male   China  UCBL        1992-12-30   186   30.1\n#> 3 Peng     Male   China  UCBL        1996-12-14   171   26.1\n#> 4 Mingyuan Male   China  UCBL        1997-08-21   164.  25.4\n\n# - From institution ECL and UCBL\npp %>% filter(institution %in% c('ECL', 'UCBL'))\n\n#> # A tibble: 13 × 7\n#>    name     gender origin  institution dateofbirth  size   age\n#>    <chr>    <chr>  <chr>   <chr>       <date>      <dbl> <dbl>\n#>  1 Salem    Male   Yemen   UCBL        1997-12-26   161.  25.1\n#>  2 Hanna    Female Ukraine ECL         1997-12-30   165.  25.1\n#>  3 Benedikt Male   Austria UCBL        1995-04-25   176.  27.8\n#>  4 Jordyn   Female USA     ECL         1997-02-19   176.  25.9\n#>  5 Jennifer Female USA     ECL         1995-05-28   179   27.7\n#>  6 Yiran    Male   China   UCBL        1995-02-04   188.  28.0\n#>  7 Leran    Male   China   UCBL        1992-12-30   186   30.1\n#>  8 Pavlo    Male   Ukraine ECL         1997-04-12   151.  25.8\n#>  9 Saulo    Male   Brazil  ECL         1994-09-24   184.  28.3\n#> 10 Ihor     Male   Ukraine ECL         1999-10-03   170.  23.3\n#> 11 Iryna    Female Ukraine ECL         1993-02-27   192   29.9\n#> 12 Peng     Male   China   UCBL        1996-12-14   171   26.1\n#> 13 Mingyuan Male   China   UCBL        1997-08-21   164.  25.4\n\n# - People older than 22 \npp %>% filter(age>22)\n\n#> # A tibble: 20 × 7\n#>    name                    gender origin      institution dateofbi…¹  size   age\n#>    <chr>                   <chr>  <chr>       <chr>       <date>     <dbl> <dbl>\n#>  1 Salem                   Male   Yemen       UCBL        1997-12-26  161.  25.1\n#>  2 Dilruwan-Shanaka-Perera Male   Sri Lanka   INSA        1997-03-28  172.  25.8\n#>  3 Hanna                   Female Ukraine     ECL         1997-12-30  165.  25.1\n#>  4 Sabin                   Male   India       INSA        1995-08-04  186.  27.5\n#>  5 Benedikt                Male   Austria     UCBL        1995-04-25  176.  27.8\n#>  6 Jordyn                  Female USA         ECL         1997-02-19  176.  25.9\n#>  7 Jennifer                Female USA         ECL         1995-05-28  179   27.7\n#>  8 Yiran                   Male   China       UCBL        1995-02-04  188.  28.0\n#>  9 Leran                   Male   China       UCBL        1992-12-30  186   30.1\n#> 10 Aymen                   Male   Tunisia     INSA        1990-03-03  160.  32.9\n#> 11 Pavlo                   Male   Ukraine     ECL         1997-04-12  151.  25.8\n#> 12 Saulo                   Male   Brazil      ECL         1994-09-24  184.  28.3\n#> 13 Nicolas-Estevan         Male   Colombia    INSA        1993-11-25  184.  29.2\n#> 14 Farzad                  Male   Iran        INSA        1996-12-27  183   26.1\n#> 15 Roein                   Male   Afghanistan INSA        1999-07-11  155.  23.5\n#> 16 Paraskevas              Male   Cyprus      INSA        1999-06-25  176.  23.6\n#> 17 Ihor                    Male   Ukraine     ECL         1999-10-03  170.  23.3\n#> 18 Iryna                   Female Ukraine     ECL         1993-02-27  192   29.9\n#> 19 Peng                    Male   China       UCBL        1996-12-14  171   26.1\n#> 20 Mingyuan                Male   China       UCBL        1997-08-21  164.  25.4\n#> # … with abbreviated variable name ¹​dateofbirth\n\n# - People with a `e` in their name\npp %>% filter(grepl('e',name))\n\n#> # A tibble: 10 × 7\n#>    name                    gender origin      institution dateofbi…¹  size   age\n#>    <chr>                   <chr>  <chr>       <chr>       <date>     <dbl> <dbl>\n#>  1 Salem                   Male   Yemen       UCBL        1997-12-26  161.  25.1\n#>  2 Dilruwan-Shanaka-Perera Male   Sri Lanka   INSA        1997-03-28  172.  25.8\n#>  3 Benedikt                Male   Austria     UCBL        1995-04-25  176.  27.8\n#>  4 Jennifer                Female USA         ECL         1995-05-28  179   27.7\n#>  5 Leran                   Male   China       UCBL        1992-12-30  186   30.1\n#>  6 Aymen                   Male   Tunisia     INSA        1990-03-03  160.  32.9\n#>  7 Nicolas-Estevan         Male   Colombia    INSA        1993-11-25  184.  29.2\n#>  8 Roein                   Male   Afghanistan INSA        1999-07-11  155.  23.5\n#>  9 Paraskevas              Male   Cyprus      INSA        1999-06-25  176.  23.6\n#> 10 Peng                    Male   China       UCBL        1996-12-14  171   26.1\n#> # … with abbreviated variable name ¹​dateofbirth\n\n\n\nFor more interesting exercises in the tidyverse, look at:\n\nCO2 emissions: data wrangling and ggplot2\nReligion and babies: data handling, ggplot2 and plotly\nCOVID-19: data wrangling, ggplot2\nNanoparticles statistics from SEM images: data wrangling, ggplot2 and fitting"
  },
  {
    "objectID": "08-reading_files.html#reading-files",
    "href": "08-reading_files.html#reading-files",
    "title": "\n7  Reading/writing all kinds of files\n",
    "section": "\n7.1 Reading files",
    "text": "7.1 Reading files\nWorking in any data-based scientific field, you will encounter many different types of files. ASCII text files are usually predominant, but you may want to read files from coming from Excel, Origin, etc. Here is a non-exhaustive reminder to help you read the kinds of files you often encounter with R. I invite you to visit the RopenSci webpage for more packages, and if this isn’t enough for your need, well, Google is your friend.\n\n7.1.1 Column text files\nMost base R functions like read.csv() or read.table() have their tidyverse counterpart, like read_csv() and read_table(). I usually prefer the tidyverse version as it outputs a tibble instead of a data.frame. These functions can also be directly provided with an url to the text file.\nIf your data file is “complicated” – in the sense that it contains lines or columns to be skipped – look at the help on the reader function with ?function_name.\n\n7.1.1.1 Comma separated values\nInput file looks like this:\n\n\n#> Country Name,Country Code,Year,Value\n#> Arab World,ARB,1960,92490932\n#> Arab World,ARB,1961,95044497\n#> Arab World,ARB,1962,97682294\n#> Arab World,ARB,1963,100411076\n#> Arab World,ARB,1964,103239902\n#> Arab World,ARB,1965,106174988\n#> Arab World,ARB,1966,109230593\n#> Arab World,ARB,1967,112406932\n#> Arab World,ARB,1968,115680165\n\n\n\nlibrary(tidyverse)\nread_csv(\"Data/tot_population.csv\")\n\n#> # A tibble: 14,885 × 4\n#>    `Country Name` `Country Code`  Year     Value\n#>    <chr>          <chr>          <dbl>     <dbl>\n#>  1 Arab World     ARB             1960  92490932\n#>  2 Arab World     ARB             1961  95044497\n#>  3 Arab World     ARB             1962  97682294\n#>  4 Arab World     ARB             1963 100411076\n#>  5 Arab World     ARB             1964 103239902\n#>  6 Arab World     ARB             1965 106174988\n#>  7 Arab World     ARB             1966 109230593\n#>  8 Arab World     ARB             1967 112406932\n#>  9 Arab World     ARB             1968 115680165\n#> 10 Arab World     ARB             1969 119016542\n#> # … with 14,875 more rows\n\nread_csv(\"Data/tot_population.csv\", skip = 1)\n\n#> # A tibble: 14,884 × 4\n#>    `Arab World` ARB   `1960` `92490932`\n#>    <chr>        <chr>  <dbl>      <dbl>\n#>  1 Arab World   ARB     1961   95044497\n#>  2 Arab World   ARB     1962   97682294\n#>  3 Arab World   ARB     1963  100411076\n#>  4 Arab World   ARB     1964  103239902\n#>  5 Arab World   ARB     1965  106174988\n#>  6 Arab World   ARB     1966  109230593\n#>  7 Arab World   ARB     1967  112406932\n#>  8 Arab World   ARB     1968  115680165\n#>  9 Arab World   ARB     1969  119016542\n#> 10 Arab World   ARB     1970  122398374\n#> # … with 14,874 more rows\n\nread_csv(\"Data/tot_population.csv\", skip = 1, col_names = LETTERS[1:4])\n\n#> # A tibble: 14,885 × 4\n#>    A          B         C         D\n#>    <chr>      <chr> <dbl>     <dbl>\n#>  1 Arab World ARB    1960  92490932\n#>  2 Arab World ARB    1961  95044497\n#>  3 Arab World ARB    1962  97682294\n#>  4 Arab World ARB    1963 100411076\n#>  5 Arab World ARB    1964 103239902\n#>  6 Arab World ARB    1965 106174988\n#>  7 Arab World ARB    1966 109230593\n#>  8 Arab World ARB    1967 112406932\n#>  9 Arab World ARB    1968 115680165\n#> 10 Arab World ARB    1969 119016542\n#> # … with 14,875 more rows\n\n\nSince version 2.0 of readr, read_csv() can also take a vector as argument, which will result in reading all files in the vector successively:\n\nread_csv(c(\"Data/test1.csv\",\n           \"Data/test2.csv\"),\n         id = \"file\", show_col_types = FALSE)\n\n#> # A tibble: 4 × 3\n#>   file           name  value\n#>   <chr>          <chr> <dbl>\n#> 1 Data/test1.csv John      3\n#> 2 Data/test1.csv Doe       2\n#> 3 Data/test2.csv Colin     5\n#> 4 Data/test2.csv Louis     8\n\n\n\n7.1.1.2 Space separated values\nInput file looks like this:\n\n\n#> 3063.7136    43.916748\n#> 3063.991 47.916748\n#> 3064.2668    44.5\n#> 3064.5442    50.5\n#> 3064.8201    50.5\n#> 3065.0972    44.5\n#> 3065.373 44.916748\n#> 3065.6504    39.916748\n#> 3065.9263    49.5\n#> 3066.2034    48.916748\n\n\nlibrary(tidyverse)\nread_table(\"Data/rubis_01.txt\")\nread_table(\"Data/rubis_01.txt\", col_names = c(\"w\",\"int\"))\n\n7.1.1.3 Other separators\nFor tab-separated values, use read_tsv(). For other exotic separators, look into read_delim().\n\n7.1.2 Excel files\nFor this, use the readxl library and its function read_excel() returning a tibble:\n\nlibrary(readxl)\nread_excel(\"Data/test.xlsx\")\n\n#> # A tibble: 10 × 2\n#>        x      y\n#>    <dbl>  <dbl>\n#>  1     1  5.21 \n#>  2     2  6.55 \n#>  3     3  3.71 \n#>  4     4  0.216\n#>  5     5  0.205\n#>  6     6  4.60 \n#>  7     7 10.3  \n#>  8     8 12.9  \n#>  9     9 11.1  \n#> 10    10  7.28\n\nread_excel(\"Data/test.xlsx\", sheet=2) # specify the sheet by its number or its name\n\n#> # A tibble: 4 × 2\n#>   hello  world   \n#>   <chr>  <chr>   \n#> 1 ac     th      \n#> 2 asc    thh     \n#> 3 ascsa  dthdh   \n#> 4 ascacs dthtdhdh\n\n\nIn case your Excel file contains merged cells, read_excel() will fill the merged cells by NA values. If you want to avoid this behavior, use openxlsx::read.xlsx() (which returns a data.frame):\n\nread_excel(\"Data/test.xlsx\", sheet=3)\n\n#> # A tibble: 5 × 3\n#>   a         b ...3 \n#>   <chr> <dbl> <chr>\n#> 1 <NA>     12 t    \n#> 2 <NA>     13 h    \n#> 3 <NA>     14 d    \n#> 4 b        15 f    \n#> 5 <NA>     16 g\n\nlibrary(openxlsx)\nread.xlsx(\"Data/test.xlsx\", fillMergedCells = TRUE, sheet = 3)\n\n#>   a  b b\n#> 1 a 12 t\n#> 2 a 13 h\n#> 3 a 14 d\n#> 4 b 15 f\n#> 5 b 16 g\n\n\n\n7.1.3 Origin files\nIf you moved to R coming from a workflow where you used Origin, chances are you have some .opj files lying around that you still want to be able to read. Lucky you, the Ropj library is here:\nlibrary(Ropj)\nread.opj(\"Data/opjfile.opj\")\n\n7.1.4 Matlab files\nTo read Matlab’s .mat format datasets, use the R.matlab package and its readMat() function.\nlibrary(R.matlab)\ndf <- readMat(\"yourfile.mat\")\n\n7.1.5 Images\nYou can read an image as a matrix. For example:\nlibrary(png)\nreadPNG(\"image.png\")\nlibrary(tiff)\nreadTIFF(\"image.tiff\")\nFor more image processing (pictures or videos), I recommend the imager package.\n\n7.1.6 Spectroscopic files\nIn case your spectroscopic data wasn’t saved as an ASCII file but as a spc or another format, take a look at the lightr package, for example.\nExample of a reading function for an spc file:\nlibrary(tidyverse)\nlibrary(lightr)\nread_spc <- function(fname){\n    d <- lr_parse_spc(fname) # d is a list\n    tibble(w         = d[[1]]$wl, \n           intensity = d[[1]]$processed)\n}\n\n7.1.7 Compressed binary data files: HDF, netCDF\nHDF: Go to this vignette to see how to read Hierarchical Data Files.\nnetCDF: see here."
  },
  {
    "objectID": "08-reading_files.html#reading-multiple-files-into-a-tidy-table",
    "href": "08-reading_files.html#reading-multiple-files-into-a-tidy-table",
    "title": "\n7  Reading/writing all kinds of files\n",
    "section": "\n7.2 Reading multiple files into a tidy table",
    "text": "7.2 Reading multiple files into a tidy table\nWe very often encounter the situation where we need to read multiple similar files into a tidy table. For this, you can use a for loop: this would work but would be un-R-ly, but in some cases you need to do this because you perform some other operations during the for loop.\nLet’s say we store the list of file names into a vector file_list and read these files using the function read_function():\nlibrary(tidyverse)\ndf <- tibble() # empty initialization\nfor (file in file_list) {\n    df_temp <- read_function(file) %>% \n            mutate(name = file) # add the column `name` to make the tibble tidy\n    df <- bind_rows(df, def_temp)\n}\nAn R-friendly way of doing this would be to avoid using a for loop:\nlibrary(tidyverse)\ndf <- tibble(name = file_list) %>% \n    mutate(data = map(name, read_function)) %>% \n    unnest(data)\nIn case you are reading csv files, you can use the fact that since the version 2.0 of readr, read_csv() takes a vector as first argument. You’ll also need to use the id argument to get a column with the list of file names:\nlibrary(tidyverse)\ndf <- read_csv(file_list, id = \"name\")"
  },
  {
    "objectID": "08-reading_files.html#writing-files",
    "href": "08-reading_files.html#writing-files",
    "title": "\n7  Reading/writing all kinds of files\n",
    "section": "\n7.3 Writing files",
    "text": "7.3 Writing files\n\n7.3.1 Text files\nSometimes, you want to output your data as a csv or an Excel file to share it with others or to save your data. Use the write_csv() function to write a csv file (prefer the tidyverse’s write_csv() to the base R write.csv() as it is more easy to use):\nlibrary(tidyverse)\nwrite_csv(df, \"your_file.csv\")\nNote that the write_*() functions will automatically compress outputs if an appropriate extension is given. Three extensions are currently supported: .gz for gzip compression, .bz2 for bzip2 compression and .xz for lzma compression. See the examples in the help for more information.\nIf you don’t want to use csv files, look into write_tsv() for tab-separated values or write_delim() for any delimiter. In case you want to output fixed width files, look into gdata’s write.fwf().\n\n7.3.2 Excel files\nTo write Excel files, use the library openxlsx and its function write.xlsx() (see the help on the function for more options):\nlibrary(openxlsx)\nwrite.xlsx(df, \"your_file.xlsx\")"
  },
  {
    "objectID": "09-lists.html#definition",
    "href": "09-lists.html#definition",
    "title": "\n8  Lists\n",
    "section": "\n8.1 Definition",
    "text": "8.1 Definition\nLists allow you to store all types of objects and types of values: booleans, doubles, characters, vectors, other lists, data.frame, etc\n\n# initialization\nL <- list(name = \"John\",\n          age  = 43,\n          kids = list(name=c(\"Kevin\", \"Pamela\"), # nested list\n                      age =c(4,5)\n                     )\n         )\nL\n\n#> $name\n#> [1] \"John\"\n#> \n#> $age\n#> [1] 43\n#> \n#> $kids\n#> $kids$name\n#> [1] \"Kevin\"  \"Pamela\"\n#> \n#> $kids$age\n#> [1] 4 5\n\n# names of entries (can be changed)\nnames(L)\n\n#> [1] \"name\" \"age\"  \"kids\"\n\n# statistics\nsummary(L)\n\n#>      Length Class  Mode     \n#> name 1      -none- character\n#> age  1      -none- numeric  \n#> kids 2      -none- list\n\nstr(L)\n\n#> List of 3\n#>  $ name: chr \"John\"\n#>  $ age : num 43\n#>  $ kids:List of 2\n#>   ..$ name: chr [1:2] \"Kevin\" \"Pamela\"\n#>   ..$ age : num [1:2] 4 5"
  },
  {
    "objectID": "09-lists.html#accessing-values-and-other-operations",
    "href": "09-lists.html#accessing-values-and-other-operations",
    "title": "\n8  Lists\n",
    "section": "\n8.2 Accessing values and other operations",
    "text": "8.2 Accessing values and other operations\n\nL$name # is a vector\n\n#> [1] \"John\"\n\nL[\"age\"];typeof(L[\"age\"])     # is a list\n\n#> $age\n#> [1] 43\n\n\n#> [1] \"list\"\n\nL[[\"age\"]];typeof(L[[\"age\"]]) # is a vector\n\n#> [1] 43\n\n\n#> [1] \"double\"\n\nL[[3]]      # is a list (because 'kids' is a list)\n\n#> $name\n#> [1] \"Kevin\"  \"Pamela\"\n#> \n#> $age\n#> [1] 4 5\n\nL[[3]]['name']   # is a list\n\n#> $name\n#> [1] \"Kevin\"  \"Pamela\"\n\nL[[3]][['name']] # is a vector\n\n#> [1] \"Kevin\"  \"Pamela\"\n\n# empty initialization\nLL <- list(); LL # no specific size\n\n#> list()\n\nLL <- vector(\"list\", length=3); LL # specific size\n\n#> [[1]]\n#> NULL\n#> \n#> [[2]]\n#> NULL\n#> \n#> [[3]]\n#> NULL\n\n# Concatenation\nL1 <- list(wife=\"Kim\", wife.age=38)\nL2 <- c(L, L1)\ntypeof(L2); L2\n\n#> [1] \"list\"\n\n\n#> $name\n#> [1] \"John\"\n#> \n#> $age\n#> [1] 43\n#> \n#> $kids\n#> $kids$name\n#> [1] \"Kevin\"  \"Pamela\"\n#> \n#> $kids$age\n#> [1] 4 5\n#> \n#> \n#> $wife\n#> [1] \"Kim\"\n#> \n#> $wife.age\n#> [1] 38"
  },
  {
    "objectID": "09-lists.html#exo-lists",
    "href": "09-lists.html#exo-lists",
    "title": "\n8  Lists\n",
    "section": "\n8.3 Exercises",
    "text": "8.3 Exercises\n\nCreate a list containing 2 strings, 2 numbers, 2 vectors, 1 list and 2 logical values.\nGive names to the elements in the list.\nAccess the first and second elements of the list.\nAdd a new item g4 = \"Hello\" to the list.\nSelect the second element of the nested list.\nRemove the second element of the list.\nCreate a second list with whatever you want\nMerge the two lists into one list.\nPrint the number of objects in the merged list.\nConvert list(1,2,3,4) to a vector\n\n\nSolution\n\n# Create a list containing 2 strings, 2 numbers, 2 vectors, 1 list and 2 logical values.\n# Give names to the elements in the list.\nfirst_list <- list(string1 = \"foo\",\n                   string2 = \"bar\",\n                   number1 = 42,\n                   number2 = pi,\n                   vec1    = seq(-10,10,1),\n                   vec2    = c(\"Hello\", \"world\"),\n                   list1   = list(a = 1:10, \n                                  b = 10:1),\n                   bool1   = TRUE,\n                   bool2   = FALSE\n                  )\nfirst_list\n\n#> $string1\n#> [1] \"foo\"\n#> \n#> $string2\n#> [1] \"bar\"\n#> \n#> $number1\n#> [1] 42\n#> \n#> $number2\n#> [1] 3.141593\n#> \n#> $vec1\n#>  [1] -10  -9  -8  -7  -6  -5  -4  -3  -2  -1   0   1   2   3   4   5   6   7   8\n#> [20]   9  10\n#> \n#> $vec2\n#> [1] \"Hello\" \"world\"\n#> \n#> $list1\n#> $list1$a\n#>  [1]  1  2  3  4  5  6  7  8  9 10\n#> \n#> $list1$b\n#>  [1] 10  9  8  7  6  5  4  3  2  1\n#> \n#> \n#> $bool1\n#> [1] TRUE\n#> \n#> $bool2\n#> [1] FALSE\n\n# Access the first and second elements of the list.\nfirst_list[[1]]\n\n#> [1] \"foo\"\n\nfirst_list[[\"string2\"]]\n\n#> [1] \"bar\"\n\n# Add a new item `g4 = \"Hello\"` to the list.\nfirst_list$g4 <- \"Hello\"\nfirst_list\n\n#> $string1\n#> [1] \"foo\"\n#> \n#> $string2\n#> [1] \"bar\"\n#> \n#> $number1\n#> [1] 42\n#> \n#> $number2\n#> [1] 3.141593\n#> \n#> $vec1\n#>  [1] -10  -9  -8  -7  -6  -5  -4  -3  -2  -1   0   1   2   3   4   5   6   7   8\n#> [20]   9  10\n#> \n#> $vec2\n#> [1] \"Hello\" \"world\"\n#> \n#> $list1\n#> $list1$a\n#>  [1]  1  2  3  4  5  6  7  8  9 10\n#> \n#> $list1$b\n#>  [1] 10  9  8  7  6  5  4  3  2  1\n#> \n#> \n#> $bool1\n#> [1] TRUE\n#> \n#> $bool2\n#> [1] FALSE\n#> \n#> $g4\n#> [1] \"Hello\"\n\n# Select the second element of the nested list.\nfirst_list[[\"list1\"]][[2]]\n\n#>  [1] 10  9  8  7  6  5  4  3  2  1\n\n# Remove the second element of the list.\nfirst_list[-2]\n\n#> $string1\n#> [1] \"foo\"\n#> \n#> $number1\n#> [1] 42\n#> \n#> $number2\n#> [1] 3.141593\n#> \n#> $vec1\n#>  [1] -10  -9  -8  -7  -6  -5  -4  -3  -2  -1   0   1   2   3   4   5   6   7   8\n#> [20]   9  10\n#> \n#> $vec2\n#> [1] \"Hello\" \"world\"\n#> \n#> $list1\n#> $list1$a\n#>  [1]  1  2  3  4  5  6  7  8  9 10\n#> \n#> $list1$b\n#>  [1] 10  9  8  7  6  5  4  3  2  1\n#> \n#> \n#> $bool1\n#> [1] TRUE\n#> \n#> $bool2\n#> [1] FALSE\n#> \n#> $g4\n#> [1] \"Hello\"\n\n# Create a second list with whatever you want\nsecond_list <- list(a=1:10, b=1:10, c=\"hello\")\n# Merge the two lists into one list.\none_list <- c(first_list, second_list)\n# Print the number of objects in the merged list.\nlength(one_list)\n\n#> [1] 13\n\n# Convert `list(1,2,3,4)` to a vector\nas.numeric(list(1,2,3,4))\n\n#> [1] 1 2 3 4"
  },
  {
    "objectID": "10-functions.html#definition",
    "href": "10-functions.html#definition",
    "title": "\n9  Functions\n",
    "section": "\n9.1 Definition",
    "text": "9.1 Definition\nTo get the manual on a base function, type ?function_name.\nA function returns the last thing that is called in it. Thus a function defined like that will return nothing:\n\ngeom_mean <- function(x, y){\n    a <- sqrt(x*y)\n}\ngeom_mean(1,2)\n\nWhile either of these three functions will return what you want:\n\ngeom_mean1 <- function(x, y){\n    a <- sqrt(x*y)\n    a\n}\ngeom_mean2 <- function(x, y){\n    a <- sqrt(x*y)\n    return(a)\n}\ngeom_mean3 <- function(x, y){\n    sqrt(x*y)\n}\ngeom_mean1(1,2) #returns a\n\n#> [1] 1.414214\n\n\nWhile an explicit call to return() is not necessary, I think it’s usually a good practise to write it explicitly to make clearer to the reader what the function actually returns.\nThe result of a function can be a list, a data.frame, a vector… or nothing. And you can attribute the result of a function to a variable:\n\nperson <- function(name, age){\n    list(name=name, age=age)\n}\njoe <- person(name=\"Joe\", age=33)\njoe\n\n#> $name\n#> [1] \"Joe\"\n#> \n#> $age\n#> [1] 33"
  },
  {
    "objectID": "10-functions.html#default-values",
    "href": "10-functions.html#default-values",
    "title": "\n9  Functions\n",
    "section": "\n9.2 Default values",
    "text": "9.2 Default values\nOne can add default values to variables by specifying them in the function definition:\n\ntestfunc <- function(x, y=1){\n    x*y\n}\ntestfunc(1)\n\n#> [1] 1\n\ntestfunc(1, y=2)\n\n#> [1] 2\n\ntestfunc(1:3, y=.1)\n\n#> [1] 0.1 0.2 0.3"
  },
  {
    "objectID": "10-functions.html#passing-arguments-to-other-functions",
    "href": "10-functions.html#passing-arguments-to-other-functions",
    "title": "\n9  Functions\n",
    "section": "\n9.3 Passing arguments to other functions",
    "text": "9.3 Passing arguments to other functions\nOne can also pass arguments to other functions within the function, without have to specify all of them:\n\ntestfunc2 <- function(x, ...){\n    y <- sin(x)\n    plot(x, y, ...)\n}\n# Here, `...` represents all the arguments you could provide to `plot()`\ntestfunc2(1:100)\ntestfunc2(1:100, type=\"l\")\n\n\n\n\n\n\n\n\n\n\n\nYou can also pass a function as argument:\n\ntestfunc3 <- function(FUN, ...){\n    FUN(...)\n}\ntestfunc3(sum, 1:10)\n\n#> [1] 55\n\ntestfunc3(plot, 1:10, sin(1:10), type=\"l\")"
  },
  {
    "objectID": "10-functions.html#error-handling",
    "href": "10-functions.html#error-handling",
    "title": "\n9  Functions\n",
    "section": "\n9.4 Error handling",
    "text": "9.4 Error handling\nIt is good practise to handle possible errors in your functions, especially if you share them with others. As a general rule of thumb, you can be sure that someone will always be able to find the one case where your function does not work or works in an unexpected way.\nFor this, the easiest way is to use if() statements together with warning(\"Warning message\") or stop(\"Stopping message\"), to either raise a warning while continuing the function, or stop the function altogether with an explanatory message. Alternatively and for ease of reading, the statement if(condition) stop(\"Stopping message\") can be replaced by stopifnot(condition is true).\nEquivalent example with stop() and stopifnot():\n\n\n\ntest <- function(x, y) {\n    if(!is.numeric(x) | !is.numeric(y)){\n        stop(\"Both x and y should numeric vectors, buddy!\")\n    }\n    if(length(x) != length(y)){\n        if(length(x) == 1 | length(y) == 1){\n            warning(glue::glue(\"x has length {length(x)}, while y has length {length(y)}, are you sure?\"))\n        }else{\n            stop(glue::glue(\"Both x and y should have same length or length of 1.\\n\n            Here x has length {length(x)}, while y has length {length(y)}.\"))\n        }\n    }\n    return(x*sqrt(y))\n}\n\n\ntest(x=1:2,y=1)\n\n#> [1] 1 2\n\ntest(x=1:2,y=\"A\")\n\n#> Error in test(x = 1:2, y = \"A\"): Both x and y should numeric vectors, buddy!\n\ntest(x=1:2,y=3:4)\n\n#> [1] 1.732051 4.000000\n\ntest(x=1:2,y=3:10)\n\n#> Error in test(x = 1:2, y = 3:10): Both x and y should have same length or length of 1.\n#> \n#> Here x has length 2, while y has length 8.\n\n\n\n\n\n\ntest <- function(x, y) {\n    # the condition is the opposite from the one used in stop():\n    stopifnot(is.numeric(x), is.numeric(y))\n    stopifnot(length(x) == length(y) | length(x) == 1 | length(y) == 1)\n    if(length(x) != length(y) & (length(x) == 1 | length(y) == 1)){\n        warning(glue::glue(\"x has length {length(x)}, while y has length {length(y)}, are you sure?\"))\n    }\n    return(x*sqrt(y))\n}\n\n\n\ntest(x=1:2,y=1)\n\n#> [1] 1 2\n\ntest(x=1:2,y=\"A\")\n\n#> Error in test(x = 1:2, y = \"A\"): is.numeric(y) is not TRUE\n\ntest(x=1:2,y=3:4)\n\n#> [1] 1.732051 4.000000\n\ntest(x=1:2,y=3:10)\n\n#> Error in test(x = 1:2, y = 3:10): length(x) == length(y) | length(x) == 1 | length(y) == 1 is not TRUE"
  },
  {
    "objectID": "10-functions.html#documenting-your-function",
    "href": "10-functions.html#documenting-your-function",
    "title": "\n9  Functions\n",
    "section": "\n9.5 Documenting your function",
    "text": "9.5 Documenting your function\nIt is also good practise to document your function by describing its purpose, its input parameters, its output, and some use cases. For this, the syntax is as follows:\n\n#' Title of the function\n#'\n#' Description of the function\n#' \n#' @param param1 Description of param1\n#' @param param2 Description of param2\n#'\n#' @return Description of the output\n#'\n#' @examples\n#' # Some examples on how to use the function\nmy_func <- function(param1, param2) {\n    # do some stuff\n}\n\nRStudio simplifies this process with a tool available in Code > Insert Roxygen skeleton (just place your cursor within the {} of the function before clicking this)."
  },
  {
    "objectID": "10-functions.html#interpolation-of-data",
    "href": "10-functions.html#interpolation-of-data",
    "title": "\n9  Functions\n",
    "section": "\n9.6 Interpolation of data",
    "text": "9.6 Interpolation of data\nIt is possible to interpolate data through the approxfun() and splinefun() functions: while the former uses linear interpolation, the latter uses cubic splines (polynomials).\n\nxmin <- -2*pi; xmax <- 2*pi\nx  <- runif(30, min=xmin, max=xmax)\nxx <- seq(xmin, xmax, .1)\ny  <- sin(x)\n# Linear interpolation\nlin_interp <- approxfun(x,y)        #is a function\nlin_interp(0); lin_interp(pi)\n\n#> [1] 0.02202484\n\n\n#> [1] -1.201713e-05\n\nyy_lin     <- approx(x, y, xout=xx) #is a list containing x and y\n\n# Cubic spline interpolation\nspl_interp <- splinefun(x,y)        #is a function\nyy_spl     <- spline(x, y, xout=xx) #is a list containing x and y\n\npar(family = \"Helvetica\", cex.lab=1.5, cex.axis=1.4, \n    mgp = c(2.4, .5, 0), tck=0.02, mar=c(4, 4, 2, .5), lwd=2, las=1)\nplot(x, y, pch=16, cex=2, ylim=c(-1,1.6))\nlines(xx, lin_interp(xx), col=\"royalblue\") # equivalent to: lines(yy_lin, col=\"royalblue\")\nlines(xx, spl_interp(xx), col=\"red\", lty=2)# equivalent to: lines(yy_spl, col=\"red\")\nlegend(\"topright\",\n    cex=1.2,\n    lty=c(NA, 1, 2),\n    lwd=c(NA, 2, 2),\n    pch=c(16, NA, NA),\n    col=c(\"black\", \"royalblue\", \"red\"),\n    bty = \"n\",\n    legend=c(\"'Experimental' points\", \"Linear interpolation\", \"Spline interpolation\")\n    )"
  },
  {
    "objectID": "10-functions.html#exo-functions",
    "href": "10-functions.html#exo-functions",
    "title": "\n9  Functions\n",
    "section": "\n9.7 Exercises",
    "text": "9.7 Exercises\nDownload the archive with all the exercises files, unzip it in your R class RStudio project, and edit the R files."
  },
  {
    "objectID": "11-conditions_loops.html#conditional-actions",
    "href": "11-conditions_loops.html#conditional-actions",
    "title": "\n10  Conditional actions and loops\n",
    "section": "\n10.1 Conditional actions",
    "text": "10.1 Conditional actions\nConditional actions in R can be determined through the usual if then else statements:\n\nx <- 1; y <- 2\nif(x>y){\n    print(\"x is larger than y\")\n}else if(x<y){\n    print(\"x is smaller than y\")\n}else{\n    print(\"x is equal to y\")\n}\n\n#> [1] \"x is smaller than y\"\n\n\nSometimes, it’s usefull to be able to do this in one line using ifesle(test, yes, no):\n\nx <- 3:7\nifelse(x>5, \"larger than 5\", \"lower than 5\") \n\n#> [1] \"lower than 5\"  \"lower than 5\"  \"lower than 5\"  \"larger than 5\"\n#> [5] \"larger than 5\""
  },
  {
    "objectID": "11-conditions_loops.html#loops",
    "href": "11-conditions_loops.html#loops",
    "title": "\n10  Conditional actions and loops\n",
    "section": "\n10.2 Loops…",
    "text": "10.2 Loops…\nLoops in R are provided through the usual for and while keywords:\n\n# For loop\nfor(i in 1:100){\n    # pass to next index directly\n    if(i %in% c(3,8,5)) next \n    # break loop\n    if(i==10) break\n    print(i)\n}\n\n#> [1] 1\n#> [1] 2\n#> [1] 4\n#> [1] 6\n#> [1] 7\n#> [1] 9\n\nphrase <- c(\"hello\", \"world\")\nfor(word in phrase){\n    print(word)\n}\n\n#> [1] \"hello\"\n#> [1] \"world\"\n\n# While loop\ni <- 1\nwhile(i<8){\n    print(i)\n    i <- i+2\n}\n\n#> [1] 1\n#> [1] 3\n#> [1] 5\n#> [1] 7"
  },
  {
    "objectID": "11-conditions_loops.html#and-how-to-avoid-them",
    "href": "11-conditions_loops.html#and-how-to-avoid-them",
    "title": "\n10  Conditional actions and loops\n",
    "section": "\n10.3 … and how to avoid them",
    "text": "10.3 … and how to avoid them\nHowever, since R is a vectorized language, it means that loops are to be avoided when possible because they are very inefficient:\n\nforloop <- function(x){\n    for(i in seq_along(x)){\n        x[i] <- 2*x[i]\n    }\n    x\n}\nnoforloop <- function(x){\n    2*x\n}\nx <- runif(1e7)\nmicrobenchmark::microbenchmark(\n    forloop   = forloop(x),\n    noforloop = noforloop(x),\n    times = 10L\n)\n\n#> Unit: milliseconds\n#>       expr       min        lq      mean    median       uq       max neval\n#>    forloop 463.78964 570.22501 613.63336 605.60541 619.4871 830.90103    10\n#>  noforloop  12.45316  13.10397  21.69227  23.97474  25.9687  29.90477    10\n\n\n\n10.3.1 The apply family\nAvoiding loops should therefore be sought for when possible. R helps us in this way through the base functions apply(), sapply() and lapply(). Operations in the tidyverse are also a very good way of avoiding loops.\nTake a look at the help on these functions, but the summary is that apply(df, direction, function) applies a function in the wanted direction (1 for rows, 2 for columns) of the given data.frame (or vector). Example:\n\nlibrary(tibble)\ndt <- tibble(x=1:5, y=x^2, z=x^3);dt\n\n#> # A tibble: 5 × 3\n#>       x     y     z\n#>   <int> <dbl> <dbl>\n#> 1     1     1     1\n#> 2     2     4     8\n#> 3     3     9    27\n#> 4     4    16    64\n#> 5     5    25   125\n\napply(dt, 1, mean) # mean of the rows\n\n#> [1]  1.000000  4.666667 13.000000 28.000000 51.666667\n\napply(dt, 2, mean) # mean of the columns\n\n#>  x  y  z \n#>  3 11 45\n\n# row/column means\nrowMeans(dt)\n\n#> [1]  1.000000  4.666667 13.000000 28.000000 51.666667\n\ncolMeans(dt)\n\n#>  x  y  z \n#>  3 11 45\n\n\nlapply() (and equivalently, sapply()) is basically the same thing but applied to lists and it returns a list (a vector):\n\nmy_list <- list(dt/3, dt/5);my_list\n\n#> [[1]]\n#>           x         y          z\n#> 1 0.3333333 0.3333333  0.3333333\n#> 2 0.6666667 1.3333333  2.6666667\n#> 3 1.0000000 3.0000000  9.0000000\n#> 4 1.3333333 5.3333333 21.3333333\n#> 5 1.6666667 8.3333333 41.6666667\n#> \n#> [[2]]\n#>     x   y    z\n#> 1 0.2 0.2  0.2\n#> 2 0.4 0.8  1.6\n#> 3 0.6 1.8  5.4\n#> 4 0.8 3.2 12.8\n#> 5 1.0 5.0 25.0\n\nlapply(my_list, \"[\", 1, )  # print first row\n\n#> [[1]]\n#>           x         y         z\n#> 1 0.3333333 0.3333333 0.3333333\n#> \n#> [[2]]\n#>     x   y   z\n#> 1 0.2 0.2 0.2\n\nsapply(my_list, rowSums)   # sum on rows\n\n#>           [,1] [,2]\n#> [1,]  1.000000  0.6\n#> [2,]  4.666667  2.8\n#> [3,] 13.000000  7.8\n#> [4,] 28.000000 16.8\n#> [5,] 51.666667 31.0\n\nlapply(my_list, round, 1)  # round to first decimal\n\n#> [[1]]\n#>     x   y    z\n#> 1 0.3 0.3  0.3\n#> 2 0.7 1.3  2.7\n#> 3 1.0 3.0  9.0\n#> 4 1.3 5.3 21.3\n#> 5 1.7 8.3 41.7\n#> \n#> [[2]]\n#>     x   y    z\n#> 1 0.2 0.2  0.2\n#> 2 0.4 0.8  1.6\n#> 3 0.6 1.8  5.4\n#> 4 0.8 3.2 12.8\n#> 5 1.0 5.0 25.0\n\n# For more complex operations, use it this way:\nsapply(1:nrow(dt), function(i){\n    dt$x[i] + dt$y[(i+2)%%nrow(dt)+1] - dt$z[(i+4)%%nrow(dt)+1]\n})\n\n#> [1]   16   19  -23  -56 -111\n\n\n\n10.3.2 The tidyverse way\nThe package tidyverse offers numerous ways to avoid explicit for loops. To see how to do this, refer to the section on operations in the tidyverse."
  },
  {
    "objectID": "11-conditions_loops.html#exo-loops",
    "href": "11-conditions_loops.html#exo-loops",
    "title": "\n10  Conditional actions and loops\n",
    "section": "\n10.4 Exercises",
    "text": "10.4 Exercises\nExercise 1\nGiven x <- runif(1e3, min=-1, max=1), create a tibble like this one:\n\n\n#> # A tibble: 1,000 × 2\n#>         x y    \n#>     <dbl> <chr>\n#>  1 -0.625 x<=0 \n#>  2  0.131 x>0  \n#>  3  0.204 x>0  \n#>  4  0.753 x>0  \n#>  5 -0.378 x<=0 \n#>  6  0.715 x>0  \n#>  7  0.440 x>0  \n#>  8  0.826 x>0  \n#>  9  0.443 x>0  \n#> 10 -0.646 x<=0 \n#> # … with 990 more rows\n\n\nExercise 2\nGiven:\nLL <- list(A = runif(1e2),\n           B = rnorm(1e3),\n           C = data.frame(x=runif(1e2), y=runif(1e2))\n           )\nPrint the sum of each element of LL in a list, in a vector.\n\nSolution\n\nLL <- list(A = runif(1e2),\n           B = rnorm(1e3),\n           C = data.frame(x=runif(1e2), y=runif(1e2))\n           )\nlapply(LL, sum)\n\n#> $A\n#> [1] 51.60701\n#> \n#> $B\n#> [1] -9.995451\n#> \n#> $C\n#> [1] 103.2238\n\nunlist(lapply(LL, sum)); sapply(LL, sum)\n\n#>          A          B          C \n#>  51.607006  -9.995451 103.223843\n\n\n#>          A          B          C \n#>  51.607006  -9.995451 103.223843\n\n\nExercise 3\n\nDownload population.csv and load it into a data.frame\n\nWhat is the total population over the years?\nWhat is the mean population for each city?\n\n\nSolution\n\n# Download population.csv and load it into a `data.frame`\ndf <- read.csv(\"Data/population.csv\")\n# What is the total population over the years?\ndata.frame(year=df[,\"year\"],\n           pop =rowSums(df[,-1]),     # a first way\n           pop2=apply(df[,-1], 1, sum)# another way\n          )\n\n#>   year     pop    pop2\n#> 1 1962 7349547 7349547\n#> 2 1968 7550999 7550999\n#> 3 1975 7298183 7298183\n#> 4 1982 6933230 6933230\n#> 5 1990 6857291 6857291\n#> 6 1999 6965325 6965325\n#> 7 2007 7235114 7235114\n#> 8 2012 7334805 7334805\n\n# A tidy-compatible version \nlibrary(tidyverse)\npopul <- pivot_longer(df, cols=-year, names_to=\"city\", values_to=\"pop\")\npopul %>%\n  group_by(year) %>%\n  summarise(totpop = sum(pop))\n\n#> # A tibble: 8 × 2\n#>    year  totpop\n#>   <int>   <int>\n#> 1  1962 7349547\n#> 2  1968 7550999\n#> 3  1975 7298183\n#> 4  1982 6933230\n#> 5  1990 6857291\n#> 6  1999 6965325\n#> 7  2007 7235114\n#> 8  2012 7334805\n\n# or equivalently\nsummarise(group_by(popul, year), totpop = sum(pop))\n\n#> # A tibble: 8 × 2\n#>    year  totpop\n#>   <int>   <int>\n#> 1  1962 7349547\n#> 2  1968 7550999\n#> 3  1975 7298183\n#> 4  1982 6933230\n#> 5  1990 6857291\n#> 6  1999 6965325\n#> 7  2007 7235114\n#> 8  2012 7334805\n\n# What is the mean population for each city?\napply(df[,-1], 2, mean)\n\n#>        Angers      Bordeaux         Brest         Dijon      Grenoble \n#>      138783.4      234814.9      149125.1      146735.2      157526.4 \n#>       LeHavre        LeMans         Lille          Lyon     Marseille \n#>      193989.6      144347.4      220018.4      470371.1      844253.4 \n#>   Montpellier        Nantes          Nice         Paris         Reims \n#>      203114.4      260924.9      334311.6     2321031.9      172278.0 \n#>        Rennes Saint.Etienne    Strasbourg        Toulon      Toulouse \n#>      193424.9      198134.6      255429.1      169682.6      382264.9\n\npopul %>%\n  group_by(city) %>%\n  summarise(avepop = mean(pop))\n\n#> # A tibble: 20 × 2\n#>    city            avepop\n#>    <chr>            <dbl>\n#>  1 Angers         138783.\n#>  2 Bordeaux       234815.\n#>  3 Brest          149125.\n#>  4 Dijon          146735.\n#>  5 Grenoble       157526.\n#>  6 LeHavre        193990.\n#>  7 LeMans         144347.\n#>  8 Lille          220018.\n#>  9 Lyon           470371.\n#> 10 Marseille      844253.\n#> 11 Montpellier    203114.\n#> 12 Nantes         260925.\n#> 13 Nice           334312.\n#> 14 Paris         2321032.\n#> 15 Reims          172278 \n#> 16 Rennes         193425.\n#> 17 Saint.Etienne  198135.\n#> 18 Strasbourg     255429.\n#> 19 Toulon         169683.\n#> 20 Toulouse       382265."
  },
  {
    "objectID": "12-plotting.html#base-graphics",
    "href": "12-plotting.html#base-graphics",
    "title": "\n11  Plotting\n",
    "section": "\n11.1 Base graphics",
    "text": "11.1 Base graphics\n\n11.1.1 Basic plotting\n\nx  <- seq(-3*pi,3*pi,length=50)\ny  <- sin(x)\nz  <- sin(x)^2\ndf <- data.frame(x=x, y=y)\nplot(x,y) # plot providing x and y data\n\n\n\nplot(df)  # plot providing a two-columns data.frame\n\n\n\nplot(df, type=\"l\")\n\n\n\nplot(df, type=\"b\")\n\n\n\ndf <- data.frame(x=x, y=y, z=z, w=z*y)\nplot(df)  # plot providing a multi-columns data.frame\n\n\n\n\n\n11.1.2 Adding some style\nOK, easy. Now let’s do some tuning of this, because it’s a tad ugly… Type in each command and see what they do.\n\n# create some fake data\nx  <- seq(-3*pi,3*pi,length=100)\ndf <- data.frame(x=x, y=sin(x), z=sin(x)^2)\n# add some styling parameters\npar(family = \"Helvetica\", cex.lab=1.5, cex.axis=1.4, \n    mgp = c(2.4, .5, 0), tck=0.02, mar=c(4, 4, 2, .5), lwd=2, las=1)\nplot(df$x,df$y,\n     type = \"l\",     # \"l\" for lines, \"p\" for points\n     xlab = \"X values\",\n     ylab = \"Intensity\",\n     axes = FALSE,\n     main = \"Some Plot\",\n     ylim = c(-1,2)\n    )\n# vertical line in 0\nabline(v=0,lty=2,lwd=2)\n# horizontal line in 0\nabline(h=0,lty=3,lwd=2)\n# line with coefficients a (intercept) and b (slope)\nabline(a=0,b=.1,lty=4,lwd=1)\n# add a line\nlines(df$x,df$z,type = \"l\",col=\"red\",lwd=3)\n# add points\npoints(df$x,df$z*df$y,col=\"royalblue\",pch=16,cex=1)\n# add custom axis. \n# Default with axis(1);axis(2);axis(3, labels=FALSE);axis(4, labels=FALSE);\n# Bottom\naxis(1,at=seq(-10,10,2),labels=TRUE,tck=0.02)\naxis(1,at=seq(-10,10,1),labels=FALSE,tck=0.01); # small inter-ticks\n# Top\naxis(3,at=seq(-10,10,2),labels=FALSE)\naxis(3,at=seq(-10,10,1),labels=FALSE,tck=0.01); # small inter-ticks\n# Left\naxis(2,at=seq(-1,2,.5),labels=TRUE)\naxis(2,at=seq(-1,2,.25),labels=FALSE,tck=0.01); # small inter-ticks\n# Right\naxis(4,at=seq(-1,2,.5),labels=FALSE)\naxis(4,at=seq(-1,2,.25),labels=FALSE,tck=0.01); # small inter-ticks\n# Draw a box\nbox()\n# Print legend\nlegend(\"topleft\",\n    cex=1.4, #size of text\n    lty=c(1,1,NA),   # type of line (1 is full, 2 is dashed...)\n    lwd=c(1,3,NA),   # line width\n    pch=c(NA,NA,16), # type of points\n    col=c(\"black\",\"red\",\"royalblue\"), # color\n    bty = \"n\", # no box around legend\n    legend=c(\"sin(x)\",expression(\"sin(x)\"^2),expression(\"sin(x)\"^3))\n    )\n\n\n\n\nMost needs should be covered with this simple plot that can be adapted.\n\n\n\n\n\n\nPro Tip: make a code snippet\n\n\n\n\n\nGo to Rstudio Preferences, Code, Edit code snippets, and add the following lines:\nsnippet plot\n    #pdf(\"xxx.pdf\", height=6, width=8)\n    par(cex.lab=1.7, cex.axis=1.7, mgp = c(3, 0.9, 0), \n        tck=0.02, mar=c(4.5, 4.5, 1, 1), lwd = 3, las=1)\n    plot(${1:x},${2:y},\n        type=\"l\",      # plot with a line\n        ylim=c( , ),\n        xlim=c( , ),\n        lwd=2,         # width of the line\n        lty=1,         # type of line\n        axes=FALSE,    # do not show axes\n        xlab=\"${1:x}\", # x label\n        ylab=\"${2:y}\", # y label\n        main=\"\")       # Title\n    legend(\"topright\",\n        cex=1.5,       # size of the text\n        pch=c(),       # list of point types\n        lty=c(),       # list of line types\n        lwd=c(),       # list of line widths\n        col=c(),       # list of line colors\n        bty=\"n\",       # no box around the legend\n        legend=c()     # list of legend labels\n        )\n    # Draw axes with minor ticks\n    axis(1, at=seq(0,1,.2), labels=TRUE)\n    axis(1, at=seq(0,1,.1), labels=FALSE, tck=0.01)\n    axis(3, at=seq(0,1,.2), labels=FALSE)\n    axis(3, at=seq(0,1,.1), labels=FALSE, tck=0.01)\n    par(mgp = c(2.5, 0.2, 0))\n    axis(2, at=seq(0,10,1), labels=TRUE)\n    axis(2, at=seq(0,10,.5), labels=FALSE, tck=0.01)\n    axis(4, at=seq(0,10,1), labels=FALSE)\n    axis(4, at=seq(0,10,.5), labels=FALSE, tck=0.01)\n    box() # drow box around plot\n    #dev.off()\n\n\n\n\n\n\n\n\n\n\nGoing further\n\n\n\n\n\n\n11.1.2.1 Panel plots\nLets create a plot with different panels (a bit ugly without styling, you need to tweak the margins and text distance to plot with par(mar(), mgp()) before each plot):\n\n# some fake data\nx  <- seq(-10,10,1)\nd1 <- data.frame(x=x, y=sin(x))\nd2 <- data.frame(x=x, y=cos(x))\nd3 <- data.frame(x=x, y=exp(-x^2)*sin(x)^2)\n# on a simple grid, use:\n# par(mfrow=c(nrows, ncols))\npar(mfrow=c(1, 3), mar=c(4,4,1,1))\nplot(d1,type=\"l\")\nplot(d2,type=\"p\")\nplot(d3,type=\"b\")\n\n\n\n\n\n# creating the layout and styling\nM  <- matrix(c(c(1,1),c(2,3)), byrow=TRUE, ncol=2); M\n\n#>      [,1] [,2]\n#> [1,]    1    1\n#> [2,]    2    3\n\nnf <- layout(M, heights=c(1), widths=c(1))\n# first plot\nplot(d1,type=\"l\")\n# second plot\nplot(d2,type=\"p\")\n# third plot\nplot(d3,type=\"b\")\n\n\n\n\n\n# creating the layout and styling\nM  <- matrix(c(c(1,1),c(2,3)), byrow=FALSE, ncol=2); M\n\n#>      [,1] [,2]\n#> [1,]    1    2\n#> [2,]    1    3\n\nnf <- layout(M, heights=c(1), widths=c(1))\n# first plot\nplot(d1,type=\"l\")\n# second plot\nplot(d2,type=\"p\")\n# third plot\nplot(d3,type=\"b\")\n\n\n\n\n\n11.1.2.2 Barplots and densities\n\nx <- rnorm(1e4, mean = 0, sd = 1)\n# Barplot\nhist(x)\n\n\n\n# Density\ny  <- density(x, bw=0.1) # small kernel bandwidth\ny2 <- density(x, bw=0.5) # larger kernel bandwidth\nplot(y, lwd=2, main=\"\", xlab=\"X values\", xlim=c(-4,4))\nlines(y2,col=\"red\",lwd=2)\npoints(x, jitter(rep(.01,length(x)), amount=.01), \n        cex=1,pch=16, col=adjustcolor(\"royalblue\", alpha=.01))\n\n\n\n\n\n\n\n\nExercise\nTry reproducing these plots:"
  },
  {
    "objectID": "12-plotting.html#advanced-plotting-using-ggplot2",
    "href": "12-plotting.html#advanced-plotting-using-ggplot2",
    "title": "\n11  Plotting\n",
    "section": "\n11.2 Advanced plotting using ggplot2",
    "text": "11.2 Advanced plotting using ggplot2\nFurther reading here and on the cheatsheet for example.\nggplot2 is a package (now even available for python) that completely changes the methodology of plotting data. With ggplot2, data are gathered in a tidy data.frame, and each column can be used as a parameter to tweak colors, point size, etc.\nFirst things first, load the library :\n\nlibrary(ggplot2)\n\nActually, ggplot2 is attached to tidyverse so a simple:\n\nlibrary(tidyverse)\n\nis enough, as it will load ggplot2 and most of the useful data manipulation libraries.\n\n11.2.1 The grammar of graphics\nWith ggplot2 is introduced the notion of “grammar of graphics” through the function ggplot(). What it means is that the plots are built through independent blocks that can be combined to create any wanted graphical display. To construct a plot, you need to provide building blocks such as:\n\ndata gathered in a tidy data.frame\nan aesthetics mapping: what column is x, y, the color, the size, etc…\ngeometric object: point, line, bar, histogram, tile…\nstatistical transformations if needed\nscales: color_manual, x_continuous, …\ncoordinate system\nfaceting: wrap, grid\ntheme: theme_bw(), theme_light()…\n\nThe typical call to ggplot() is thus (the arguments between <> are yours to specify):\nggplot(data=<data>, aes(x=<x>, y=<y>, color=<z>, size=<w>))+\n    geom_<geometry>()+\n    scale_<scales>()+\n    facet_<facets>()+\n    <theme>\nSince a figure is worth a thousand words, let’s get to it. We will use the dataset diamonds built-in with the ggplot2 package. Let’s have a look:\n\ndiamonds\n\n#> # A tibble: 53,940 × 10\n#>    carat cut       color clarity depth table price     x     y     z\n#>    <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n#>  1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n#>  2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n#>  3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n#>  4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n#>  5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n#>  6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n#>  7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n#>  8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n#>  9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n#> 10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n#> # … with 53,930 more rows\n\n\ndiamonds contains 53940 lines and 10 columns in a tibble. ggplot2 can easily handle such large dataset.\nLet’s say we want to see whether there is a correlation between price and weight (carat) of the diamonds. We will make a call to ggplot() by providing it the data and the x and y mapping:\n\np <- ggplot(data = diamonds, aes(x = carat,y = price))\np\n\n\n\n\nYou see that our plot p here has the proper axis labels and range:\n\nglue::glue(\"carat {c('min','max')}: {range(diamonds$carat)}\")\n\n#> carat min: 0.2\n#> carat max: 5.01\n\nglue::glue(\"price {c('min','max')}: {range(diamonds$price)}\")\n\n#> price min: 326\n#> price max: 18823\n\n\nbut it does not display any data points. For this we have to add a geometry to the plot, using one of the geom_xx() functions. Let’s plot it using points for now:\n\np + geom_point()\n\n\n\n\nOK, we’re onto something, but we can probably add some information to this plot. We will first cut the data above 3 carats because they are not relevant, and add some transparency to the points to see some statistical information. Let’s make use of the pipe operator %>% to make this easily readable:\n\np <- diamonds %>% \n        filter(carat<=3) %>% \n        ggplot(aes(x = carat, y = price))\np + geom_point(alpha=0.5)\n\n\n\n\nLet’s now see whether the type of cut plays a role here by coloring the points according to the cut variable:\n\np <- diamonds %>% \n        filter(carat<=3) %>% \n        ggplot(aes(x = carat, y = price, color = cut))\np + geom_point(alpha=0.5)\n\n\n\n\nIt looks like the price dispersion is homogeneous, we can make sure by adding a spline smoothing:\n\np + geom_point(alpha=0.5) + \n    geom_smooth()\n\n\n\n\nThe slope evolution shows that in general, the better the cut, the higher the price. But there are some discrepancies that may be explained in another manner:\n\np <- diamonds %>% \n        filter(carat<=3) %>% \n        ggplot(aes(x = carat, y = price, color = clarity))\np + geom_point(alpha=0.5) + geom_smooth()\n\n\n\n\nIt is often easier to grasp a multi-variable problem by plotting all our data in a facet plot using facet_wrap(~variable1) if you want one variable changing in each plot:\n\ncolors <- rainbow(length(unique(diamonds$clarity)))\np <- ggplot(diamonds, \n            aes(x = price, y = carat, color = clarity)) + \n        geom_point(alpha = 0.5, size = 1) + \n        geom_smooth(color = \"black\") + \n        scale_colour_manual(values = colors, name = \"Clarity\") +\n        facet_wrap(~cut) \np\n\n\n\n\n… or facet_grid(y_variable ~ x_variable) if you want to see one variable as a function of another:\n\np <- ggplot(diamonds, \n            aes(x = price, y = carat, color = color)) + \n        geom_point(alpha = 0.8, size = 1) + \n        geom_smooth(method = \"lm\", color = \"black\") + \n        scale_colour_brewer(palette = \"Spectral\", name = \"Color\") +\n        facet_grid(clarity ~ cut) \np\n\n\n\n\nOr by adding another graphical parameter such as the size of the points:\n\np <- ggplot(diamonds, \n            aes(x = price, y = carat, color = clarity, size = cut)) + \n        geom_point(alpha = 0.5) + \n        scale_colour_manual(values = colors, name = \"Clarity\")\np\n\n\n\n\nOK, maybe not here because the graph gets clogged, so we can lighten it by sampling data:\n\np <- ggplot(diamonds %>% sample_n(500), \n            aes(x = carat, y = price, color = clarity, size = cut)) + \n        geom_point(alpha = 0.5) + \n        scale_colour_manual(values = colors, name = \"Clarity\")\np\n\n\n\n\n\n11.2.2 Writing maths in the plot\nTo write mathematical expressions, including subscripts/superscripts, or more complex mathematical formulas, the easiest way is to use \\(\\LaTeX\\) expressions thanks to the latex2exp package. Just write your text within a latex2exp::TeX() function, and that is all. Since R 4.0, it is recommended to use the raw string literal syntax. The syntax looks like r\"(...)\", where ... can contain any character sequence, including \\ (no need to escape the \\ character).\nAlso, in case you have a variables to add in your string, it is often clearer/easier to use glue::glue() instead of paste() or paste0(). Then, you can combine the two, like so:\n\nlibrary(latex2exp)\nlibrary(glue)\nA <- 1.2\nB <- 4\ndiamonds %>% sample_n(100) %>% \n    ggplot(aes(x = carat, y = price))+\n        geom_point(alpha = 0.5) +\n        labs(x = \"Carat\",\n             y = \"Price\",\n             title = TeX(glue(r'(This is some $\\,\\LaTeX\\,$ maths: $\\,\\sum_i^{N}\\frac{x_i^{[A]}}{[B]}$)', \n                        .open = \"[\", .close = \"]\"))\n            )\n\n\n\n\nFor more complicated stuff, I however advise to not use the TeX() function and write directly your text in \\(\\LaTeX\\), then export your plot with tikzDevice as a .tex file and compile it to pdf.\n\n11.2.3 Theming\nIt is very easy to keep the same theme on all your graphs thanks to the theme() function. There are a collection of pre-defined themes, like:\n\n\n\n\n\nYou can define all the parameters you want, like this (hit ?theme like usual to see all the parameters):\n\nmy_theme <- theme_bw()+\n            theme(text = element_text(size = 18, \n                                      family = \"Times\", \n                                      face = \"bold\"),\n                  axis.ticks = element_line(linewidth = 1),\n                  legend.text = element_text(size = 14, family = \"Times\"),\n                  panel.border = element_rect(linewidth = 2),\n                  panel.grid.major = element_blank(), \n                  panel.grid.minor = element_blank()\n                  )\np + my_theme\n\n\n\n\n\n\n\n\n\n\nPro Tip: make a code snippet\n\n\n\n\n\nGo to Rstudio Preferences, Code, Edit code snippets, and add the following lines:\nsnippet ggplot\n    %>%\n    ggplot(aes(x=${1:x}, y=${2:y}, color=${3:z})) +\n        geom_point() +\n        labs(x = \"X label\", \n             y = \"Y label\",\n             color = \"Color label\") +\n        theme_bw()\n\n\n\n\n11.2.4 Making interactive plots with ggplot2 and plotly\nThanks to the plotly package, it is really easy to transform a ggplot plot into an interactive plot:\n# load plotly\nlibrary(plotly)\n\np <- ggplot(diamonds %>% sample_n(100), \n        aes(x = carat, y = price)) + \n        geom_point(aes(color = clarity), alpha = 0.5, size = 2) + \n        my_theme\nggplotly(p, dynamicTicks = TRUE)\n\n\n\n\n\n\n11.2.5 Gathering plots on a grid\nIf you have several plot you want to gather on a grid and you can’t use facet_wrap() (because they come from different data sets), you can use the library patchwork:\n\nlibrary(patchwork)\nlibrary(ggplot2)\ntheme_set(theme_bw())\nx  <- seq(-2*pi,2*pi,.1)\np1 <- qplot(x,sin(x), geom = \"line\")\np2 <- qplot(x,cos(x), geom = \"line\")\np3 <- qplot(x,atan(x), geom = \"line\")\np4 <- qplot(x,dnorm(x), geom = \"line\")\np1 + p2\n\n\n\np1 + p2 / p3 + p4 + \n    plot_annotation(tag_levels = 'a', tag_suffix=\")\")\n\n\n\n(p1 + p2 + plot_layout(widths = c(1,3))) /\np3/p4 + \n    plot_layout(heights = c(6, 2, 1))\n\n\n\n\n\n11.2.6 Plots with insets\nIf you want to make an inset plot, first make your plots, then add a plot within the other using patchwork::inset_element(), specifying the x and y positions of the 4 corners of the inset plot using relative values:\n\np4 + inset_element(p3, left   = 0.01, right = .4, \n                       bottom = .45,    top = .99)"
  },
  {
    "objectID": "12-plotting.html#exporting-a-plot-to-pdf-or-png",
    "href": "12-plotting.html#exporting-a-plot-to-pdf-or-png",
    "title": "\n11  Plotting\n",
    "section": "\n11.3 Exporting a plot to pdf or png",
    "text": "11.3 Exporting a plot to pdf or png\n\n11.3.1 A single plot\nA plot can be exported if surrounded by XXX and dev.off(), with XXX that can be pdf(\"xxx.pdf\",height=6, width=8), png(\"xxx.png\",height=600, width=800)… Examples:\nP <- ggplot(df, aes(x,y)) + geom_point()\npdf(\"plot.pdf\",height=6, width=8)\nP\ndev.off()\npdf(\"plot.png\",height=600, width=800)\nplot(x,y,\n     type=\"l\",\n     xlab=\"x\"\n     )\ndev.off()\nYou can also export the graph as a .tex file using tikz(), which allows you to use \\(\\LaTeX\\) mathematical expressions (don’t forget to escape the \\ character or to use raw string literal syntax r\"(...)\"):\nlibrary(tikzDevice)\ntikz(\"plot.tex\",height=6, width=8, pointsize = 10, standAlone=TRUE)\nplot(x,y,\n     type=\"l\",\n     xlab=r\"($\\omega_i$)\"\n     )\ndev.off()\ntools::texi2pdf(\"plot.tex\") # compile the tex file to pdf\nsystem(\"open -a Skim plot.pdf\") # on Mac: open the resulting pdf with Skim\n\n11.3.2 Multiple plots\nIn case you want to output multiple plots with a for loop, you have two options:\n\nOuptut a separate file for each plot\nFor pdf output only: ouptut a single file with a each plot in a different page\n\nIn both cases, if you plot with ggplot (which I always recommend), then you need to explicitly print() the plot in the for loop, like so:\n\nOutput a separate file for each plot:\n\nfor(i in 1:4){\n    plot_name = paste0(\"plot_\",i,\".pdf\")\n    P <- ggplot(my_data[[i]], aes(x, y)) + geom_point()\n    pdf(plot_name, height = 6, width = 8)\n    print(P)\n    dev.off()\n}\n\nOutput a single file with a page for each plot:\n\npdf(\"plots.pdf\", height = 6, width = 8)\nfor(i in 1:4){\n    P <- ggplot(my_data[[i]], aes(x, y)) + geom_point()\n    print(P)\n}\ndev.off()"
  },
  {
    "objectID": "12-plotting.html#exo-plots",
    "href": "12-plotting.html#exo-plots",
    "title": "\n11  Plotting\n",
    "section": "\n11.4 Exercises",
    "text": "11.4 Exercises\nInteractive exercises can be found in the tutor package. For this, simply run:\nlibrary(tutor)\ntuto(\"plots\")\nAlternatively, you can just download the archive with all the exercises files, unzip it in your R class RStudio project, and edit the R files.\n\nExercise 1\n\nDownload the two sample Raman spectra: PPC60_G_01.txt and PPC60_G_30.txt\n\nLoad them in two separate tibble\n\nGather the two data.frame in another single tidy one: it should have three columns, w, Intensity and file_name\n\nCreate a function norm01() that, given a vector, returns the same vector normalized to [0,1]\nUsing group_by() and mutate(), add a column norm_int of normalized intensity for each file\nPlot the two normalized spectra on the same graph using lines of different colors\nPlay with the theme and parameters to reproduce the following plot:\n\n\n\n\n\n\n\nSolution\n\n# Load them in two separate `tibbles`\nlibrary(tidyverse)\n# Using read.table (who returns a data.frame)\ndf1 <- read.table(\"Data/PPC60_G_01.txt\", col.names=c(\"w\",\"Intensity\"))\ndf2 <- read.table(\"Data/PPC60_G_30.txt\", col.names=c(\"w\",\"Intensity\"))\ndf1 <- tibble(df1) # make a tibble from a data.frame\ndf2 <- tibble(df2)\n# Direct version using tidyverse (read_table returns a tibble)\ndf1 <- read_table(\"Data/PPC60_G_01.txt\", col_names=c(\"w\",\"Intensity\"))\ndf2 <- read_table(\"Data/PPC60_G_30.txt\", col_names=c(\"w\",\"Intensity\"))\n# Gather the two `tibbles` in another single tidy one: \n# it should have three columns, `w`, `Intensity` and `file_name`\ndf1$file_name <- \"PPC60_G_01\" # add the \"file_name\" column\ndf2$file_name <- \"PPC60_G_30\"\ndf_tidy <- bind_rows(df1,df2) # stack the two tibbles\n# Create a function `norm01` that, given a vector, returns the same vector normalized to [0,1]\nnorm01 <- function(x) {\n    (x-min(x))/(max(x)-min(x))\n}\n# Using `group_by` and `mutate`, add a column `norm_int` in df_tidy of normalized intensity for each file\ndf_tidy <- df_tidy %>% \n    group_by(file_name) %>% \n    mutate(norm_int=norm01(Intensity))\n# Plot the two normalized spectra on the same graph using lines of different colors\nlibrary(ggplot2)\ndf_tidy %>% \n    ggplot(aes(x=w, y=norm_int, color=file_name))+\n        geom_line()+\n        theme_bw()\n\n\n\n# Play with the theme and parameters to reproduce the following plot:\ndf_tidy %>% \n    ggplot(aes(x=w, y=norm_int, color=file_name)) +\n        geom_line()+\n        scale_color_manual(values=c(\"red\",\"royalblue\"), name=\"\") +\n        labs(x=\"Raman Shift [1/cm]\", y=\"Intensity [arb. unit]\") +\n        theme_bw() +\n        theme(legend.position = \"top\",\n              text = element_text(size = 14,family = \"Times\"),\n              panel.grid.major = element_blank(), \n              panel.grid.minor = element_blank())\n\n\n\n\nExercise 2\n\nDownload rubis_01.txt, rubis_02.txt, rubis_03.txt and rubis_04.txt and load them into a tidy tibble.\nNormalize all data to [0,1] in a 4th column\nPlot the 4 spectra on top of each other with a vertical shift of 1, with a different color for each spectrum\n\nFor this, check out the factor() function:\n\n\n\n\nx <- c(\"a\",\"a\",\"b\",\"c\",\"a\")\nfactor(x)\n\n#> [1] a a b c a\n#> Levels: a b c\n\nas.numeric(factor(x))\n\n#> [1] 1 1 2 3 1\n\n\n\nAnnotate on the base line with the name of the file. For this, use annotate(\"text\", x, y, label)\n\nIt should look like this:\n\n\n\n\n\n\n\nSolution\n\nlibrary(tidyverse)\nlibrary(ggplot2)\ndf <- tibble()\nnorm01 <- function(x) {\n    (x-min(x))/(max(x)-min(x))\n}\nfor (i in 1:4) {\n    d  <- read_table(paste0(\"Data/rubis_0\",i,\".txt\"), col_names=c(\"w\", \"Int\"))\n    d$Int_n <- norm01(d$Int)\n    d$name  <- paste0(\"rubis_0\",i)\n    df      <- bind_rows(df, d)\n}\nfnames <- unique(df$name)\nggplot(data=df, aes(x=w, \n                    y=Int_n+as.numeric(factor(name))-1, \n                    color=name))+\n    geom_line(size=1)+\n    annotate(\"text\", x=3080, y=1:length(fnames)-.85, label=fnames, size=5)+\n    labs(x=\"Raman Shift [1/cm]\", y=\"Intensity [arb. units]\")+\n    theme_bw()+\n    theme(legend.position = \"none\",\n          text            = element_text(size = 14),\n          axis.text.y     = element_blank(),\n          axis.text       = element_text(size = 14))\n\n\n\n\nExercise 3\n\nDownload dataG.zip\n\nMake a plot similar to this one (don’t bother with the fit), plotting the evolution of a Raman spectrum as a function of pressure:\n\n\nBonus:\n\nLooking at data for increasing pressures\n\nPlot the data using an interactive slider (see about the frame option here)\nPlot the data using a 3D color map. Since the data are not on a regular grid, you will need to interpolate the data on a regular grid with the akima package and its interp() function. See chapter @ref(colorplots) on 3D plotting for help.\n\n\n\n\nSolution\n\n# get the list of files for the ramps up and down and out of cell\nfiles_up   <- list.files(\"Data/dataG/\", pattern=\"up\")\nfiles_down <- list.files(\"Data/dataG/\", pattern=\"down\")\nout_cell   <- list.files(\"Data/dataG/\", pattern=\"out\")\n# store all file names in the correct order\nallfiles <- c(out_cell, files_up, files_down)\n# load the wanted package\nlibrary(tidyverse)\nlibrary(ggplot2)\n# create the norm01 function\nnorm01 <- function(x) { (x-min(x))/(max(x)-min(x)) }\n# initialize an empty tibble to store all data\nalldata <- tibble()\nfor (file in allfiles) {#file <- allfiles[1]\n    # read the data and stor it in d\n    d <- read_table(paste0(\"Data/dataG/\",file), col_names=TRUE)\n    # normalize data\n    d$Int_n <- norm01(d$Int)\n    # store file name\n    d$name <- file\n    # store run number for the stacking\n    d$run_number <- which(file==allfiles)\n    # store all data in a single tidy tibble\n    alldata <- bind_rows(alldata, d)\n}\n# plot all data\nalldata %>% \n    filter(w<=1750, w>=1500) %>% # zoom on the interesting part\n    ggplot(aes(x=w, \n               y=Int_n + run_number - 1))+ # to stack the plots\n        geom_point(color=\"gray\", alpha=.5, size=.2)+ #plot data with points\n        xlim(c(1500,1800))+ #add some white space on the right to write the pressure\n        geom_vline(xintercept=1592, lty=2, size=1)+#show a vertical line\n        annotate(geom  = \"text\", size=5, #show the pressure values\n                 x = 1760, y=seq_along(allfiles)-1, hjust = 0,\n                 label  = paste(unique(alldata$P),\"GPa\"),\n                 family = \"Times\")+\n        labs(x=\"Raman Shift [1/cm]\", #have the good axis labels\n             y=\"Intensity [arb. units]\")+\n        theme_bw()+#black and white theme\n        theme(legend.position = \"none\",#no legend\n              text            = element_text(size = 14, family = \"Times\"),#text in font Times\n              axis.text.y     = element_blank(),# no y axis values\n              axis.text       = element_text(size = 14),\n              panel.grid.major = element_blank(), # no grid\n              panel.grid.minor = element_blank())\n\n\n\n\n\n# Looking at data for increasing pressures, plot the data using an interactive slider\nlibrary(plotly)\nP <- alldata %>% \n    filter(grepl(\"up\",name)) %>% # only increasing pressures\n    filter(w<=1850, w>=1500) %>% # zoom on the interesting part\n    ggplot(aes(x=w, \n               y=Int_n,\n               frame=P))+ # each pressure in a new frame\n        geom_point(color=\"gray\", alpha=.5, size=1)+ #plot data with points\n        labs(x=\"Raman Shift [1/cm]\", #have the good axis labels\n             y=\"Intensity [arb. units]\")+\n        theme_bw()+#black and white theme\n        theme(legend.position = \"none\",#no legend\n              text            = element_text(size = 14, family = \"Times\"),\n              axis.text       = element_text(size = 14),\n              panel.grid.major = element_blank(), # no grid\n              panel.grid.minor = element_blank())\nggplotly(P, dynamicTicks = TRUE)\n\n\n\n\n\n\n# Plot the data using a 3D color map. Since the data are not on a regular grid, \n# you will need to interpolate the data on a regular grid \n# with the `akima` package and its `interp()` function\nlibrary(akima)\ntoplot <- alldata %>% \n            filter(grepl(\"up\",name)) %>%\n            filter(w<=1850, w>=1500)\ntoplot.interp <- with(toplot, \n                    interp(x = w, y = P, z = Int_n, \n                           duplicate=\"median\",\n                           xo=seq(min(toplot$w), max(toplot$w), length = 100),\n                           yo=seq(min(toplot$P), max(toplot$P), length = 100),\n                           extrap=FALSE, linear=FALSE)\n                   )\n# toplot.interp is a list of 2 vectors and a matrix\nstr(toplot.interp)\n\n#> List of 3\n#>  $ x: num [1:100] 1500 1504 1507 1511 1514 ...\n#>  $ y: num [1:100] 2.12 2.35 2.59 2.82 3.06 ...\n#>  $ z: num [1:100, 1:100] NA NA NA 0.0287 0.0291 ...\n\n# Regrouping this list to a 3-columns data.frame\nmelt_x <- rep(toplot.interp$x, times=length(toplot.interp$y))\nmelt_y <- rep(toplot.interp$y, each=length(toplot.interp$x))\nmelt_z <- as.vector(toplot.interp$z)\ntoplot.smooth <- na.omit(data.frame(w=melt_x, Pressure=melt_y, Intensity=melt_z))\n# Plotting\ncolors <- colorRampPalette(c(\"white\",\"royalblue\",\"seagreen\",\"orange\",\"red\",\"brown\"))(500)\nP <- ggplot(data=toplot.smooth, aes(x=w, y=Pressure, fill=Intensity)) + \n      geom_raster() + \n      scale_fill_gradientn(colors=colors, name=\"Normalized\\nIntensity\\n[arb. units]\") +\n      labs(x = \"Raman Shift [1/cm]\",y=\"Pressure [GPa]\") +\n      theme_bw()+\n      theme(text            = element_text(size = 14, family = \"Times\"),\n            axis.text       = element_text(size = 14),\n            panel.grid.major = element_blank(), # no grid\n            panel.grid.minor = element_blank())\nggplotly(P, dynamicTicks = TRUE)\n\n\n\n\n\nExercise 4\n\nDownload population.csv and load it into a data.frame\n\nIs it a tidy data.frame?\n\nDo we want a tidy data.frame?\nWhy?\nAct accordingly\n\n\nPlot the population vs. year with a color for each city\n\nWith points\nWith lines\nWith a black and white theme\nMake it interactive\n\n\nTry reproducing the following plots (Google is your friend) (look into the function reorder() and this help to use it with facets):\n\n\n\n\n\n\n\n\n\n\nSolution\n# Load and tidy population data.frame\nlibrary(tidyverse)\ndf <- read.csv(\"Data/population.csv\")\ndf <- pivot_longer(df, \n                   cols=-year, #year should stay a column\n                   names_to=\"City\", #column names should go to the column `city`\n                   values_to=\"Population\" #values should go to the column `population`\n                   )\ndf$City <- gsub(\"\\\\.\", \" \", df$City) # replace dots by spaces in city names\n# Plot the population vs. year with a different color for each city\np <- ggplot(data=df, aes(x=year, y=Population, color=City))\n# With points\np + geom_point()\n# With lines\np + geom_line()\n# With a black and white theme\n# Change the axis labels to \"year\" and \"Population\"\np <- p + geom_line() + theme_bw(); p\n# Make it interactive\nlibrary(plotly)\nggplotly(p, dynamicTicks = TRUE)\n# Reproduce the plots\nmy_theme <- theme_bw()+\n            theme(axis.text = element_text(size = 14,family = \"Helvetica\",colour=\"black\"),\n                  text = element_text(size = 14,family = \"Helvetica\"),\n                  axis.ticks = element_line(colour = \"black\"),\n                  legend.text = element_text(size = 10,family = \"Helvetica\",colour=\"black\"),\n                  panel.border = element_rect(colour = \"black\", fill=NA, linewidth=1)\n                  )\ncolors <- c(\"royalblue\",\"red\")\np1 <- df %>% filter(City%in%c(\"Montpellier\",\"Nantes\")) %>% \n        ggplot(aes(x=year, y=Population, size=Population, color=City)) +\n            geom_point() + \n            geom_smooth(method=\"lm\", aes(fill=City), \n                        alpha=0.1, show.legend = FALSE) + \n            scale_color_manual(values=colors)+\n            scale_fill_manual(values=colors)+\n            ggtitle(\"Population in Montpellier and Nantes\")+\n            labs(x=\"Year\", y=\"Population\")+\n            my_theme \np1\np2 <- df %>% filter(year==2012) %>% \n        ggplot(aes(x=reorder(City,-Population), \n                   y=Population/1e6, \n                   fill=Population/1e6)) +\n            geom_bar(stat=\"identity\", position=\"dodge\") + \n            ggtitle(\"Population in 2012 (in millions)\")+\n            labs(x=\"\", y=\"Population (in millions)\")+\n            scale_fill_gradientn(colors=colors, \n                                 name=\"Population\\n(in millions)\") +\n            my_theme + \n            theme(axis.text.x = element_text(angle = 45, hjust=1))\np2"
  },
  {
    "objectID": "13-3d_plots.html#the-ggplot2-solution",
    "href": "13-3d_plots.html#the-ggplot2-solution",
    "title": "12  3D color plots",
    "section": "\n12.1 The ggplot2 solution",
    "text": "12.1 The ggplot2 solution\nLet’s create a dummy set of spectra that we will gather in a tidy tibble.\n\nlibrary(tidyverse)\nNspec <- 40                           # Amount of spectra\nN     <- 500                          # Size of the x vector\n# Create a fake data tibble\nfake_data <- tibble(T = round(seq(273, 500, length=Nspec), 1)) %>% \n    mutate(spec = map(T, ~tibble(w = seq(0, 100, length = N),\n                         Intensity = 50*dnorm(w, mean = (./T[1])*20 + 25, \n                                                  sd  = 10+runif(1,max=5)))))\nfake_data\n\n#> # A tibble: 40 × 2\n#>        T spec              \n#>    <dbl> <list>            \n#>  1  273  <tibble [500 × 2]>\n#>  2  279. <tibble [500 × 2]>\n#>  3  285. <tibble [500 × 2]>\n#>  4  290. <tibble [500 × 2]>\n#>  5  296. <tibble [500 × 2]>\n#>  6  302. <tibble [500 × 2]>\n#>  7  308. <tibble [500 × 2]>\n#>  8  314. <tibble [500 × 2]>\n#>  9  320. <tibble [500 × 2]>\n#> 10  325. <tibble [500 × 2]>\n#> # … with 30 more rows\n\nfake_data <- fake_data %>% unnest(spec)\nfake_data\n\n#> # A tibble: 20,000 × 3\n#>        T     w Intensity\n#>    <dbl> <dbl>     <dbl>\n#>  1   273 0       0.00981\n#>  2   273 0.200   0.0103 \n#>  3   273 0.401   0.0107 \n#>  4   273 0.601   0.0112 \n#>  5   273 0.802   0.0117 \n#>  6   273 1.00    0.0122 \n#>  7   273 1.20    0.0127 \n#>  8   273 1.40    0.0133 \n#>  9   273 1.60    0.0139 \n#> 10   273 1.80    0.0145 \n#> # … with 19,990 more rows\n\n\nOK, so now we have some fake experimental data stored in a tidy tibble called fake_data. We want to plot it as a color map in order to grasp the evolution of the spectra. This can be done through the use of geom_contour() and geom_contour_filled() functions and by providing the z aesthetics, or by using the geom_raster() or geom_tile() functions with a fill aesthetics. Both methods can be combined, as shown below:\n\n# Plotting\ncolors <- colorRampPalette(c(\"white\",\"royalblue\",\"seagreen\",\n                             \"orange\",\"red\",\"brown\"))\nNbins <- 10\nggplot(data=fake_data, aes(x=w, y=T, z=Intensity)) + \n      geom_contour_filled(bins = Nbins) + \n      ggtitle(\"Some fake data\") + \n      scale_fill_manual(values = colors(Nbins),\n                        name = \"Intensity\\n[arb. units]\") +\n      labs(x = \"Fake Raman Shift [1/cm]\",\n           y = \"Fake Temperature [K]\") +\n      theme_bw()\n\n\n\nggplot(data=fake_data, aes(x = w, y = T)) + \n      geom_raster(aes(fill = Intensity)) + #geom_tile would work\n      geom_contour(aes(z = Intensity), color = \"black\", bins = 5)+\n      ggtitle(\"Some fake data\") + \n      scale_fill_gradientn(colors = colors(10), \n                           name = \"Intensity\\n[arb. units]\") +\n      labs(x = \"Fake Raman Shift [1/cm]\",\n           y = \"Fake Temperature [K]\") +\n      theme_bw()\n\n\n\n\nAnother option is to make a “ridge plot”, or a stacking of plots:\n\ncolors <- colorRampPalette(c(\"royalblue\",\"seagreen\",\"orange\",\n                             \"red\",\"brown\"))(length(unique(fake_data$T)))\nggplot(data = fake_data, \n       aes(x = w, \n           y = Intensity + as.numeric(factor(T))-1,\n           color = factor(T))\n       ) + \n    geom_line() + \n    labs(x = \"Fake Raman Shift [1/cm]\", \n         y = \"Fake Intensity [arb. units]\") +\n    coord_cartesian(xlim = c(25,75)) +\n    scale_color_manual(values=colors,name=\"Fake\\nTemperature [K]\") +\n    theme_bw()\n\n\n\nggplot(data=fake_data, \n       aes(x = w, \n           y = Intensity + as.numeric(factor(T))-1, \n           color = T, \n           group = T)\n       )+\n    geom_line() + \n    labs(x=\"Fake Raman Shift [1/cm]\", y=\"Fake Intensity [arb. units]\") +\n    scale_color_gradientn(colors=colors,name=\"Fake\\nTemperature [K]\") +\n    coord_cartesian(xlim = c(25,75)) +\n    theme_bw()"
  },
  {
    "objectID": "13-3d_plots.html#the-base-graphics-solution",
    "href": "13-3d_plots.html#the-base-graphics-solution",
    "title": "12  3D color plots",
    "section": "\n12.2 The base graphics solution",
    "text": "12.2 The base graphics solution\nIn some cases you end up with a matrix z, and two vectors x and y. This is easy to plot using the base image() function. For the sake of example, let’s just pivot our 3-columns data.frame to such a matrix using pivot_wider():\n\nx <- sort(unique(fake_data$w))\ny <- sort(unique(fake_data$T))\nz <- as.matrix(fake_data %>% \n                pivot_wider(values_from = Intensity, names_from = T) %>% \n                select(-w)\n               )\ncolors <- colorRampPalette(c(\"white\",\"royalblue\",\"seagreen\",\"orange\",\"red\",\"brown\"))(50)\npar(mar = c(4, 4, .5, 4), lwd = 2)\nimage(x, y, z, col = colors)\n\n\n\n\nYou can add a legend by using the image.plot function:\n\nlibrary(fields)\npar(mar=c(4, 4, .5, 4), lwd=2)\nimage.plot(x,y,z, col = colors)"
  },
  {
    "objectID": "13-3d_plots.html#the-plotly-solution",
    "href": "13-3d_plots.html#the-plotly-solution",
    "title": "12  3D color plots",
    "section": "\n12.3 The plotly solution",
    "text": "12.3 The plotly solution\nAnd finally, if you want to make this an interactive plot, you can use plot_ly():\n\nlibrary(plotly)\naX <- list(title = \"Raman Shift [1/cm]\")\naY <- list(title = \"Temperature [K]\")\n# Weird but you need to use t(z) here:\nz <- t(z)\n# Color plot\nplot_ly(x = x, y = y, z = z, type = \"heatmap\", colors = colors) %>% \n   layout(xaxis = aX, yaxis = aY)\n\n\n\n\n\nOr, very cool, an interactive surface plot:\n\nplot_ly(x=x, y=y, z=z, type = \"surface\", colors=colors) %>%\n   layout(scene = list(xaxis = aX, yaxis = aY, dragmode=\"turntable\"))"
  },
  {
    "objectID": "13-3d_plots.html#the-case-of-non-regular-data",
    "href": "13-3d_plots.html#the-case-of-non-regular-data",
    "title": "12  3D color plots",
    "section": "\n12.4 The case of non-regular data",
    "text": "12.4 The case of non-regular data\nIn case you have a set of non-regular data, plotting it as a color map can get tricky: how do we tell the plotting device what color should be in a place where there is no data point?\nThe solution is to use a spline (or linear, but spline looks usually nicer) interpolation of your 2D data. For this, we can use the akima package and its interp() function, like so:\n\n# let's make our data irregular and see the plot is now not working:\nirreg.df <- fake_data[sample(nrow(fake_data), nrow(fake_data)/3),]\n# let's plot these irregular data\ncolors <- colorRampPalette(c(\"white\",\"royalblue\",\"seagreen\",\n                             \"orange\",\"red\",\"brown\"))(500)\nggplot(data=irreg.df, aes(x=w, y=T, fill=Intensity)) + \n      geom_raster() + #geom_tile would work\n      ggtitle(\"Some irregular and ugly fake data\") + \n      scale_fill_gradientn(colors=colors,name=\"Intensity\\n[arb. units]\") +\n      labs(x = \"Fake Raman Shift [1/cm]\",\n           y = \"Fake Temperature [K]\") +\n      theme_bw()\n\n\n\n# now let's interpolate the data on a 100x100 regular grid\n# linear = FALSE -> cubic interpolation\nlibrary(akima)\nirreg.df.interp <- with(irreg.df, \n    interp(x=w, y=T, z=Intensity, nx = 100, ny = 100,\n           duplicate = \"median\", extrap = FALSE, linear = FALSE)\n    )\n# irreg.df.interp is a list of 2 vectors and a matrix\nstr(irreg.df.interp)\n\n#> List of 3\n#>  $ x: num [1:100] 0 1.01 2.02 3.03 4.04 ...\n#>  $ y: num [1:100] 273 275 278 280 282 ...\n#>  $ z: num [1:100, 1:100] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ...\n\n# Regrouping this list to a 3-columns data.frame\nirreg.df.smooth <- expand.grid(w = irreg.df.interp$x, \n                               T = irreg.df.interp$y) %>% \n                        tibble() %>% \n                        mutate(Intensity = as.vector(irreg.df.interp$z)) %>% \n                        na.omit()\n# Plotting\nirreg.df.smooth %>% \n    ggplot(aes(x=w, y=T, fill=Intensity)) + \n        geom_raster() + \n        ggtitle(\"Some irregular fake data that have been interpolated with cubic splines\") + \n        scale_fill_gradientn(colors=colors, name=\"Intensity\\n[arb. units]\") +\n        labs(x = \"Fake Raman Shift [1/cm]\", \n             y = \"Fake Temperature [K]\") +\n        theme_bw()"
  },
  {
    "objectID": "13-3d_plots.html#d-density-of-points",
    "href": "13-3d_plots.html#d-density-of-points",
    "title": "12  3D color plots",
    "section": "\n12.5 2D density of points",
    "text": "12.5 2D density of points\nIn case you want to plot a density of points, you have a variety of solutions:\n\ndf <- tibble(x=rnorm(1e3, mean=c(1,5)),\n             y=rnorm(1e3, mean=c(5,1)))\np1 <- ggplot(data=df, aes(x=x,y=y))+ geom_density2d() + ggtitle('geom_density2d()')\np2 <- ggplot(data=df, aes(x=x,y=y))+ geom_hex() + ggtitle('geom_hex()')\np3 <- ggplot(data=df, aes(x=x,y=y))+ geom_bin2d() + ggtitle('geom_bin2d()')\np4 <- ggplot(data=df, aes(x=x,y=y))+ ggtitle('stat_density2d()') +\n        stat_density2d(aes(fill = ..density..), geom = \"tile\", contour = FALSE, n = 200) +\n        scale_fill_continuous(low = \"white\", high = \"dodgerblue4\")\nlibrary(cowplot)\nplot_grid(p1,p2,p3,p4)\n\n\n\n\nOr the base smoothScatter() function could do the trick:\n\nsmoothScatter(df)"
  },
  {
    "objectID": "14-fitting.html#what-does-it-mean-to-fit-data",
    "href": "14-fitting.html#what-does-it-mean-to-fit-data",
    "title": "\n13  Fitting\n",
    "section": "\n13.1 What does it mean to “fit data”?",
    "text": "13.1 What does it mean to “fit data”?\nIt might sound like a trivial question, but let’s make sure we’re all on the same page…\nFor the sake of the example, let’s take a look at some experimental measurements of the oscillation period T of a pendulum1. We have measured this period for various lengths L of the pendulum, and it results in such an evolution:\n\n\n\n\nFigure 13.1: Some experimental data showing the period of a pendulum measured as a function of its length.\n\n\n\n\nNow, if you recall your high school physics class, you might remember that the period of the pendulum T is given by:\n\\[\nT \\simeq 2\\pi\\sqrt{\\frac{L}{g}},\n\\tag{13.1}\\]\nwhere L is the pendulum length and g the acceleration of gravity. Such and experimental design, i.e. measuring T as a function of L, is therefore a means to determining the value of g.\nFor this, we need to perform a fit of our experimental data by a physical model, i.e. the one of Equation 13.1. When doing so, one has to distinguish between:\n\n\nVariables: the period T and the pendulum length L are variables, they are the quantities that vary for a given parameter.\n\nParameters: the gravity g is a parameter of the fit that we wish to determine, i.e. it is a constant for this set of measurements.\n\nSo now, what exactly is a fit? Fitting a model to experimental data means:\n\n\nFinding the model that best describe our data (a line, a Gaussian, a polynomial, Equation 13.1, etc.). You can try several models and compare them to determine which model is the best.\n\nFinding the set of parameters of the model that best describe our data. This is done through an optimization algorithm.\n\nHow do we know the parameters are the best ones to describe our data? For this, we use a tool called the residual error function, \\(\\chi^2\\), such as:\n\\[\n\\chi^2=\\sum_{i=1}^N\\left(y_i(x_i)-m_i(x_i, \\{p\\})\\right)^2,\n\\] where \\(y_i(x_i)\\) are the N experimental observations of the value \\(y\\) as a function of \\(x\\), and \\(m_i(x_i, \\{p\\})\\) is the value of the model as a function of \\(x\\) and the ensemble of parameters \\(\\{p\\}\\). We use the squared sum of residuals and not the simple sum of residuals because the residuals can either be positive or negative, and we are interested in minimizing the total distance from our model to our experimental data.\nApplied to our pendulum problem, we thus get:\n\\[\n\\chi^2=\\sum_{i=1}^N\\left(T_i-2\\pi\\sqrt{\\frac{L_i}{g}}\\right)^2,\n\\] where the summation is performed on the N points that we have recorded in Figure 13.1. In this case there is only one parameter, but you will encounter other models needing more than one parameter (for example if you’re fitting a peak, you’ll need it’s position, width, and height: 3 parameters). The rule of thumb is that the less parameters you introduce the better, “less” being compared to the number of data points.\nSo, let’s try a few values of g and see what is the resulting residual error \\(\\chi^2\\) in each case:\n\n\n\n\nFigure 13.2: Measuring the residual error \\(\\chi^2\\) for different values of the parameter g. The distances from each experimental data point to the model (i.e. the residuals) are shown in blue. The sum of these squared residuals yields the residual error \\(\\chi^2\\).\n\n\n\n\nWe see that there is a sweet spot in the values of g for which \\(\\chi^2\\) is minimum:\n\n\n\n\nFigure 13.3: Evolution of \\(\\chi^2\\) as a function of g. The sweet spot is found for the red value.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nFitting a model to experimental data is finding this sweet spot for the ensemble of parameters through an optimization procedure.\n\n\nThere are numerous algorithms out there to perform this optimization, but we will not delve into their mechanics.\nNow, let’s get back to R. You will mainly use two functions to perform these optimizations:\n\nWhen your experimental data follow a linear trend: lm()\nThe rest of the time: nls()\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that, whenever it is possible, it is usually preferable to make your problem a linear one, as it avoids the hassle of providing starting values. For example, in the pendulum case above, taking the square of Equation 13.1 results in a linear evolution of \\(T^2\\) with respect to \\(l\\)."
  },
  {
    "objectID": "14-fitting.html#linear-fitting-with-lm",
    "href": "14-fitting.html#linear-fitting-with-lm",
    "title": "\n13  Fitting\n",
    "section": "\n13.2 Linear fitting with lm()\n",
    "text": "13.2 Linear fitting with lm()\n\n\nLet’s learn how to do simple linear fits with R’s lm() and plot the results. Let’s start with some fake data stored in a tibble called d:\n\n\n\n\n\nWe see that this data shows a linear evolution, for which we might want to extract the slope and the intercept. This is very simply done by applying the lm() function, like so:\n\n# Fit with a linear model:\n# 3 equivalent ways of calling it\nfit <- lm(data = d, y ~ x)\nfit <- lm(d$y ~ d$x)\nfit <- d %>% lm(data=., y ~ x)\n\nHere, and everywhere else in R, the operator ~ is to be understood as “as a function of”. So with this bit of code, we tell R to “fit y as a function of x using a linear model”.\nNow to see the fit results, we can just display fit, or call summary(fit)\n\n# Summary of the fit\nfit\n\n#> \n#> Call:\n#> lm(formula = y ~ x, data = .)\n#> \n#> Coefficients:\n#> (Intercept)            x  \n#>      1.8703       0.1986\n\nsummary(fit)\n\n#> \n#> Call:\n#> lm(formula = y ~ x, data = .)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -0.13550 -0.06451 -0.02995  0.06564  0.18165 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  1.87030    0.07759   24.11 9.35e-09 ***\n#> x            0.19861    0.01250   15.88 2.47e-07 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.1136 on 8 degrees of freedom\n#> Multiple R-squared:  0.9693, Adjusted R-squared:  0.9654 \n#> F-statistic: 252.3 on 1 and 8 DF,  p-value: 2.471e-07\n\n\nTo actually retrieve and store the fit parameters, call coef(fit):\n\n# Retrieve the coefficients and errors\ncoef(fit)\n\n#> (Intercept)           x \n#>   1.8703047   0.1986085\n\ncoef(fit)[1]\n\n#> (Intercept) \n#>    1.870305\n\n\nTo get it properly stored in a tibble, see the broom package that we describe later in this chapter:\n\n# Summary of the fit\nbroom::tidy(fit)\n\n#> # A tibble: 2 × 5\n#>   term        estimate std.error statistic       p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>         <dbl>\n#> 1 (Intercept)    1.87     0.0776      24.1 0.00000000935\n#> 2 x              0.199    0.0125      15.9 0.000000247\n\n\nTo get the standard error of the fitted parameters and the R2:\n\nsummary(fit)$coefficients\n\n#>              Estimate Std. Error  t value     Pr(>|t|)\n#> (Intercept) 1.8703047 0.07758807 24.10557 9.351860e-09\n#> x           0.1986085 0.01250445 15.88303 2.471313e-07\n\nsummary(fit)$coefficients[\"x\", \"Std. Error\"]\n\n#> [1] 0.01250445\n\nsummary(fit)$r.squared\n\n#> [1] 0.9692628\n\n\nAnd finally, to plot the result of the fit:\n\n# Get the fitted paramters and make a string with it to be printed\nto_print <- paste(\"y = \", round(coef(fit)[1],2),\" + x*\",\n                    round(coef(fit)[2],2), sep=\"\")\n# Base plot\nplot(d, pch = 16, main = \"With base plot\", sub = to_print)\nabline(coef(fit), col=\"red\")\n\n\n\n# GGplot versions\nggplot(data=d, aes(x,y)) + \n    geom_point(cex=3) +\n    geom_abline(slope = coef(fit)[2], intercept = coef(fit)[1], col=\"red\") +\n    labs(title = \"With ggplot and the parameters you get from the external call to lm()\",\n         subtitle = to_print)\n\n\n\nggplot(data=d, aes(x,y)) + \n    geom_point(cex=3) +\n    geom_smooth(method=\"lm\") + # does the fit but does not allow saving the parameters\n    labs(title = \"With ggplot and the geom_smooth() function\",\n         subtitle = to_print)\n\n\n\n\nThe function geom_smooth() will fit the data and display the fitted line, but to retrieve the actual coefficients you still need to run lm().\nFinally, you may want to impose an intercept that will be 0 or a given value. For this, you will need to add +0 in the formula, like so:\n\nfit0 <- lm(data = d, y ~ x + 0) # intercept will be fixed in 0\nfit1.5 <- lm(data = d, y - 1.5 ~ x + 0) # intercept will be fixed in 1.5\n\n\nd %>% ggplot(aes(x,y)) + \n    geom_point(cex=3) +\n    geom_abline(slope = coef(fit0)[1], intercept = 0, col=\"red\")+\n    geom_abline(slope = coef(fit1.5)[1], intercept = 1.5, col=\"royalblue\")+\n    expand_limits(x = 0, y = 0)"
  },
  {
    "objectID": "14-fitting.html#nonlinear-least-squares-fitting",
    "href": "14-fitting.html#nonlinear-least-squares-fitting",
    "title": "\n13  Fitting\n",
    "section": "\n13.3 Nonlinear Least Squares fitting",
    "text": "13.3 Nonlinear Least Squares fitting\n\n13.3.1 The nls() workhorse\nYou can fit data with your own functions and constraints using nls(). Here is an example of data we may want to fit, stored into a tibble called df:\n\n\n\n\nggplot(data=df, aes(x,y))+\n    geom_point()+\n    ggtitle(\"Some fake data we want to fit with 2 Gaussians\")\n\n\n\n\nWe first need to define a function to fit our data. We see here that it contains two peaks that look Gaussian, so let’s go with the sum of two Gaussian functions:\n\n# Create a function to fit the data\nmyfunc <- function(x, y0, x1, x2, A1, A2, sd1, sd2) {\n    y0 +                                # baseline\n    A1 * dnorm(x, mean = x1, sd = sd1) +# 1st Gaussian\n    A2 * dnorm(x, mean = x2, sd = sd2)  # 2nd Gaussian\n}\n# Fit the data using a user function\nfit_nls <- nls(data=df,\n               y ~ myfunc(x, y0, x1, x2, A1, A2, sd1, sd2),\n               # Provide starting point for parameters values:\n               start = list(y0=0, x1=0, x2=1.5, \n                            sd1=.2, sd2=.2, \n                            A1=1, A2=1) \n               )\nsummary(fit_nls)\n\n#> \n#> Formula: y ~ myfunc(x, y0, x1, x2, A1, A2, sd1, sd2)\n#> \n#> Parameters:\n#>      Estimate Std. Error t value Pr(>|t|)    \n#> y0  -0.002168   0.003642  -0.595    0.553    \n#> x1  -0.004475   0.013103  -0.342    0.733    \n#> x2   2.027445   0.040470  50.097   <2e-16 ***\n#> sd1  0.520251   0.012234  42.524   <2e-16 ***\n#> sd2  0.960788   0.042475  22.620   <2e-16 ***\n#> A1   1.029825   0.031589  32.601   <2e-16 ***\n#> A2   0.961031   0.040063  23.988   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.02886 on 114 degrees of freedom\n#> \n#> Number of iterations to convergence: 8 \n#> Achieved convergence tolerance: 4.426e-06\n\n\nAnd to plot the data and the result of the fit, we use predict(fit) to retrieve the fitted y values:\n\n# With base R\nplot(x, y, pch=16)\nlines(x, predict(fit_nls), col=\"red\", lwd=2)\n\n\n\n# With ggplot2\nggplot(data=df, aes(x,y))+\n    geom_point(size=2, alpha=.5) +\n    geom_line(aes(y = predict(fit_nls)), color=\"red\", linewidth=1)\n\n\n\n\n\n13.3.2 Using constraints\nIn nls() it is even possible to constraint the fitting by adding lower and upper boundaries. These boundaries are useful when you want to give some physical meaning to your parameters, for example, like forcing the width and amplitude to be positive or above a certain minimum value. However, you have to be careful with these and not provide stupid ones, e.g.:\n\n# Constraining the upper and lower values of the fitting parameters\nfit_constr <- nls(data = df,\n                  y ~ myfunc(x, y0, x1, x2, A1, A2, sd1, sd2),\n                  start = list(y0=0, x1=0, x2=3, \n                               sd1=.2, sd2=.2, A1=1, A2=1),\n                  upper = list(y0=Inf, x1=Inf, x2=Inf, \n                               sd1=Inf, sd2=Inf, A1=2, A2=2),\n                  lower = list(y0=-Inf, x1=-Inf, x2=2.9, \n                               sd1=0, sd2=0, A1=0, A2=0),\n                  algorithm = \"port\"\n                 )\n# Plotting the resulting function in blue\nggplot(data=df, aes(x,y))+\n    ggtitle(\"Beware of bad constraints!\")+\n    geom_point(size=2, alpha=.5) +\n    geom_line(aes(y = predict(fit_constr)), color=\"royalblue\", linewidth=1)\n\n\n\n\n\n13.3.3 A more robust version of nls\n\nSometimes, nls() will struggle to converge towards a solution, especially if you provide initial guesses that are too far from the expected values.\n\nfit3 <- nls(data = df,\n            y ~ myfunc(x, y0, x1, x2, A1, A2, sd1, sd2),\n            start = list(y0 = 0, x1 = 1, x2 = 5, \n                         sd1 = .2, sd2 = .2, A1 = 10, A2 = 10)\n            )\n\n#> Error in numericDeriv(form[[3L]], names(ind), env, central = nDcentral): Missing value or an infinity produced when evaluating the model\n\n\nIn that case, you may want to use a more robust nls() function such as nlsLM() from the minpack.lm package.\n\nlibrary(minpack.lm)\nfit_nlsLM <- nlsLM(data = df,\n                   y ~ myfunc(x, y0, x1, x2, A1, A2, sd1, sd2),\n                   start = list(y0 = 0, x1 = 1, x2 = 5, \n                                sd1 = .2, sd2 = .2, A1 = 10, A2 = 10)\n                   )\nsummary(fit_nlsLM)\n\n#> \n#> Formula: y ~ myfunc(x, y0, x1, x2, A1, A2, sd1, sd2)\n#> \n#> Parameters:\n#>      Estimate Std. Error t value Pr(>|t|)    \n#> y0  -0.002168   0.003642  -0.595    0.553    \n#> x1  -0.004476   0.013103  -0.342    0.733    \n#> x2   2.027444   0.040470  50.097   <2e-16 ***\n#> sd1  0.520250   0.012234  42.524   <2e-16 ***\n#> sd2  0.960791   0.042475  22.620   <2e-16 ***\n#> A1   1.029824   0.031589  32.601   <2e-16 ***\n#> A2   0.961033   0.040063  23.988   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.02886 on 114 degrees of freedom\n#> \n#> Number of iterations to convergence: 19 \n#> Achieved convergence tolerance: 1.49e-08\n\n\nAlso, nlsLM() won’t fail when the fit is exact, whereas nls() will:\n\ntestdf <- tibble(x = seq(-10,10),\n                 y = dnorm(x))\nnls(data = testdf,\n    y ~ A*dnorm(x, sd=B, mean=x0) + y0,\n    start = list(y0=0, x0=0, A=1, B=1)\n    )\n\n#> Error in nls(data = testdf, y ~ A * dnorm(x, sd = B, mean = x0) + y0, : number of iterations exceeded maximum of 50\n\nnlsLM(data = testdf,\n    y ~ A*dnorm(x, sd=B, mean=x0) + y0,\n    start = list(y0=0, x0=0, A=1, B=1)\n    )\n\n#> Nonlinear regression model\n#>   model: y ~ A * dnorm(x, sd = B, mean = x0) + y0\n#>    data: testdf\n#> y0 x0  A  B \n#>  0  0  1  1 \n#>  residual sum-of-squares: 0\n#> \n#> Number of iterations to convergence: 1 \n#> Achieved convergence tolerance: 1.49e-08"
  },
  {
    "objectID": "14-fitting.html#the-broom-library",
    "href": "14-fitting.html#the-broom-library",
    "title": "\n13  Fitting\n",
    "section": "\n13.4 The broom library",
    "text": "13.4 The broom library\nThanks to the broom library, it is easy to retrieve all the fit parameters in a tibble:\n\nlibrary(broom)\n# Get all parameters and their error\ntidy(fit_nls)\n\n#> # A tibble: 7 × 5\n#>   term  estimate std.error statistic  p.value\n#>   <chr>    <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 y0    -0.00217   0.00364    -0.595 5.53e- 1\n#> 2 x1    -0.00448   0.0131     -0.342 7.33e- 1\n#> 3 x2     2.03      0.0405     50.1   1.77e-79\n#> 4 sd1    0.520     0.0122     42.5   8.94e-72\n#> 5 sd2    0.961     0.0425     22.6   5.85e-44\n#> 6 A1     1.03      0.0316     32.6   1.28e-59\n#> 7 A2     0.961     0.0401     24.0   2.30e-46\n\n# Get the fitted curve and residuals next to the original data\naugment(fit_nls)\n\n#> # A tibble: 121 × 4\n#>        x        y  .fitted   .resid\n#>    <dbl>    <dbl>    <dbl>    <dbl>\n#>  1  -5    0.0353  -0.00217  0.0374 \n#>  2  -4.9  0.0485  -0.00217  0.0507 \n#>  3  -4.8  0.00540 -0.00217  0.00756\n#>  4  -4.7  0.0119  -0.00217  0.0140 \n#>  5  -4.6 -0.00431 -0.00217 -0.00214\n#>  6  -4.5  0.0439  -0.00217  0.0461 \n#>  7  -4.4 -0.0410  -0.00217 -0.0388 \n#>  8  -4.3  0.0274  -0.00217  0.0296 \n#>  9  -4.2  0.0182  -0.00217  0.0203 \n#> 10  -4.1 -0.0388  -0.00217 -0.0367 \n#> # … with 111 more rows\n\n\nIt is then easy to make a recursive fit on your data without using a for loop, like so:\n\nlibrary(broom)\nlibrary(tidyverse)\nlibrary(ggplot2)\ntheme_set(theme_bw())\n# Create fake data\na <- seq(-10,10,.1)\ncenters <- c(-2*pi,pi,pi/6)\nwidths  <- runif(3, min=0.5, max=1)\namp     <- runif(3, min=2, max=10)\nnoise   <- .3*runif(length(a))-.15\nd <- tibble(x=rep(a,3),\n            y=c(amp[1]*dnorm(a,mean=centers[1],sd=widths[1])+sample(noise),\n                amp[2]*dnorm(a,mean=centers[2],sd=widths[2])+sample(noise),\n                amp[3]*dnorm(a,mean=centers[3],sd=widths[3])+sample(noise)),\n            T=rep(1:3, each=length(a))\n            )\n# Plot the data\nd %>% ggplot(aes(x=x, y=y, color=factor(T))) + \n    geom_point()\n\n\n\n# Fit all data\nd_fitted <- d %>% \n    nest(data = -T) %>%\n    mutate(fit = map(data, ~ nls(data = .,\n                     y ~ y0 + A*dnorm(x, mean=x0, sd=FW), \n                     start=list(A  = max(.$y),\n                                y0 = .01, \n                                x0 = .$x[which.max(.$y)], \n                                FW = .7)\n                     )),\n           tidied = map(fit, tidy),\n           augmented = map(fit, augment)\n          )\nd_fitted\n\n#> # A tibble: 3 × 5\n#>       T data               fit    tidied           augmented         \n#>   <int> <list>             <list> <list>           <list>            \n#> 1     1 <tibble [201 × 2]> <nls>  <tibble [4 × 5]> <tibble [201 × 4]>\n#> 2     2 <tibble [201 × 2]> <nls>  <tibble [4 × 5]> <tibble [201 × 4]>\n#> 3     3 <tibble [201 × 2]> <nls>  <tibble [4 × 5]> <tibble [201 × 4]>\n\n\nIn case you want to provide fit parameters that vary depending on the group you are looking at, use the notation .$column_name, like is done here.\nThen you can see the results for all your data at once:\n\n# data and fit resulting curve\nd_fitted %>% \n  unnest(augmented)\n\n#> # A tibble: 603 × 8\n#>        T data               fit    tidied       x       y .fitted  .resid\n#>    <int> <list>             <list> <list>   <dbl>   <dbl>   <dbl>   <dbl>\n#>  1     1 <tibble [201 × 2]> <nls>  <tibble> -10   -0.0776 0.00260 -0.0802\n#>  2     1 <tibble [201 × 2]> <nls>  <tibble>  -9.9 -0.0287 0.00262 -0.0313\n#>  3     1 <tibble [201 × 2]> <nls>  <tibble>  -9.8 -0.122  0.00264 -0.125 \n#>  4     1 <tibble [201 × 2]> <nls>  <tibble>  -9.7 -0.0603 0.00268 -0.0630\n#>  5     1 <tibble [201 × 2]> <nls>  <tibble>  -9.6  0.142  0.00275  0.139 \n#>  6     1 <tibble [201 × 2]> <nls>  <tibble>  -9.5 -0.137  0.00287 -0.139 \n#>  7     1 <tibble [201 × 2]> <nls>  <tibble>  -9.4  0.0142 0.00307  0.0111\n#>  8     1 <tibble [201 × 2]> <nls>  <tibble>  -9.3  0.120  0.00342  0.116 \n#>  9     1 <tibble [201 × 2]> <nls>  <tibble>  -9.2 -0.101  0.00399 -0.105 \n#> 10     1 <tibble [201 × 2]> <nls>  <tibble>  -9.1  0.114  0.00491  0.109 \n#> # … with 593 more rows\n\n# fit parameters\nd_fitted %>% \n  unnest(tidied)\n\n#> # A tibble: 12 × 9\n#>        T data     fit    term  estimate std.error statistic   p.value augmented\n#>    <int> <list>   <list> <chr>    <dbl>     <dbl>     <dbl>     <dbl> <list>   \n#>  1     1 <tibble> <nls>  A      4.77      0.0623     76.6   1.40e-148 <tibble> \n#>  2     1 <tibble> <nls>  y0     0.00259   0.00695     0.373 7.10e-  1 <tibble> \n#>  3     1 <tibble> <nls>  x0    -6.28      0.0102   -619.    0         <tibble> \n#>  4     1 <tibble> <nls>  FW     0.753     0.0106     71.2   1.38e-142 <tibble> \n#>  5     2 <tibble> <nls>  A      7.34      0.0641    114.    3.48e-182 <tibble> \n#>  6     2 <tibble> <nls>  y0     0.00226   0.00697     0.324 7.47e-  1 <tibble> \n#>  7     2 <tibble> <nls>  x0     3.15      0.00709   443.    2.10e-297 <tibble> \n#>  8     2 <tibble> <nls>  FW     0.791     0.00740   107.    2.12e-176 <tibble> \n#>  9     3 <tibble> <nls>  A      9.19      0.0571    161.    5.25e-211 <tibble> \n#> 10     3 <tibble> <nls>  y0    -0.00126   0.00674    -0.186 8.52e-  1 <tibble> \n#> 11     3 <tibble> <nls>  x0     0.517     0.00437   118.    4.74e-185 <tibble> \n#> 12     3 <tibble> <nls>  FW     0.671     0.00452   148.    3.80e-204 <tibble>\n\n# fit parameters as a wide table\nd_fitted %>% \n  unnest(tidied) %>% \n  select(T, term, estimate, std.error) %>% \n  pivot_wider(names_from = term, \n              values_from = c(estimate,std.error))\n\n#> # A tibble: 3 × 9\n#>       T estimate_A estimate_y0 estimat…¹ estim…² std.e…³ std.e…⁴ std.e…⁵ std.e…⁶\n#>   <int>      <dbl>       <dbl>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n#> 1     1       4.77     0.00259    -6.28    0.753  0.0623 0.00695 0.0102  0.0106 \n#> 2     2       7.34     0.00226     3.15    0.791  0.0641 0.00697 0.00709 0.00740\n#> 3     3       9.19    -0.00126     0.517   0.671  0.0571 0.00674 0.00437 0.00452\n#> # … with abbreviated variable names ¹​estimate_x0, ²​estimate_FW, ³​std.error_A,\n#> #   ⁴​std.error_y0, ⁵​std.error_x0, ⁶​std.error_FW\n\n# plot fit result\nd_fitted %>% \n    unnest(augmented) %>% \n    ggplot(aes(x=x, color=factor(T)))+\n        geom_point(aes(y=y), alpha=0.5, size=3) + \n        geom_line(aes(y=.fitted))\n\n\n\n# plot fit parameters\nd_fitted %>% \n  unnest(tidied) %>% \n  ggplot(aes(x=T, y=estimate, color=term))+\n    geom_point()+\n    geom_errorbar(aes(ymin=estimate-std.error,\n                      ymax=estimate+std.error),\n                  width=.1)+\n    facet_wrap(~term, scales=\"free_y\")+\n    theme(legend.position = \"none\")"
  },
  {
    "objectID": "14-fitting.html#exo-fits",
    "href": "14-fitting.html#exo-fits",
    "title": "\n13  Fitting\n",
    "section": "\n13.5 Exercises",
    "text": "13.5 Exercises\nInteractive exercises can be found in the tutor package. For this, simply run:\nlibrary(tutor)\ntuto(\"fits\")\nAlternatively, you can just download the archive with all the exercises files, unzip it in your R class RStudio project, and edit the R files.\n\nExercise 1\n\nLoad exo_fit.txt in a tibble.\nUsing lm() or nls() fit each column as a function of x and display the “experimental” data and the fit on the same graph.\n\nTip: Take a look at the function dnorm() to define a Gaussian\n\n\nExercise 2\n\nLoad the tidyverse library.\nDefine a function norm01(x), that, given a vector x, returns this vector normalized to [0,1].\nLoad the Raman spectrum rubis_01.txt, normalize it to [0,1] and plot it\nDefine the normalized Lorentzian function Lorentzian(x, x0, FWHM), defined by \\(L(x, x_0, FWHM)=\\frac{FWHM}{2\\pi}\\frac{1}{(FWHM/2)^2 + (x-x_0)^2}\\)\n\nGuess grossly the initial parameters and plot the resulting curve as a blue dashed line\nFit the data by a sum of 2 Lorentzians using nls()\n\nAdd the result on the plot as a red line\nAdd the 2 Lorentzian components as area-filled curves with alpha=0.2 and two different colors\n\n\nSolution\n\n# Load the `tidyverse` library.\nlibrary(tidyverse)\n# Define a function `norm01(x)`, that, given a vector `x`, returns this vector normalized to [0,1].\nnorm01 <- function(x) {(x-min(x))/(max(x)-min(x))}\n# Load rubis_1.txt, normalize it to [0,1] and plot it\nd <- read_table(\"Data/rubis_01.txt\", col_names=c(\"w\", \"Int\")) %>% \n    mutate(Int_n = norm01(Int))\nP <- d %>%\n    ggplot(aes(x=w, y=Int_n))+\n        geom_point(alpha=0.5)\nP\n\n\n\n# Define the Lorentzian function\nLorentzian <- function(x, x0=0, FWHM=1){\n    FWHM / (2*pi) / ((FWHM/2)^2 + (x - x0)^2)\n}\n# Guess grossly the initial parameters and plot the resulting curve as a blue dashed line\nP+geom_line(aes(y = 0.03 + \n                    3*Lorentzian(w, x0=3160, FWHM=10) +\n                    7*Lorentzian(w, x0=3210, FWHM=10)),\n            col=\"blue\", lty=2)\n\n\n\n# Fit the data by a sum of 2 Lorentzians using `nls`\nfit <- nls(data=d, \n           Int_n ~ y0 + A1*Lorentzian(w,x1,FWHM1) + A2*Lorentzian(w,x2,FWHM2), \n           start=list(y0=0.03,\n                      x1=3160, FWHM1=10, A1=3,\n                      x2=3200, FWHM2=10, A2=7))\nsummary(fit)\n\n#> \n#> Formula: Int_n ~ y0 + A1 * Lorentzian(w, x1, FWHM1) + A2 * Lorentzian(w, \n#>     x2, FWHM2)\n#> \n#> Parameters:\n#>        Estimate Std. Error   t value Pr(>|t|)    \n#> y0    9.825e-03  6.165e-04     15.94   <2e-16 ***\n#> x1    3.173e+03  3.503e-02  90576.82   <2e-16 ***\n#> FWHM1 1.076e+01  1.087e-01     98.96   <2e-16 ***\n#> A1    1.039e+01  8.028e-02    129.38   <2e-16 ***\n#> x2    3.203e+03  2.709e-02 118214.68   <2e-16 ***\n#> FWHM2 1.453e+01  8.600e-02    169.00   <2e-16 ***\n#> A2    2.112e+01  9.680e-02    218.19   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.01568 on 1008 degrees of freedom\n#> \n#> Number of iterations to convergence: 10 \n#> Achieved convergence tolerance: 7.651e-06\n\n# Add the result on the plot as a red line\nP + geom_line(aes(y=.03 + \n                    3*Lorentzian(w, x0=3160, FWHM=10) +\n                    7*Lorentzian(w, x0=3210, FWHM=10)),\n            col=\"blue\", lty=2)+\n    geom_line(aes(y=predict(fit)), col=\"red\")\n\n\n\n# Add the 2 Lorentzian components as area-filled curves with `alpha=0.2` and two different colors\ny0  <- coef(fit)['y0']\nA1  <- coef(fit)['A1'];   A2   <- coef(fit)['A2']\nx1  <- coef(fit)['x1'];   x2   <- coef(fit)['x2']\nFW1 <- coef(fit)['FWHM1']; FW2 <- coef(fit)['FWHM2']\nP + geom_line(aes(y=.03 + \n                    3*Lorentzian(w, x0=3160, FWHM=10) +\n                    7*Lorentzian(w, x0=3210, FWHM=10)),\n            col=\"blue\", lty=2)+\n    geom_line(aes(y = predict(fit)), col=\"red\")+\n    geom_area(aes(y = A1*Lorentzian(w, x0=x1, FWHM=FW1)), \n                fill=\"royalblue\", alpha=.2)+\n    geom_area(aes(y = A2*Lorentzian(w, x0=x2, FWHM=FW2)), \n                fill=\"orange\", alpha=.2)\n\n\n\n\nExercise 3\n\nCreate an Exercise_fit folder, and create an Rstudio project linked to this folder\n\nDownload the corresponding data files and unzip them in a folder called Data.\nCreate a new .R file in which you will write your code and save it.\nLike in the previous exercise, we will fit ruby Raman spectra, but we will do it on many files at once. First, using list.files(), save in flist the list of ruby files in the Data folder.\nDefine the Lorentzian(x, x0, FWHM) function\nDefine the norm01(x) function returning the vector x normalized to [0,1]\nCreate a read_ruby(filename) function that, given a file name filename, reads the file into a tibble, gives the proper column names, normalizes the intensity to [0,1], and returns the tibble.\nCreate a fitfunc(tib) function that, given a tibble tib, fits this tibble’s y values as a function of the x values using the sum of two Lorentzians, and returns the nls() fit result. Make sure to provide “clever” starting parameters, especially for the positions of the two peaks. For example, one peak is where the spectrum maximum is, and the other one is always at an energy roughly 30 cm-1 lower.\nUsing pipe operations and map(), recursively:\n\nCreate a tibble with only one column called file, which contains the list of file names flist.\nCreate a column data containing a list of all read files, obtained by mapping read_ruby() onto flist.\nCreate a column fit containing a list of nls() objects, obtained by mapping fitfunc() onto the list (column) data\n\nCreate a column tidied containing a list of tidied nls() tibbles, obtained by mapping bbroom::tidy() onto the list (column) fit\n\nCreate a column augmented containing a list of augmented nls() tibbles, obtained by mapping bbroom::augment() onto the list (column) fit\n\nTurn the file column into run that will contain the run number (i.e. the number in the file name) as a numeric value. Use the function separate() to do so.\n\n\nPlot all the experimental data and the fit with black points and red lines into a faceted plot. Play with various ways of plotting this, like a ridge or slider plot.\nPlot the evolution of all fitting parameters as a function of the run number.\nPlot the evolution of the position of the higher energy peak as a function of the file name.\nFit linearly the evolution of the higher intensity peaks with respect to the run number. Display the fitted parameters and R2. Plot the fit as a red line. Make sure to use proper weighing for the fit.\n\n\nSolution\n\n# - Using `list.files()`, save in `flist` the list of ruby files in the `Data` folder.\nflist <- list.files(path = \"Data\", pattern = \"rubis\", full.names = TRUE)\n# - Define the `Lorentzian(x, x0, FWHM)` function\nLorentzian <- function(x, x0 = 0, FWHM = 1) {\n    2 / (pi * FWHM) / (1 + ((x - x0) / (FWHM / 2))^2)\n}\n# - Define the `norm01(x)` function returning the vector x normalized to [0,1]\nnorm01 <- function(x) {\n    (x - min(x)) / (max(x) - min(x))\n}\n# - Create a `read_ruby(filename)` function that, given a file name `filename`, \n# reads the file into a tibble, gives the proper column names, normalizes the \n# intensity to [0,1], and returns the tibble.\nread_ruby <- function(filename){\n    read_table(filename, col_names = c(\"w\", \"Int\")) %>% \n        mutate(Int = norm01(Int))\n}\n# - Create a `fitfunc(tib)` function that, given a tibble `tib`, fits this \n# tibble's *y* values as a function of the *x* values using the sum of two Lorentzians, \n# and returns the `nls()` fit result. Make sure to provide \"clever\" starting parameters, \n# especially for the positions of the two peaks. For example, one peak is where \n# the spectrum maximum is, and the other one is always at an energy roughly 30 cm^-1^ lower.\nfitfunc <- function(tib){\n    nls(data=tib,\n        Int ~ y0 + A1*Lorentzian(w,x1,FWHM1)+\n                   A2*Lorentzian(w,x2,FWHM2), \n           start=list(y0=0.03,\n                      x1=tib$w[which.max(tib$Int)] - 30, \n                      FWHM1=10, \n                      A1=max(tib$Int)*10,\n                      x2=tib$w[which.max(tib$Int)], \n                      FWHM2=10, \n                      A2=max(tib$Int)*10)\n        )\n}\n# Test it:\n# df <- read_ruby(flist[1])\n# fitfunc(df)\n# \n# - Using pipe operations and `map()`, recursively:\n#     - Create a tibble with only one column called `file`, which contains the \n#       list of file names `flist`.\n#     - Create a column `data` containing a list of all read files, \n#       obtained by mapping `read_ruby()` onto `flist`.\n#     - Create a column `fit` containing a list of `nls()` objects, \n#       obtained by mapping `fitfunc()` onto the list (column) `data`\n#     - Create a column `tidied` containing a list of tidied `nls()` tibbles, \n#       obtained by mapping `bbroom::tidy()` onto the list (column) `fit`\n#     - Create a column `augmented` containing a list of augmented `nls()` tibbles, \n#       obtained by mapping `bbroom::augment()` onto the list (column) `fit`\n#     - Turn the `file` column into `run` that will contain the run number \n#       (*i.e.* the number in the file name) as a numeric value. \n#       Use the function `separate()`{.R} to do so.\ndata <- tibble(file=flist) %>% \n    mutate(data = map(file, read_ruby),\n           fit = map(data, fitfunc),\n           tidied = map(fit, tidy),\n           augmented = map(fit, augment)\n           ) %>% \n    separate(file, c(NA, NA, \"run\", NA), convert = TRUE)\n# - Plot all the experimental data and the fit with black points and red lines \n# into a faceted plot. Play with various ways of plotting this, \n# like a ridge or slider plot.\ndata %>% \n    unnest(augmented) %>% \n    ggplot(aes(x = w, y = Int)) +\n       geom_point(alpha=0.5)+\n       geom_line(aes(y=.fitted), col=\"red\")+\n       facet_wrap(~run)\n\n\n\ndata %>% \n    unnest(augmented) %>% \n    ggplot(aes(x = w, y = Int + as.numeric(factor(run)), group=run)) +\n       geom_point(alpha=0.5)+\n       geom_line(aes(y = .fitted + as.numeric(factor(run))), col = \"red\")\n\n\n\nP <- data %>%\n    unnest(augmented) %>%\n    ggplot(aes(x = w, y = Int, frame=run)) +\n        geom_point(alpha = 0.5) +\n        geom_line(aes(y = .fitted), col = \"red\")\nlibrary(plotly)\nggplotly(P, dynamicTicks = TRUE) %>% \n    animation_opts(5)%>%\n    layout(xaxis = list(autorange=FALSE, range = c(3050, 3550)))\n\n\n\n\n# - Plot the evolution of all fitting parameters as a function of the run number.\ndata %>%\n    unnest(tidied) %>% \n    ggplot(aes(x = run, y = estimate)) +\n        geom_point()+\n        facet_wrap(~term, scales = \"free\")+\n        geom_errorbar(aes(ymin = estimate - std.error,\n                          ymax = estimate + std.error),\n                      width=0.5)\n\n\n\n# - Plot the evolution of the position of the higher intensity peaks.\ndata %>%\n    unnest(tidied) %>% \n    filter(term == \"x1\") %>% \n    ggplot(aes(x = run, y = estimate)) +\n        geom_point()+\n        geom_errorbar(aes(ymin = estimate - std.error,\n                          ymax = estimate + std.error),\n                      width=0.5)\n\n\n\n# - Fit linearly the evolution of the higher intensity peaks with respect to the \n#   run number. Display the fitted parameters and R^2^.Plot the fit as a red line. \n#   Make sure to use proper weighing for the fit.\nlmfit <- data %>%\n    unnest(tidied) %>%\n    filter(term == \"x1\") %>% \n    lm(data=., \n       estimate ~ run, \n       weights = 1/std.error^2)\ntidy(lmfit)\n\n#> # A tibble: 2 × 5\n#>   term        estimate std.error statistic  p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept)  3169.      0.557     5686.  6.39e-49\n#> 2 run             6.99    0.0756      92.6 4.23e-22\n\nglance(lmfit)\n\n#> # A tibble: 1 × 12\n#>   r.squared adj.r.squa…¹ sigma stati…²  p.value    df logLik   AIC   BIC devia…³\n#>       <dbl>        <dbl> <dbl>   <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>\n#> 1     0.998        0.998  57.6   8566. 4.23e-22     1  -36.0  78.1  80.6  49695.\n#> # … with 2 more variables: df.residual <int>, nobs <int>, and abbreviated\n#> #   variable names ¹​adj.r.squared, ²​statistic, ³​deviance\n\ndata %>%\n    unnest(tidied) %>% \n    filter(term == \"x1\") %>% \n    ggplot(aes(x = run, y = estimate)) +\n        geom_point()+\n        geom_errorbar(aes(ymin = estimate - std.error,\n                          ymax = estimate + std.error),\n                      width=0.5)+\n        geom_line(aes(y = predict(lmfit)), col=\"red\")"
  },
  {
    "objectID": "15-rmarkdown.html#what-is-markdown",
    "href": "15-rmarkdown.html#what-is-markdown",
    "title": "\n14  Writing documents with Rmarkdown\n",
    "section": "\n14.1 What is markdown?",
    "text": "14.1 What is markdown?\nMarkdown is a simplified language that can be used to produce a variety of rich documents: a single .md file can be compiled and outputted to .docx, .odt, .html, .rtf, .pdf or .tex. This output format as well as various options (font size, template, table of contents, numbered sections…) are specified in a YAML header, i.e. text between two lines like that --- (we’ll see an example later).\nIn markdown can be embedded \\(\\LaTeX\\) code for displaying math expressions, html tags for more complicated html stuff, and the rest of the formatting is given by a few easy commands:\n# Title\n## sub-title\n### sub-sub-title\n...\n###### sub_6-title\n**bold**\n*italic*\n[link](http://google.com/)\n![image caption](image.png)\n\nTable:\n|   |   |\n|---|---|\n|   |   |\n\nUnordered list:\n- bla\n- bla bla\n\nOrdered list:\n1. bla\n1. bla bla\n\nLaTeX code: $\\int_{-\\infty}^{\\infty} e^{-x^2}=\\sqrt{\\pi}$\n\nHTML code: <a href=\"some_link\">text</a>\nYou can also add in-line code by writing text between back-ticks:\nText with `in-line code`\nwill render as: Text with in-line code\nFor more commands, you can for example get a digested cheat-sheet here and a tutorial here."
  },
  {
    "objectID": "15-rmarkdown.html#and-rmarkdown",
    "href": "15-rmarkdown.html#and-rmarkdown",
    "title": "\n14  Writing documents with Rmarkdown\n",
    "section": "\n14.2 … and Rmarkdown?",
    "text": "14.2 … and Rmarkdown?\nRmarkdown is basically the same thing as markdown (extension: .Rmd instead of .md), with the difference that code chunks in which you specify the language between accolades will be computed and the result will be displayed.\n```r\n# Not computed\n1+1\n```\n```{r}\n# Computed\n1+1\n```\n\n\n#> [1] 2\n\n\nAnd in-line code can be computed and rendered:\nIn-line code `r 1+2`\nwill render as: In-line code 3.\nThis very webpage is fully written in Rmarkdown (see the “View book source” at the bottom of the left sidebar).\nRmarkdown supports a number of languages:\n\nnames(knitr::knit_engines$get())\n\n#>  [1] \"awk\"       \"bash\"      \"coffee\"    \"gawk\"      \"groovy\"    \"haskell\"  \n#>  [7] \"lein\"      \"mysql\"     \"node\"      \"octave\"    \"perl\"      \"php\"      \n#> [13] \"psql\"      \"Rscript\"   \"ruby\"      \"sas\"       \"scala\"     \"sed\"      \n#> [19] \"sh\"        \"stata\"     \"zsh\"       \"asis\"      \"asy\"       \"block\"    \n#> [25] \"block2\"    \"bslib\"     \"c\"         \"cat\"       \"cc\"        \"comment\"  \n#> [31] \"css\"       \"ditaa\"     \"dot\"       \"embed\"     \"eviews\"    \"exec\"     \n#> [37] \"fortran\"   \"fortran95\" \"go\"        \"highlight\" \"js\"        \"julia\"    \n#> [43] \"python\"    \"R\"         \"Rcpp\"      \"sass\"      \"scss\"      \"sql\"      \n#> [49] \"stan\"      \"targets\"   \"tikz\"      \"verbatim\"  \"ojs\"       \"mermaid\"  \n#> [55] \"include\"   \"glue\"      \"glue_sql\"  \"gluesql\"\n\n\nAnd python and R code chunks can communicate thanks to the reticulate package.\nSo… are you starting to see the power of this tool…?\nBasically, you can use the best language for each task and combine it in a single Rmd file that will display text and images that are computed at each compilation of the Rmd file: you can fully automatize your data treatment and reporting.\nExample:\n\nchunk 1: bash, call a program that creates some files\nchunk 2: python, call a program that do some big computation on these files\nchunk 3: R, do some data treatment, plot the data"
  },
  {
    "objectID": "15-rmarkdown.html#further-readings-and-ressources",
    "href": "15-rmarkdown.html#further-readings-and-ressources",
    "title": "\n14  Writing documents with Rmarkdown\n",
    "section": "\n14.3 Further readings and ressources",
    "text": "14.3 Further readings and ressources\nThere are numerous ressources out there to help you on your Rmarkdown journey:\n\nRmarkdown cheatsheet\nCode chunks\nLanguages\n\nThe Rmarkdown gallery: to help you chose between formats and templates, with example codes\n\nThe Rmarkdown cookbook: just go there when you have a question about Rmarkdown.\n\nThe Rmarkdown definitive guide: ibidem.\n\nflexdashboard: a package to make dashboards\n\nrticles: a package with a collection of pdf templates for scientific articles\n\nrmdformats: a package with a collection of html templates\n\nstevetemplates: a collection of pdf templates by Steve Miller\n\nRexams: a package to help define exams with included/hidden solution\n\nThesisdown: a package to help you write your thesis with Rmarkdown\n\nvitae: a package to help you write your CV using Rmarkdown, with helpful functions to add content to the documents.\n\nI also provide a collection of examples on the github repo of this class:\n\nHTML outputs:\n\nSimple page\nDashboard\nGitbook\nBS4 book (like this course)\nBook\nioslide\n\n\nPDF outputs:\n\nOne- and two-columns documents with template\nBeamer presentation\nBook\nExercises with hidden solution\nData-driven CV\n\n\nMicrosoft Office outputs:\n\nPowerPoint\nWord (with template)"
  },
  {
    "objectID": "15-rmarkdown.html#example-Rmd",
    "href": "15-rmarkdown.html#example-Rmd",
    "title": "\n14  Writing documents with Rmarkdown\n",
    "section": "\n14.4 Example",
    "text": "14.4 Example\nDownload biblio.bib and nature.csl and put them at the root of your project.\nCreate an example.Rmd file with the following YAML header (the indentation is important):\n---\ntitle  : Your Title\nauthor : John Doe\ndate   : `r format(Sys.time(), '%d/%m/%Y')`\noutput :\n    html_document:\n        toc            : true\n        toc_float      : true\n        highlight      : tango\n        number_sections: false\n        code_folding   : hide    \nbibliography: \"biblio.bib\"\ncsl         : \"nature.csl\"\n---\nHow to understand this header:\n\n\n---: surrounds the YAML header. The body comes after that.\n\ntitle, author and date: easy. The date can automatically be set to the current one by setting it to:\n\n`r format(Sys.time(), '%d/%m/%Y')`\n\n\noutput: this tells pandoc the output format you want. When compiling, in case there are multiple entries, the compiler will only look to the first one. Click the links below to see more options.\n\nIn this case, the output will be an html document with a floating table of contents, unnumbered sections and all code chunks will be hidden with a button to allow showing them.\n\nOther output formats allowed:\n\nword_document\npdf_document\nrtf_document\nbeamer_presentation\n…\n\n\nUseful options:\n\nWord document: you can supply a reference style (template) to use. Just write a dummy .docx file in which you edit the style, save it as word-template.docx (for example), and set the option reference_docx: word-template.docx.\nLaTeX: keep_tex: when pandoc compiles your markdown file to a PDF, it goes through the intermediate step of creating a .tex file. You can decide to keep it for tweaking the style of the PDF output, like you would normally do with a .tex file.\n\nfig_caption: allows for figure caption.\n\ntoc: creates a table of contents.\n\nnumber_sections: allows for section numbering.\n\nhighlight: syntax highlighting theme for the code chunks.\n\n\n\n\n\nbibliography: path to your .bib file. To create a bibliography, add a # References header at the end of your document.\n\ncsl: path to the bibliography style for the output – in this example, nature.csl. Find your style or edit your own.\nThere are many other options, but with this you’ll fulfill most of your needs.\n\nNow you can start adding some content, like:\n---\ntitle: \"The title\"\noutput: \n    bookdown::html_document2: #instead of html_document, for referencing images\n        toc            : true\n        toc_float      : true\n        highlight      : tango\n        number_sections: true\n        code_folding   : show\n        code_download  : true\n    # bookdown::pdf_document2 #instead of pdf_document, for referencing images\nbibliography: \"biblio.bib\"\ncsl         : \"nature.csl\"\n---\n\n# First section\n## First subsection\nI am writing _italic_ and __bold__ stuff.\n\n- This is an item\n- Another one with citations [@bevan_statistical_2013;@rcoreteam_language_2017]\n\n# Second section\n## First subsection that I want to refer to {#subsectionID}\n\nThis is a text with a footnote[^1].\n\nNow I can refer to my subsection using `\\@ref(subsectionID)` like so:\nsection \\@ref(subsectionID).\n\nThis is an image with a caption:\n\n```{r CHUNKname, echo=FALSE, fig.cap=\"This is a very nice caption\", out.width=\"50%\", fig.align='center'}\nknitr::include_graphics(\"https://cdn.foliovision.com/images/2017/03/i-love-markdown.png\")\n```\n\nAnd I can refer to this figure using `\\@ref(fig:CHUNKname)`. \nExample : Figure \\@ref(fig:CHUNKname).\n\nHere is a code chunk in R in Fig. \\@ref(fig:Rfigure):\n```{r Rfigure, fig.cap=\"Test figure in R\", fig.align='center'}\nx <- seq(0,10,.1)\nplot(x, sin(x), main=\"R plot\")\n```\n\nAnd one in python in Fig. \\@ref(fig:Pythonfigure):\n```{python Pythonfigure, fig.cap=\"Test figure in python\", fig.align='center'}\n# Load some libraries\nimport numpy as np\n# Matplotlib non interactive plots\nimport matplotlib.pyplot as plt\nN = 100\nx = np.linspace(0,10,N)\nplt.plot(x, np.sin(x))\nplt.title(\"Matplotlib Plot\")\nplt.ylabel(\"Y values\")\nplt.xlabel(\"X values\")\nplt.show()\n```\n\n# References\n\n[^1]: This is a footnote.\nNote that when you choose to output to an html format, you can’t use PDF images: use .svg (pdf2svg) or other non vectorial images.\nWhat’s nice with html output, it’s that you can include interactive figures with plotly like we saw in the previous sections. Of course, this won’t work with static documents like PDF or Word…\nFor cross referencing to figures and tables, use output: bookdown::html_document2 instead of output: html_document (requires the package bookdown), and see here for details."
  },
  {
    "objectID": "15-rmarkdown.html#code-chunks-options",
    "href": "15-rmarkdown.html#code-chunks-options",
    "title": "\n14  Writing documents with Rmarkdown\n",
    "section": "\n14.5 Code chunks options",
    "text": "14.5 Code chunks options\nYou can add options to a code chunk, like:\n\n\necho=FALSE: hide the code, show the output\n\ninclude=FALSE: hide the code and the output\n\nwarnings=FALSE: hide the warning messages\n\nerror=TRUE: will compute the code chunk despite errors\n\ncache=TRUE: cache the result of the chunk for faster re-compilation\n\nfig.asp: figure aspect ratio\n\nfig.caption: figure caption\n…and more"
  },
  {
    "objectID": "15-rmarkdown.html#compilation",
    "href": "15-rmarkdown.html#compilation",
    "title": "\n14  Writing documents with Rmarkdown\n",
    "section": "\n14.6 Compilation",
    "text": "14.6 Compilation\nTo knit your Rmd file to the desired output, in Rstudio, click the little triangle next to the “Knit” button, like this:\n\n\n\n\n\nThe corresponding output file will be created in the same folder."
  },
  {
    "objectID": "15-rmarkdown.html#going-further-parameters",
    "href": "15-rmarkdown.html#going-further-parameters",
    "title": "\n14  Writing documents with Rmarkdown\n",
    "section": "\n14.7 Going further: parameters",
    "text": "14.7 Going further: parameters\nYou can even provide a list of parameters to your Rmarkdown file that you can retrieve in code chunks with params$param_name. More information here, but here is a short example: try compiling it directly using the “Knit” button, and then try compiling it using “Knit with parameters”. Alternatively, you can have the popup interactive window by rendering with the command rmarkdown::render(\"MyDocument.Rmd\", params = \"ask\").\n---\ntitle: \"Test\"\noutput: html_document\nparams:\n  city:\n    label: \"City:\"\n    value: Angers\n    input: select\n    multiple: TRUE\n    choices: [Angers, Paris, Montpellier, Nantes, Marseille]\n  printcode:\n    label: \"Display Code\"\n    value: TRUE\n  data:\n    label: \"Input dataset:\"\n    value: \"Data/population.txt\"\n    input: file\n---\n\n```{r, setup, include=FALSE}\n# set this option in the first code chunk in the document\nknitr::opts_chunk$set(echo = params$printcode)\n```\n\nPlotting `r paste(params$data)`:\n\n```{r, message=FALSE}\nlibrary(ggplot2)\nlibrary(plotly)\ndf <- read.table(params$data, header=TRUE)\np <- ggplot(data=subset(df, city %in% params$city), \n            aes(x=year, y=pop, size=pop, color=city)) +\n        geom_point() + \n        geom_smooth(method=\"lm\", aes(fill=city), alpha=0.1, show.legend = FALSE) + \n        ggtitle(paste(\"Population in \",paste(params$city, collapse=\", \"),sep=\"\"))+\n        labs(x=\"Year\", y=\"Population\")+\n        theme_light()\nggplotly(p)\n```\nThe “Knit with parameters” launches a Shiny user interface allowing interactively choosing the parameters. All options are listed here."
  },
  {
    "objectID": "15-rmarkdown.html#and-what-about-jupyter-notebooks",
    "href": "15-rmarkdown.html#and-what-about-jupyter-notebooks",
    "title": "\n14  Writing documents with Rmarkdown\n",
    "section": "\n14.8 And what about JuPyteR notebooks?",
    "text": "14.8 And what about JuPyteR notebooks?\nJuPyteR notebooks are basically a web-based interactive version or Rmarkdown documents working with Julia, Python and R.\nBut the installation is a tad complicated for the non initiate, and, well, it is just like Rmarkdown but less easy to use and share, in my opinion. But if you mostly work with python, then you have to get used to it since Rmarkdown is better suited for R."
  },
  {
    "objectID": "15-rmarkdown.html#welcome-to-the-future-quarto",
    "href": "15-rmarkdown.html#welcome-to-the-future-quarto",
    "title": "\n14  Writing documents with Rmarkdown\n",
    "section": "\n14.9 Welcome to the future: Quarto",
    "text": "14.9 Welcome to the future: Quarto\nQuarto is a standalone project from the Rstudio team built on pandoc that lookis a lot like Rmarkdown, but tailored not only for R but also for python, julia and observable.\nAll your Rmd files should be directly compatible with quarto upon changing the extension name to qmd. Then I invite you to explore the new possibilities offerd by quarto."
  },
  {
    "objectID": "15-rmarkdown.html#exo-rmd",
    "href": "15-rmarkdown.html#exo-rmd",
    "title": "\n14  Writing documents with Rmarkdown\n",
    "section": "\n14.10 Exercise",
    "text": "14.10 Exercise\nExercise 1\n\nCreate a new R Markdown file (.Rmd) in RStudio.\nInsert a YAML Header with title, author and date of your choice at the top of your .Rmd script. Make it so that the output is an html document.\nDisplay the summary of “cars” dataset in your report.\nMake a plot of the “cars” dataset under the summary you just created.\nCreate a small experimental dataframe and display it in your report.\nHide the code from your the last code chunk (HINT: Use echo.)\nHide the code from all code chunks with the possibility to show it (HINT: use code_folding).\nLoad the package “knitr” in your .Rmd file. and hide the code chunk. HINT: Use include.\nHide the warning message that appeared in your report. HINT: Use warning.\nSet fig.width and fig.height of your plot to 5.\nOutput the same report as pdf.\nExercise 2\nCreate an html experimental logbook doing all the data treatment in the example in the previous section.\nExercise 3\nCreate an html experimental logbook doing all the data treatment in the example in the previous section for a single experimental file that you will provide as a parameter in the YAML header. The file path must be given as an interactive file input selection."
  },
  {
    "objectID": "16-shiny.html#stand-alone-shiny-application",
    "href": "16-shiny.html#stand-alone-shiny-application",
    "title": "\n15  Graphical interfaces with Shiny\n",
    "section": "\n15.1 Stand-alone shiny application",
    "text": "15.1 Stand-alone shiny application\nA shiny application is an app.R file (it must be named like that) containing 3 elements:\n\n\nui: definition of the interface layout (where are the buttons, text input, plot output, etc.) and the input parameters\n\nserver: definition of the various actions to perform with the input parameters\n\nshinyApp(ui, server): launches the shiny app with the above defined parameters\n\nIn Rstudio, create a new “Shiny web app”. It will create an app.R file containing this:\nlibrary(shiny)\n# Define UI for application that draws a histogram\nui <- fluidPage(\n    # Application title\n    titlePanel(\"Old Faithful Geyser Data\"),\n    # Sidebar with a slider input for number of bins \n    sidebarLayout(\n        sidebarPanel(\n            sliderInput(\"bins\",\n                        \"Number of bins:\",\n                        min = 1,\n                        max = 50,\n                        value = 30)\n        ),\n        # Show a plot of the generated distribution\n        mainPanel(\n           plotOutput(\"distPlot\")\n        )\n    )\n)\n# Define server logic required to draw a histogram\nserver <- function(input, output, session) {\n    output$distPlot <- renderPlot({\n        # generate bins based on input$bins from ui.R\n        x    <- faithful[, 2]\n        bins <- seq(min(x), max(x), length.out = input$bins + 1)\n\n        # draw the histogram with the specified number of bins\n        hist(x, breaks = bins, col = 'darkgray', border = 'white')\n    })\n}\n# Run the application \nshinyApp(ui = ui, server = server)\nRun it by clicking “Run App”: a window opens and you can pan the slider and see the resulting output.\nIn case you want to clean up the code, you can separate app.R into ui.R and server.R. No need to add the shinyApp(ui = ui, server = server) line in that case.\nAll user-defined functions and variable definitions can be defined in a global.R file that will be sourced by default when launching the app.\n\n15.1.1 The layout\nIn the ui <- fluidPage(...) item, you define the layout of your application. In the above example:\n\n\ntitlePanel(\"Title\") creates a title\n\nsidebarLayout() separates the layout in a short one on the left (sidebarPanel()) and a main one on the right (mainPanel())\n\nsliderInput(\"name_of_slider\", \"text to display\", min=min_value, max=max_value, value=current_value, step=step_value) creates a slider to input a value. This value will be retrieved by input$name_of_slider in the server() function.\n\nplotOutput(\"name_of_plot\") plots the result of output$name_of_plot defined in the server() function.\n\nSee the guide to application layout for more layout options. I also recommend taking a look at the packages shinydashboard and shinymaterial.\n\n15.1.2 The server\nIn the server <- function(input, output){...} function, you define the various actions and outputs in reaction to an input change.\nIn the above example, we define output$distPlot as a renderPlot() function whose results depends on input$bins. The plot is rendered in the ui by plotOutput(\"distPlot\").\n\n15.1.3 Various useful functions\n\n\n\nInput\n\nButtons\nCheckbox\nText/numeric\nSlider\nFile\nDropdown menu\n\n\n\nOutput\n\nDisplay a plot\nDisplay text\nDisplay a table\nReactive events\nWriting a file\n\n\n\n\n\n15.1.3.1 Input\n\n\n15.1.3.1.1 Buttons\n# # # # # # # # # \n# In ui:\nactionButton(\"button_name\", \"Text to display\")\n# # # # # # # # # \n# In server:\nobserveEvent(input$button_name, {\n    # do something\n})\n# or\nsome_function <- eventReactive(input$button_name, {\n                               # do something\n                               })\n\n15.1.3.1.2 Checkbox\n# # # # # # # # # \n# In ui:\ncheckboxInput(\"checkbox_name\", \"Text to display\", value=FALSE)\n# # # # # # # # # \n# In server:\ninput$checkbox_name #TRUE or FALSE\n\n\n15.1.3.1.3 Text/numeric\n# # # # # # # # # \n# In ui:\ntextInput(\"text_name\", \n            label = \"Text to display\", \n            value = \"initial value\", \n            width = '100%')\n\ntextAreaInput(\"text_name\", \n            label=\"Text to display\", \n            value = \"initial_value\", \n            rows = 5) %>%\n            shiny::tagAppendAttributes(style = 'width: 100%;')\n\nnumericInput(\"value_name\", 'Text to display', value=0)\n# # # # # # # # # \n# In server, retrieve it as:\ninput$text_name\ninput$value_name\n\n\n15.1.3.1.4 Slider\n# # # # # # # # # \n# In ui:\nsliderInput(\"slider_name\", \"Text to display\",\n            min = 1,\n            max = 50,\n            step= 1,\n            value = 30)\n# # # # # # # # # \n# In server, retrieve it as:\ninput$slider_name\n\n\n15.1.3.1.5 File\n# # # # # # # # # \n# In ui:\nfileInput(\"file_in\", \n          \"Choose input file:\", accept = c(\".txt\") \n          )\n# # # # # # # # # \n# In server, retrieve it as:\ninput$file_in$datapath\n# For example, read it as a data.frame with myData():\nmyData <- reactive({\n        inFile <- input$file_in\n        if (is.null(inFile)) {\n            return(NULL)\n        } else {\n            return(read.table(inFile$datapath, header=TRUE))\n        }\n    })\n\n\n15.1.3.1.6 Dropdown menu\n# # # # # # # # # \n# In ui:\nselectInput(\"menu_name\", \"Text to display\", \n            choices=c(\"choice 1\", \"choice 2\"), \n            multiple = FALSE # multiple selection possible\n            )\n# # # # # # # # # \n# In server, retrieve it as:\ninput$menu_name\n\n\n15.1.3.2 Output\n\n\n15.1.3.2.1 Display a plot\n# # # # # # # # # \n# In ui:\nplotOutput(\"plot_name\", height = 600,\n           click = \"plot_click\", # to retrieve the click position\n           dblclick = \"plot_dblclick\", # to retrieve the double click position\n           hover = \"plot_hover\", # to retrieve the mouse position\n           brush = \"plot_brush\" # to retrieve the rectangle coordinates\n           )\n# # # # # # # # # \n# In server:\noutput$plot_name <- renderPlot({\n        # do plot:\n        plot(...)\n        # or\n        ggplot(...)\n    })\nIf you want an interactive plot, use plotlyOutput() and renderPlotly() instead.\n\n\n15.1.3.2.2 Display text\n# # # # # # # # # \n# In ui:\ntextOutput(\"text_to_display\")\n# Verbatim text (fixed width characters):\nverbatimTextOutput(\"text_to_display\")\n# # # # # # # # # \n# In server:\noutput$text_to_display <- renderText({ \"some text\" })\noutput$text_to_display <- renderPrint({ \"some text\" })\n\n\n15.1.3.2.3 Display a table\n# # # # # # # # # \n# In ui:\ntableOutput(\"table_to_display\")\n# # # # # # # # # \n# In server:\noutput$table_to_display <- renderTable({ df })\nOr in case you want interactive tables, use the package datatable:\nlibrary(DT)\n# # # # # # # # # \n# In ui:\ndataTableOutput(\"table_to_display\")\n# # # # # # # # # \n# In server:\noutput$table_to_display <- renderDataTable({ df })\n\n\n15.1.3.2.4 Reactive events\nIn case you want the plots or text display to react to a change in input value, you can wrap the corresponding code in the reactive() function on the server side:\n# # # # # # # # # \n# In ui:\nfileInput(\"file_in\", \n          \"Choose input file:\", accept = c(\".txt\") \n          ),\ncheckboxInput(\"header\", \"Header?\", value=TRUE),\nselectInput(\"menu\", \"Columns to display\", \n            choices=1, selected = 1, multiple = TRUE),\ntableOutput(\"table\")\n# # # # # # # # # \n# In server:\nmyData <- reactive({\n        inFile <- input$file_in\n        if (is.null(inFile)) {\n            return(NULL)\n        } else {\n            df <- read.table(inFile$datapath, header=input$header)\n            updateSelectInput(session, \"menu\", choices=1:ncol(df), selected=input$menu)\n            return(df)\n        }\n    })\noutput$table <- renderTable( myData()[,sort(as.numeric(input$menu))] )\nThe various input default values can be updated using the following functions on the server side:\n# Dropdown menu\nupdateSelectInput(session, \"menu_name\", choices=new_choices)\n# Text\nupdateTextInput(session, \"text_name\", value = new_value)\n# Numeric\nupdateNumericInput(session, \"value_name\", value = new_value)\n\n\n15.1.3.2.5 Writing a file\nThis is not a function of shiny, but you may want to write a text file. If this comes from a data.frame, you can use the function write.table():\ndf <- data.frame(x=1:10,y=sin(1:10))\nwrite.table(df, \"test.dat\", quote=FALSE, row.names=FALSE)\nFor other forms of printing, look into the write() function:\ntoprint <- paste(\"hello\", \"world\")\noutfile <- file(\"file_name.txt\", encoding=\"UTF-8\")\nwrite(toprint, file=outfile)\nclose(outfile)\nYou can for example write a Rmd file that you will render (as pdf, etc…) using render():\nrmarkdown::render(\"file_name.Rmd\")\n\n\n15.1.4 Example\nCreate a new shiny app with the following code, and play around with it. The input file should be the tidy population.txt.\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(DT)\n\nui <- fluidPage(\n    titlePanel(\"City population in France\"),\n    sidebarLayout(\n        sidebarPanel(\n            fileInput(\"file_in\", \"Choose input file:\",\n                      accept = c(\".txt\") ),\n            selectInput(\"sel_city\", \"City:\", choices = \"\", multiple = TRUE)\n        ),\n        mainPanel(\n            tabsetPanel(\n                tabPanel(\"Plot\", plotlyOutput(\"cityplot\", height = \"400px\")),\n                tabPanel(\"Table\", dataTableOutput(\"table\"))\n            )\n        )\n    )\n)\n\nserver <- function(input, output, session) {\n\n    # myData() returns the data if a file is provided\n    myData <- reactive({\n        inFile <- input$file_in\n        if (is.null(inFile)) {\n            return(NULL)\n        } else {\n            df <- read.table(inFile$datapath, header=TRUE)\n            # in case something changes,\n            # update the city input selection list\n            updateSelectInput(session, \"sel_city\",\n                              choices = unique(df$city),\n                              selected = unique(df$city)[1])\n            return(df)\n        }\n    })\n\n    # plot the pop vs year for the selected cities\n    output$cityplot <- renderPlotly({\n        df <- myData()\n        if(is.null(df)) return(NULL)\n        p <- df %>% \n            filter(city %in% input$sel_city) %>%\n            ggplot(aes(x=year, y=pop, size=pop, color=city)) +\n                geom_point() +\n                geom_smooth(method=\"lm\", alpha=0.1,\n                            show.legend = FALSE,\n                            aes(fill=city)) +\n                ggtitle(paste0(\"Population in \",\n                               paste(input$sel_city, collapse = \", \")\n                               ))+\n                labs(x=\"Year\", y=\"Population\")+\n                theme_light()\n        ggplotly(p, dynamicTicks = TRUE)\n    })\n\n    # show data as a table\n    output$table <- renderDataTable({\n        df <- myData() %>% filter(city %in% input$sel_city)\n        if(is.null(df)) return(NULL)\n        df <- pivot_wider(df, names_from=year, values_from=pop)\n        datatable(df, rownames = FALSE)\n    })\n\n}\n\nshinyApp(ui = ui, server = server)\nThis will render like this."
  },
  {
    "objectID": "16-shiny.html#rmarkdown-embedded-shiny-application",
    "href": "16-shiny.html#rmarkdown-embedded-shiny-application",
    "title": "\n15  Graphical interfaces with Shiny\n",
    "section": "\n15.2 Rmarkdown-embedded shiny application",
    "text": "15.2 Rmarkdown-embedded shiny application\nA shiny application can even be embedded inside a Rmarkdown document by providing runtime: shiny in the YAML header. A short example here, try to compile it:\n---\ntitle: \"Test\"\noutput: html_document\nruntime: shiny\n---\n\nThis is a test Rmarkdown document.\n\n```{r, echo=FALSE, message=FALSE}\nlibrary(ggplot2)\nlibrary(plotly)\ndf <- read.table(\"Data/population.txt\", header=TRUE)\n\n\nshinyApp(\n  ui = fluidPage(\n    selectInput(\"city\", \"City:\", choices = unique(df$city)),\n    plotlyOutput(\"cityplot\", height = 600)\n  ),\n\n  server = function(input, output) {\n    output$cityplot = renderPlotly({\n      p <- ggplot(data=subset(df,city==input$city), \n                aes(x=year, y=pop, size=pop)) +\n            geom_point() + \n            geom_smooth(method=\"lm\", alpha=0.1, show.legend = FALSE) + \n            ggtitle(paste(\"Population in \",input$city,sep=\"\"))+\n            labs(x=\"Year\", y=\"Population\")+\n            theme_light()\n      ggplotly(p)\n    })\n  }\n)\n```\nThe only “problem” with this solution is that the html file that is produced will not run the shiny app by itself, you have to open the Rmd file in Rstudio and hit “Run Document”.\nAnother solution consists in deploying your app on shinyapps.io and embedding the page in your document with:\n```{r, echo=FALSE}\nknitr::include_app(\"https://cbousige.shinyapps.io/shiny_example/\", \n                    height = \"800px\")\n```"
  },
  {
    "objectID": "16-shiny.html#deploying-your-shiny-app",
    "href": "16-shiny.html#deploying-your-shiny-app",
    "title": "\n15  Graphical interfaces with Shiny\n",
    "section": "\n15.3 Deploying your shiny app",
    "text": "15.3 Deploying your shiny app\nThere are 4 ways to deploy your app: passing the app.R file to your users, deploying to shinyapps.io, deploying on your own server, or building an executable with Electron.\n\n\n15.3.1 Passing the app.R file to your users\nThis option is certainly easy: just send your app.R file (or Rmd file with shiny embedded app) as well as any other files needed (e.g. global.R) to your users, explain to them how to run it, and voilà.\nHowever, this needs a little bit of know-how from the users: they need to install R and Rstudio, install the needed packages, and run the app.\nA good option to remove the “package-installing” step is to define a function check.package() that will check if the package is installed, install it if needed, and load it:\ncheck.packages <- function(pkg){\n    new.pkg <- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n# Usage:\ncheck.packages(\"ggplot2\")\n\n\n15.3.2 Deploying to shinyapps.io\nApplications deployed on shinyapps.io will be accessible from anywhere through a weblink. See for example my application to determine the pressure from a ruby Raman spectrum or the expected Raman shift for a given pressure and laser wavelength. Your application will however be public and you will have some limitations in the number of online applications and time of use (if you don’t pay a fee, see here for the various plans).\n\nFirst, create an account on shinyapps.io\n\nFollow the steps described here to:\n\n\nConfigure RSconnect: in your shinyapps.io dashboard, click your name, then Tokens, and create a token for a new app. Copy the text in the popup window.\n\nDeploy the app from the Rstudio window by clicking on the “Publish” button in the top right corner of the interface. Follow the steps along the shinyapps.io way.\n\n\n\nNote that in that case, you should not have any install.package() command in your code. Most packages are supported by shinyapps.io.\n\n\n15.3.3 Deploying on your own Linux server\nThis option is more advanced and I’m not going into details for that, but you have a number of tutorials online. See e.g. here, here or here.\nYou might consider this option if you work in a company that want to handle privately its data (which sounds plausible) and not pay the shinyapps.io fee to password protect the app. In that case, just work with the IT department to get it running.\n\n\n15.3.4 Building an executable\nOn Windows, there is this possibility that looks nice but that I never tried because I don’t have Windows: RInno.\nOn any platform: there is the possibility described here with the corresponding github page. This option is actually awesome and a quite recent possibility. However, since the produced application will contain R and the needed packages, the executable file is quite heavy."
  },
  {
    "objectID": "16-shiny.html#further-reading",
    "href": "16-shiny.html#further-reading",
    "title": "\n15  Graphical interfaces with Shiny\n",
    "section": "\n15.4 Further reading",
    "text": "15.4 Further reading\n\nThe Shiny cheatsheet\n\nHelp on deploying your shiny app\n\nGuide to application layout\nThe Shiny Gallery: find what you want to do and adapt it to your needs\nThe official Shiny video tutorial"
  },
  {
    "objectID": "16-shiny.html#exo-shiny",
    "href": "16-shiny.html#exo-shiny",
    "title": "\n15  Graphical interfaces with Shiny\n",
    "section": "\n15.5 Exercises",
    "text": "15.5 Exercises\nExercise 1\n\nCreate a new empty app with a blank user-interface and run it.\nAdd a title, a left panel and a main panel\nAdd an input numerical value defaulting to 1 and with a step of 0.05, name it “bw”\nAdd a slider input from 0 to 1e3 by steps of 1e2 defaulting to 5e2, name it “N_val”\nAdd a plot of the density of rnorm(N_val) with bandwidth bw\n\nMake sure bw>0, otherwise don’t produce the plot\n\n\nSolution\nlibrary(shiny)\n\nui <- fluidPage(\n    titlePanel(\"Some title\"),\n    sidebarLayout(\n        sidebarPanel(\n            numericInput(\"bw\", \"Enter bandwidth:\", 1, step=0.05),\n            sliderInput(\"N_val\", \"Number of points:\", \n                        min = 0, max = 1e4, step= 1e2, value = 5e2)\n        ),\n        mainPanel(\n            plotOutput(\"plot\", height = 600)\n        )\n    )\n)\n\nserver <- function(input, output, session) {\n    output$plot <- renderPlot({\n                        if(input$bw==0) return(NULL)\n                        plot(density(rnorm(input$N_val), bw=abs(input$bw)))\n                    })\n}\n\nshinyApp(ui = ui, server = server)\nExercise 2\nCreate a shiny application that will:\n\nread an input (through a file dialog) Raman spectrum from a ruby (XPdata.zip)\nfit the data by two Lorentzians\nplot the data interactively\nask for the laser wavelength as an input and give 568.189 nm as default\nwrite the corresponding pressure on the page using the Pruby() function defined in myfunc.R found in XPdata.zip.\ninsert a button that will, when pressed, render a pdf report displaying the laser wavelength, the plot, the fit and the pressure found:\n\nwrite a separate Rmd file with the proper parameters\nrender the Rmd file as a pdf (see the render() function and this help)"
  },
  {
    "objectID": "17-units.html#working-with-units",
    "href": "17-units.html#working-with-units",
    "title": "\n16  Working with units and experimental errors\n",
    "section": "\n16.1 Working with units",
    "text": "16.1 Working with units\nIt is easy to work with units in R thanks to the package units (see vignette).\nWorking with the units package can prove a very good idea to avoid conversion errors in your data treatment…\nHere is the gist of it:\n\n# Load the 'units' library\nlibrary(units)\nt <- seq(0.1,1,length=3)\n# attribute a unit, here 'seconds':\nt <- set_units(t, \"s\") \nt\n\n#> Units: [s]\n#> [1] 0.10 0.55 1.00\n\n\n\nWhen possible, automatic units conversion is performed. Also, it is possible to attribute units to columns of tibbles and data.frames:\n\n\nd1    <- set_units(seq(1,2,length=3), \"m\")\n(tib1 <- tibble(t=t, d=d1, speed=d1/t))\n\n#> Error in tibble(t = t, d = d1, speed = d1/t): could not find function \"tibble\"\n\nd2    <- set_units(seq(0.1, .3, length=3), \"cm\")\n(tib2 <- tibble(t=t, d=d2, speed=d2/t))\n\n#> Error in tibble(t = t, d = d2, speed = d2/t): could not find function \"tibble\"\n\nbind_rows(tib1,tib2)\n\n#> Error in bind_rows(tib1, tib2): could not find function \"bind_rows\"\n\n\n\nYou can moreover convert between units systems using set_units(vector, \"unit\") or units(vector) <- \"unit\":\n\n\nT <- set_units(1, \"fs\")\nset_units(1/T, \"THz\")\n\n#> 1000 [THz]\n\nunits(T) <- \"ns\"; T\n\n#> 1e-06 [ns]\n\n\n\nSometimes, you may define the unit outside of the set_units() call and want to retrieve it, or use a unit from another variable in a new variable. For this, use the mode=\"standard\" option:\n\n\nUNIT <- \"m\"\nset_units(1:10, UNIT)\n\n#> Error: In 'UNIT', 'UNIT' is not recognized by udunits.\n#> \n#> See a table of valid unit symbols and names with valid_udunits().\n#> Custom user-defined units can be added with install_unit().\n#> \n#> See a table of valid unit prefixes with valid_udunits_prefixes().\n#> Prefixes will automatically work with any user-defined unit.\n\nset_units(1:10, UNIT, mode=\"standard\")\n\n#> Units: [m]\n#>  [1]  1  2  3  4  5  6  7  8  9 10\n\nset_units(1:10, units(T), mode=\"standard\")\n\n#> Units: [ns]\n#>  [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\nYou can give units to table columns:\n\n\nlibrary(tidyverse)\nstarwars\n\n#> # A tibble: 87 × 14\n#>    name        height  mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n#>    <chr>        <int> <dbl> <chr>   <chr>   <chr>     <dbl> <chr> <chr>  <chr>  \n#>  1 Luke Skywa…    172    77 blond   fair    blue       19   male  mascu… Tatooi…\n#>  2 C-3PO          167    75 <NA>    gold    yellow    112   none  mascu… Tatooi…\n#>  3 R2-D2           96    32 <NA>    white,… red        33   none  mascu… Naboo  \n#>  4 Darth Vader    202   136 none    white   yellow     41.9 male  mascu… Tatooi…\n#>  5 Leia Organa    150    49 brown   light   brown      19   fema… femin… Aldera…\n#>  6 Owen Lars      178   120 brown,… light   blue       52   male  mascu… Tatooi…\n#>  7 Beru White…    165    75 brown   light   blue       47   fema… femin… Tatooi…\n#>  8 R5-D4           97    32 <NA>    white,… red        NA   none  mascu… Tatooi…\n#>  9 Biggs Dark…    183    84 black   light   brown      24   male  mascu… Tatooi…\n#> 10 Obi-Wan Ke…    182    77 auburn… fair    blue-g…    57   male  mascu… Stewjon\n#> # … with 77 more rows, 4 more variables: species <chr>, films <list>,\n#> #   vehicles <list>, starships <list>, and abbreviated variable names\n#> #   ¹​hair_color, ²​skin_color, ³​eye_color, ⁴​birth_year, ⁵​homeworld\n\nstarwars %>% \n    mutate(height = set_units(height,\"cm\"),\n           mass   = set_units(mass,\"kg\"))\n\n#> # A tibble: 87 × 14\n#>    name        height mass hair_c…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n#>    <chr>         [cm] [kg] <chr>    <chr>   <chr>     <dbl> <chr> <chr>  <chr>  \n#>  1 Luke Skywa…    172   77 blond    fair    blue       19   male  mascu… Tatooi…\n#>  2 C-3PO          167   75 <NA>     gold    yellow    112   none  mascu… Tatooi…\n#>  3 R2-D2           96   32 <NA>     white,… red        33   none  mascu… Naboo  \n#>  4 Darth Vader    202  136 none     white   yellow     41.9 male  mascu… Tatooi…\n#>  5 Leia Organa    150   49 brown    light   brown      19   fema… femin… Aldera…\n#>  6 Owen Lars      178  120 brown, … light   blue       52   male  mascu… Tatooi…\n#>  7 Beru White…    165   75 brown    light   blue       47   fema… femin… Tatooi…\n#>  8 R5-D4           97   32 <NA>     white,… red        NA   none  mascu… Tatooi…\n#>  9 Biggs Dark…    183   84 black    light   brown      24   male  mascu… Tatooi…\n#> 10 Obi-Wan Ke…    182   77 auburn,… fair    blue-g…    57   male  mascu… Stewjon\n#> # … with 77 more rows, 4 more variables: species <chr>, films <list>,\n#> #   vehicles <list>, starships <list>, and abbreviated variable names\n#> #   ¹​hair_color, ²​skin_color, ³​eye_color, ⁴​birth_year, ⁵​homeworld\n\n\n\nYou can plot using units and easily convert between units while plotting:\n\n\np <- starwars %>% \n    mutate(height = set_units(height,\"cm\"),\n           mass   = set_units(mass,\"kg\")) %>% \n    filter(sex != \"hermaphroditic\") %>%\n    ggplot(aes(x=height, y=mass, color=sex))+\n        geom_point(size=2)+\n        labs(x=\"Height\", y=\"Mass\")\np\n\n\n\np + ggforce::scale_x_unit(unit = \"inches\") +\n    ggforce::scale_y_unit(unit = \"pounds\")"
  },
  {
    "objectID": "17-units.html#working-with-experimental-errors",
    "href": "17-units.html#working-with-experimental-errors",
    "title": "\n16  Working with units and experimental errors\n",
    "section": "\n16.2 Working with experimental errors",
    "text": "16.2 Working with experimental errors\nWhen working in experimental science, you have to account for measurement errors and error propagation all along your data treatment. This is made really easy thanks to the quantities package that gathers the error and units packages. Most importantly, this allows you propagating the errors in the proper way. So, you input your experimental error once, and you don’t have to think about it anymore. Neat, isn’t it?\nHere is the gist of it:\n\nlibrary(quantities)\noptions(errors.notation=\"plus-minus\", errors.digits=4)\na <- set_errors(1, 0.1)\nb <- set_errors(2, 0.2)\na+b\n\n#> 3.0000 ± 0.2236\n\na*b\n\n#> 2.0000 ± 0.2828\n\na^3\n\n#> 1.0000 ± 0.3000\n\nerrors(a)\n\n#> [1] 0.1\n\nerrors_min(a)\n\n#> [1] 0.9\n\nerrors_max(a)\n\n#> [1] 1.1\n\n\nIt thus becomes easy to plot the error bars from your experimental data. I recommend using the ggforce library to make ggplot2 work better with quantities:\n\nlibrary(ggforce)\noptions(errors.notation=\"parenthesis\", errors.digits=1)\nstarwars %>% \n    mutate(height=set_quantities(height,\"cm\",height*.05),\n           mass=set_quantities(mass,\"kg\",mass*.05)\n           )\n\n#> # A tibble: 87 × 14\n#>    name     height     mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n#>    <chr>  (err) [… (err) [… <chr>   <chr>   <chr>     <dbl> <chr> <chr>  <chr>  \n#>  1 Luke …   172(9)    77(4) blond   fair    blue       19   male  mascu… Tatooi…\n#>  2 C-3PO    167(8)    75(4) <NA>    gold    yellow    112   none  mascu… Tatooi…\n#>  3 R2-D2     96(5)    32(2) <NA>    white,… red        33   none  mascu… Naboo  \n#>  4 Darth…  200(10)   136(7) none    white   yellow     41.9 male  mascu… Tatooi…\n#>  5 Leia …   150(8)    49(2) brown   light   brown      19   fema… femin… Aldera…\n#>  6 Owen …   178(9)   120(6) brown,… light   blue       52   male  mascu… Tatooi…\n#>  7 Beru …   165(8)    75(4) brown   light   blue       47   fema… femin… Tatooi…\n#>  8 R5-D4     97(5)    32(2) <NA>    white,… red        NA   none  mascu… Tatooi…\n#>  9 Biggs…   183(9)    84(4) black   light   brown      24   male  mascu… Tatooi…\n#> 10 Obi-W…   182(9)    77(4) auburn… fair    blue-g…    57   male  mascu… Stewjon\n#> # … with 77 more rows, 4 more variables: species <chr>, films <list>,\n#> #   vehicles <list>, starships <list>, and abbreviated variable names\n#> #   ¹​hair_color, ²​skin_color, ³​eye_color, ⁴​birth_year, ⁵​homeworld\n\nstarwars %>% \n    mutate(height=set_quantities(height,\"cm\",height*.05),\n           mass=set_quantities(mass,\"kg\",mass*.05)\n           ) %>% \n    filter(sex!=\"hermaphroditic\") %>% \n    ggplot(aes(x=height, y=mass, color=sex))+\n        geom_point(size=2)+\n        labs(x=\"Height\", y=\"Mass\")+\n        geom_errorbar(aes(ymin=errors_min(mass),\n                          ymax=errors_max(mass)))+\n        geom_errorbarh(aes(xmin=errors_min(height),\n                           xmax=errors_max(height)))"
  },
  {
    "objectID": "18-exercises.html",
    "href": "18-exercises.html",
    "title": "17  List of exercises",
    "section": "",
    "text": "The solutions are only available after the correction is done in class.\n\n\n\nExercise\nSolution\n\n\n\n\nBasicsdata.frame manipulation, simple plots and linear fits\nSolution\n\n\nFTIR dataReading data, plotting curves\nSolution\n\n\nSpectroscopic dataReading data, plotting and stacking curves\nSolution\n\n\nTGA dataLoading a complicated text file, derivative and plotting\nSolution\n\n\nCO2 emissionsData wrangling and ggplot2\nSolution\n\n\nCognitive PsychologyData wrangling and ggplot2\nSolution\n\n\nG(r,t)Reading data from MD simulations treatments and plotting them\nSolution\n\n\nReligion and babiesData wrangling, ggplot2 and plotly\nSolution\n\n\nBulk modulusData wrangling, ggplot2 and fitting\nSolution\n\n\nNanoparticles statistics from SEM imagesData wrangling, ggplot2 and fitting\nSolution\n\n\nCOVID-19Data wrangling, ggplot2, fits\nSolution"
  },
  {
    "objectID": "code_bits.html",
    "href": "code_bits.html",
    "title": "Code bits",
    "section": "",
    "text": "Here I provide various code bits that I wrote for fun or for learning some aspects. It’s here for you if you are interested:\n\n\n2D Gaussian fits: a colleague coming from Matlab wanted to be able to fit 2D Gaussians from Raman mappings. It’s actually not too hard with nls() and a tidy tibble. The only thing is that the initial guess of peaks positions needs to be not very far from the actual peaks positions – you can either provide it by hand or make an automatic peak finder.\n\nFractals: I played around with the Mandelbrot set to make fractals, and compared a full-R code to a code using Rcpp (it was for me the occasion of my first code using Rcpp)… Rcpp wins (largely!) in terms of speed here! And it makes for suuuuuper nice images, too. I got inspiration from a R-bloggers post.\n\n\n\n\n\n\n\n\n\n\nFitting a Raman map\n\n\n\n\n\n\nA fractal showing the Mandelbrot set"
  }
]