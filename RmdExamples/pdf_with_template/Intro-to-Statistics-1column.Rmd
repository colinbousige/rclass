---
title    : "Applied Statistics"
subtitle : "A one-hour introduction"
date     : "`r format(Sys.time(), '%B %d, %Y')`"
author:
- name       : Colin Bousige
  affiliation: "[Laboratoire des Multimatériaux et Interfaces](https://lmi.cnrs.fr/), Lyon, France"
  email      : colin.bousige@univ-lyon1.fr
- name       : John Doe
  affiliation: "Some university, Europe"
  email      : john.doe@some-university.eu
output: 
  bookdown::pdf_document2:
    template        : template.tex
    keep_tex        : false
    citation_package: biblatex
    highlight       : tango
    number_sections : true
    fig_caption     : true
    fig_number      : true
    fig_width       : 4
    fig_height      : 3
    df_print        : kable
    toc             : true
    toc_depth       : 5
abstract: |
    This class aims to provide an introduction to statistics applied to experimental measurements in physics, chemistry or biology. By studying practical cases, we will see the importance of using statistical tools to make sense of experimental data.
geometry : margin=1in
fontsize : 11pt
showdate : false
bibliography: "biblio.bib"
---


```{r setup, include=FALSE, warning = FALSE, cache=FALSE}
library(pracma)
library(knitr)
library(kableExtra)
library(tidyverse)
library(patchwork)
library(quantities)
library(broom)
theme_set(theme_bw()+
    theme(text=element_text(size=10),
          strip.background = element_blank(),
          strip.text = element_text(face="bold"),
          panel.border = element_rect(size=1)
         )
    )
colors <- colorRampPalette(c("black","royalblue","seagreen","orange","red"))
kable <- function(data, ...) {
    knitr::kable(data, digits = 3, booktabs = TRUE, linesep = "", ...) %>% kable_styling(latex_options = c("striped", "hold_position"), full_width = F, position = "center")
}
knit_print.data.frame <- function(x, ...) {
    res <- paste(c("", "", kable(x)), collapse = "\n")
    asis_output(res)
}
registerS3method("knit_print", "data.frame", knit_print.data.frame)
opts_chunk$set(fig.align = "center", message = FALSE, warning = FALSE, echo = FALSE)
```

# Why are statistical tools necessary in physical science?

When doing Science, one has to fully grasp the concept of *physical measurement*. Let's take an example to visualize the importance of this concept.

## A practical example

Let's say you want to communicate to someone a temperature, and tell this person that the temperature is "38". If this is a random person in the street, they might think: "nice, let's go to the beach today!". If this random person is from the USA, they're gonna think: "damn, where did I put my coat?". If that person happens to be a physician, they might think: "that kid's got a slight fever". If they are a physicist doing a cryostat experiment, they might think "let's check the He tank level"... you see that one of the most important part of the measurement is missing: its unit. Units are there so that people understand each other when exchanging data, and you see here that 38 Celsius, 38 Fahrenheit or 38 Kelvin are quite different, and this quantity will mean different things in different contexts. A physical quantity given without its unit would be absolutely meaningless (unless, of course, you are looking at a unit-less quantity, like a count).

Now let's consider the body temperature of 38 °C given to a physician. How did you measure this temperature? With a mercury graduated thermometer or with a thermocouple? In the first case, you can probably assume that this value is given with a measurement error of at least 1 °C, meaning that the temperature you give to the physician is $(38\pm1)$ °C, $i.e.$ the physician won't be able to decide whether they should be concerned or not. In the second case, the temperature is often given with a 0.1 °C precision, so the physician, seeing that the body temperature is $(38\pm0.1)$ °C, will probably tell you to take an aspirin and rest instead of giving you something stronger to treat a possible infection. Given that the uncertainty on the given value is of 0.1 °C, one should in fact give the temperature with matching decimal precision, *i.e.* $(38.0\pm0.1)$ °C. Writing $(38\pm0.1)$ °C, $(38.00001\pm0.1)$ °C or $(38.00\pm0.10000)$ °C would be meaningless too.

> With this, we see that a physical measurement should be given with four parts: its actual **value**, its **decimal precision**, its **uncertainty**, and its **unit**. Should any of these four parts be missing in a physical quantity that you wanted to share, it would at best be imprecise, and at worst be utterly meaningless.

## Probabilistic description of physical systems

Let's continue with our example of the body temperature measured with a thermocouple or a laser thermometer with a 0.1 °C precision.
Our first measurement of the body temperature yielded $(38.0\pm0.1)$ °C. Now let's repeat this measurement a number of times in various area of the body (which are left to your imagination). Let's say it then shows $(38.1\pm0.1)$ °C, $(38.0\pm0.1)$ °C, $(38.3\pm0.1)$ °C, $(37.9\pm0.1)$ °C, $(38.2\pm0.1)$ °C, $(38.1\pm0.1)$ °C, $(38.1\pm0.1)$ °C, $(39.8\pm0.1)$ °C. What is the actual body temperature then? Should we stick to a single measurement? Of course not. We have to make an histogram of the measured values, and study the distribution of the measurements (Fig. \@ref(fig:histogramT)). We can then see that one of the values is clearly an outlier -- something might have gone wrong there. What if we had done the measurement only once and only measured that value? We might have jumped to a very wrong conclusion, with possibly a very serious consequence like giving the wrong medicine.


```{r, histogramT, echo=FALSE, fig.cap="Histogram of the body temperature measurements. The red line is the mean value, the orange one is the mode and the blue one is the median."}
temp <- tibble(T=c(38, 38.1, 38.0, 38.0, 38.3, 37.9, 38.1, 38.2, 39.8))
temp %>% 
    ggplot(aes(x = T)) +
       geom_histogram(alpha=.5, color="black")+
       geom_vline(xintercept = mean(temp$T), color="red", size=1)+
       geom_vline(xintercept = 38, color="orange", size=1)+
       geom_vline(xintercept = median(temp$T), color="royalblue", size=1)+
       labs(x = "T [°C]")
```

With this example, we see that **a physical measurement is not absolute**. In fact, a physical measurement is a **probability** that the measured value is within a certain range. In the case of our example, after removing the outlier for which we are certain that the measurement is wrong, it means that the measured body temperature has a high probability to be somewhere between 38.0 °C and 38.2 °C.
In other (more general) terms, one could consider a measurement of a quantity $X$ as a probability $P(x - \sigma < X < x + \sigma )$ that the quantity $X$ has a value between $x-\sigma$ and $x+\sigma$. The **uncertainty** $\sigma$ around the **mean value** $x$ is usually given as the **standard deviation** of the distribution of measurements around the mean.

> Since physical measurements are in fact **probabilities**, we **can** -- and **must** -- use **statistical tools** to characterize them.



# Quantifying the properties of data

## Data representation -- presenting a measurement

Depending on the data you are looking at, various ways of representing them are possible. I can't stress enough the importance of picking the right representation for your data, it is the expression of your physical sense. A good representation will help you make sense of your data and communicate your results. A bad representation, well...

### Histograms

When looking at discrete values or when you want to characterize the distribution of a measurement, it is often a good idea to use the histogram representation, which represents the frequency at which a measurement is made within a certain range, called bin. Let's take Fig. \@ref(fig:histogramT) and plot it with various bin sizes. One can see that the choice of bin size is important, as it determines whether your data are noisy or lack fine information.

```{r, out.width='32%', fig.show="hold", fig.cap="Histogram of the body temperature measurements with different bin widths."}
temp <- tibble(T=c(38, 38.1, 38.0, 38.0, 38.3, 37.9, 38.1, 38.2, 39.8))
temp %>% 
    ggplot(aes(x = T)) +
       geom_histogram(alpha=.5, color="black", binwidth = 0.1)+
       geom_vline(xintercept = mean(temp$T), color="red", size=1)+
       geom_vline(xintercept = median(temp$T), color="royalblue", size=1)+
       theme(text = element_text(size = 20), title = element_text(size = 16))+
       labs(title="bin width = 0.1 °C", x="T [°C]")
temp %>% 
    ggplot(aes(x = T)) +
       geom_histogram(alpha=.5, color="black", binwidth = 0.2)+
       geom_vline(xintercept = mean(temp$T), color="red", size=1)+
       geom_vline(xintercept = median(temp$T), color="royalblue", size=1)+
       theme(text = element_text(size = 20), title = element_text(size = 16))+
       labs(title="bin width = 0.2 °C", x="T [°C]")
temp %>% 
    ggplot(aes(x = T)) +
       geom_histogram(alpha=.5, color="black", binwidth =1)+
       geom_vline(xintercept = mean(temp$T), color="red", size=1)+
       geom_vline(xintercept = median(temp$T), color="royalblue", size=1)+
       theme(text = element_text(size = 20), title = element_text(size = 16))+
       labs(title="bin width = 1 °C", x="T [°C]")
```

### Graphs

In case you want to represent continuous data, say the evolution of a quantity $y$ with respect to a quantity $x$, you should then use the graph representation. As we saw before, any physical quantity should be given with its uncertainty and unit. **The same applies to a graph**: it **must** clearly display the **units** of the quantities $x$ and $y$, and **error bars** that are usually taken as the standard deviation of each individual measurement (that should thus be performed a number of times, depending on what you are looking at). 

```{r, out.width='32%', fig.show="hold", fig.cap="Representing the same datapoints without error bars, with large error bars with respect to the data, and with small error bars with respect to the data: the difference between meaningless data, noise, and meaningful data."}
dat <- tibble(x=seq(0,100,1), 
              y=1+dnorm(x,mean=10,sd=5)+dnorm(x,mean=58,sd=10)+dnorm(x,mean=75,sd=15)+.01*runif(length(x)),
              dy1=.1/y,
              dy2=.1*(y-1)
              )
dat %>% 
    ggplot(aes(x = x, y = y)) +
       geom_point(alpha=0.5)+
       theme(text=element_text(size=20), title = element_text(size=16))+
       labs(title="Meaningless graph", x="x", y="y")
dat %>% 
    ggplot(aes(x = x, y = y)) +
       geom_point(alpha=0.5)+
       geom_errorbar(aes(ymin=y-dy1,ymax=y+dy1))+
       theme(text=element_text(size=20), title = element_text(size=16))+
       labs(title="Noise", x="x [unit]", y="y [unit]")
dat %>% 
    ggplot(aes(x = x, y = y)) +
       geom_point(alpha=0.5)+
       geom_errorbar(aes(ymin=y-dy2,ymax=y+dy2))+
       theme(text=element_text(size=20), title = element_text(size=16))+
       labs(title="Meaningful graph", x="x [unit]", y="y [unit]")
```

You can think of each set of {datapoint + error bar} as an histogram: the displayed point is the mean value of the histogram, and the error bar is its standard deviation. **Therefore, plotting a straight line between points is usually pointless**. Plotting a line going through the data points only has meaning if this line results from a physical model explaining the variation of the quantity $y$ with the evolution of the quantity $x$ -- this is called a **fit**, and we will see more about it in the R class later.

## Characterizing an ensemble of measurements

If we take $N$ repeated measurements of an observable $x$, it is then natural to try to assess our knowledge of the ensemble of measures through (1) a single number representing the measured quantity, and (2) a second number representing the spread of measurements. As we saw before, the observable $x$ is thus generally defined by its central (mean) value $\ave{x}$, its spread $\sigma_x$ (standard deviation or uncertainty), and its unit. 

### Central value: mode, median and mean

The **mode** of an ensemble of measurements is its *most frequent value*. If the measurement in question is of a continuous variable, one has to bin the data in terms of a histogram in order to quantify the modal value of that distribution: the mode will be the position of the maximum of the histogram.

The **median** value of the ensemble is the value of $x$ for which there are an equal number of measurements above and below that point. If there is an even number of measurements, then the median value is taken as the midpoint between the two most central values.

The **mean** (or arithmetic average) is more often used than the two previous quantities, as it usually provides a better way to quantify the "typical" value measured. The mean value is denoted either by $\mean{x}$ or $\ave{x}$, and is given by:

\begin{equation}
\mean{x}=\ave{x}=\frac{1}{N}\sum_{i=1}^Nx_i,
\end{equation}
where $x_i$ is the $i$-th measurement of $x$. 

Figure \@ref(fig:histogramT) shows the representation of a sample of data plotted in a histogram. This figure shows the mode, mean and median. For this particular sample of data, the mean is `r round(mean(temp$T),1)` °C, the median is `r round(median(temp$T),1)` °C, and the mode is 38.0 °C. The fact that the mode is smaller than the mean is an indication that the data are asymmetric about the mean. We usually refer to such a distribution as being skewed, and in this case the data are skewed to the right.


### Quantifying the spread of data: variance and standard deviation

The mean of an ensemble of data doesn't provide any information as to how the data are distributed. So any description of a set of data just quoting a mean value is incomplete. We need a second number in order to quantify the dispersion of data around the mean value. The average deviations from the mean, $\ave{x-\mean{x}}$, is not a useful quantity as, by definition, this will be zero for a symmetrically distributed sample of data (which is always the case for randomly distributed data -- a consequence of the central limit theorem, as we will see later). We should rather consider the average value of the squared deviations from the mean as a measure of the spread of our ensemble of measurements. This is called the **variance** $V(x)$, which is given by:

\begin{equation}
\begin{aligned}
V(x)&=\ave{(x-\mean{x})^2}\\
    &=\frac{1}{N}\sum_{i=1}^N(x_i-\mean{x})^2\\
    &=\mean{x^2}-\mean{x}^2
\end{aligned}
(\#eq:variance)
\end{equation}

The square root of the mean-squared (root-mean-squared or RMS) deviation is called the **standard deviation**, and this is given by:

\begin{equation}
\begin{aligned}
\sigma(x)&=\sqrt{V(x)}\\
         &=\sqrt{\mean{x^2}-\mean{x}^2}
\end{aligned}
(\#eq:sd)
\end{equation}

The standard deviation quantifies the amount by which it is reasonable for a measurement of $x$ to differ from the mean value $\mean{x}$. Considering a Gaussian distribution, we would expect to have 31.7% of measurements deviating from the mean value by more than 1$\sigma$, and this goes down to 4.5% of measurements to deviate by more than 2$\sigma$, and 0.3% of measurements to deviate by more than 3$\sigma$. Thus, if we perform a measurement that deviates by a significant margin from the expected value of $\ave{x}\pm\sigma$, we need to ask ourselves about the significance of our measurement.

In general, scientists often prefer using the standard deviation rather than the variance when describing data, since as the former has the same units as the observable being measured.

> A measurement of a quantity $x$ is therefore usually presented under the form $\ave{x}\pm\sigma_x$, where $\ave{x}$ is the arithmetic average and $\sigma_x$ is the standard deviation of the data.

### Caveats

The above considerations all assume that the distribution of measured values is mono-modal, *i.e.* the histogram of the measured values is centered around a single value. In the case of a multimodal distribution such as shown in Fig. \@ref(fig:multimodal), it would be meaningless to use such tools as the fine information on the distribution would be lost. 

```{r, multimodal, fig.cap="A trimodal distribution of measurements. The red line shows the mean value of the distribution: it fails to grasp the complexity of the distribution."}
dat <- tibble(x=seq(0,130,.1), 
              y=dnorm(x,mean=12,sd=2)+1.5*dnorm(x,mean=54,sd=10)+dnorm(x,mean=80,sd=12)) %>% 
        mutate(y = y/(sum(y)*.1))
MM <- sum(dat$x*dat$y)*.1
dat %>% 
    ggplot(aes(x = x, y = y)) +
       geom_line()+
       geom_vline(xintercept = MM, color="red", size=1)+
       labs(x = "x [unit]", y = "Density")
```

In this case, one should try to deconvolute the distribution in terms of individual peaks, and gather their positions, widths and intensities.



# Useful distributions

## Probability Density Functions

We should now introduce the notion of **Probability Density Function** (PDF). 
*By definition*, a PDF is a distribution where the **total area is unity**. The variation of the PDF is represents the probability of something occurring at that point in the parameter space.
In general, a PDF will be described by some function $P(x)$, where

\begin{equation}
\int_a^b P(x)dx=1,
(\#eq:PDFnorm)
\end{equation}
where $a$ and $b$ are the limits of the valid domain for the $P(x)$ function. The probability of obtaining a result between $x$ and $x + dx$ is thus $P(x)dx$. Usual PDFs encountered in physics are the Poisson distribution as well as the Gaussian distribution, that we will describe in a bit.

## PDFs, mean and variance

Let us define a PDF $P(x)$ describing a continuous distribution.
We can compute the average value of some quantity by computing the integral over this quantity multiplied by the PDF. 

For example, the **average value** of the variable $x$, distributed according to the PDF $P(x)$ in the domain $-\infty < x <\infty$, is given by:

\begin{equation}
\begin{aligned}
\ave{x}&=\int_{-\infty}^{\infty}xP(x)dx\\
\text{or } \ave{x}&=\sum_{i}x_iP(x_i) \text{ in the case of a discrete distribution}
\end{aligned}
(\#eq:firstmoment)
\end{equation}

This is called the *first moment* of the PDF.

This method can be used to compute average values of more complicated expressions. The mean value of $(x - \mean{x})^2$, *i.e.* the variance $V$, is thus given by the $\mean{x}$-centered second moment of the PDF, such as:

\begin{equation}
\begin{aligned}
V&=\int_{-\infty}^{\infty}(x - \mean{x})^2P(x)dx\\
\text{or } V&=\sum_{i}(x_i - \mean{x})^2P(x_i) \text{ in the case of a discrete distribution}
\end{aligned}
(\#eq:secondmoment)
\end{equation}


## The Poisson distribution

### Definition

When a certain reaction happens randomly in time with an average frequency $\lambda$ in a given time interval, then the number $k$ of reactions in that time interval will follow a Poisson distribution:

\begin{equation}
P_\lambda(k) = \frac{\lambda^ke^{-\lambda}}{k!}
(\#eq:poisson)
\end{equation}

Examples of encounters of Poisson distributions could be as various as the number of calls received per hours in a call center, the yearly number of Prussian soldiers killed by horse kicks... or the number of particles (photons, neutrons, neutrinos...) hitting a detector every second.



```{r, poissondistrib, out.width="32%", fig.show="hold", fig.cap="Poisson distribution for various parameters. While asymmetric for small values of $k$ and $\\lambda$, it tends towards a Gaussian lineshape at larger values."}
P <- function(lam, k) {
    lam^k*exp(-lam)/factorial(k)
}
P2 <- function(lambda, k) {
    tibble(lam = lambda, poisson = lambda^k * exp(-lam) / factorial(k))
}
dat <- tibble(k=rep(0:10,4), 
              lam=rep(c(1,2,5,8), each=11),
              poisson=P(lam,k))
dat2 <- tibble(k=0:5) %>%  
        mutate(poisson=map(k, ~P2(seq(0,15,.1), .))) %>% 
        unnest(poisson)
dat3 <- tibble(k = 100) %>%
    mutate(poisson = map(k, ~ P2(seq(0, 200, .1), .))) %>%
    unnest(poisson)
dat %>% 
    ggplot(aes(x = k, y = poisson, color=factor(lam))) +
       geom_line(size=1)+
       labs(color=expression(lambda), 
            x=expression(italic("k")), 
            y=expression(italic("P")[lambda]*(italic("k"))))+
        theme(text = element_text(size = 20), title = element_text(size = 16))
dat2 %>% 
    ggplot(aes(x = lam, y = poisson, color=factor(k))) +
       geom_line(size=1)+
       labs(color = expression(italic("k")), 
            x = expression(lambda), 
            y = expression(italic("P")[lambda] * (italic("k"))))+
        theme(text = element_text(size = 20), title = element_text(size = 16))
dat3 %>% 
    ggplot(aes(x = lam, y = poisson, color=factor(k))) +
       geom_line(size=1)+
       labs(color = expression(italic("k")), 
            x = expression(lambda), 
            y = expression(italic("P")[lambda] * (italic("k"))))+
        theme(text = element_text(size = 20), title = element_text(size = 16))
```


### Characteristics

As shown on Fig. \@ref(fig:poissondistrib), for small $\lambda$ the distribution is asymmetric and skewed to the right. As $\lambda$ increases the Poisson distribution becomes more symmetric.

Following Eq. \@ref(eq:firstmoment), the average number of observed events, $\ave{k}$, is given by:

\begin{equation}
\begin{aligned}
\ave{k} &= \sum_{k=0}^\infty kP_\lambda(k) = \sum_{k=1}^\infty k\frac{\lambda^ke^{-\lambda}}{k!}\\
        &= \lambda e^{-\lambda} \sum_{k=1}^\infty \frac{\lambda^{k-1}}{(k-1)!}= \lambda e^{-\lambda} \sum_{k=0}^\infty \frac{\lambda^{k}}{k!}\\
        &= \lambda
\end{aligned}
\end{equation}


In the same manner and by using the "trick" $x^2=x(x-1)+x$, the variance $\sigma^2(k)$ of the distribution is given by:

\begin{equation}
\begin{aligned}
\sigma^2(k) &= \sum_{k=1}^\infty (k-\lambda)^2\frac{\lambda^k e^{-\lambda}}{k!}\\
        &= \lambda e^{-\lambda} \left[\sum_{k=1}^\infty k^2\frac{\lambda^{k-1}}{k!} \underbrace{-2\lambda\sum_{k=1}^\infty \frac{\lambda^{k-1}}{(k-1)!}}_{-2\lambda e^\lambda}+\underbrace{\sum_{k=1}^\infty \lambda^2\frac{\lambda^{k-1}}{k!}}_{\lambda e^\lambda}\right]\\
        &= \lambda e^{-\lambda} \left[ \underbrace{\sum_{k=2}^\infty k(k-1)\frac{\lambda^{k-1}}{k!}}_{\lambda e^\lambda} + \underbrace{\sum_{k=1}^\infty k\frac{\lambda^{k-1}}{k!}}_{e^\lambda} - \lambda e^\lambda\right]\\
        &=\lambda = \ave{k}
\end{aligned}
\end{equation}

> The important result here is that, **when counting random events with an average of** $\mathbold{\ave{N}}$, **the standard deviation is** $\mathbold{\sigma=\sqrt{\ave{N}}}$. This is typically what happens when performing a diffraction or spectroscopic measurement, such as X-ray diffraction, Raman, IR or neutron spectroscopy, etc.: the longer we acquire data, the higher the number of detected "events" $N$ (particle hits detector), and the "better is the statistics". Indeed, the relative error is thus $\sqrt{N}/N=1/\sqrt{N}$. 
> \
>
> The consequence of this is that to make a factor 10 improvement on the relative error, one has to increase by 100 the number of events. This is usually done by increasing the acquisition time, which is fine as long as it is short enough. If irrealistic acquisition times start to become necessary, one should maybe try to find another way to increase $N$: this can be done by improving the detector efficiency, increasing the probe (laser, neutron/x-ray) brightness, changing the experimental geometry, etc.
>\
>
> Finally, for "large" numbers ($\lambda\gtrsim 100$) the Poisson distribution tends towards a symmetric Gaussian distribution that we will describe just after. 


\clearpage

## The Gaussian distribution

### Definition

The Gaussian distribution, also known as the *normal distribution*, with a mean value $\mu$ and standard deviation $\sigma$ as a function of some variable $x$ is given by:

\begin{equation}
P(x, \mu, \sigma)=\frac{1}{\sigma\sqrt{2\pi}}e^{-(x-\mu)^2/2\sigma^2}
\end{equation}

It is useful to transform data from the $x$ space to a corresponding $z$ space which has a mean value of zero, and a standard deviation of one. This transformation is given by the mapping $z=\frac{x-\mu}{\sigma}$, and the Gaussian distribution in terms of $z$ is thus:

\begin{equation}
P(z)=\frac{1}{\sqrt{2\pi}}e^{-z^2/2}
\end{equation}

### Characteristics

```{r, gaussian, fig.cap="A zero-centered Gaussian distribution with standard deviation of 1, $P(z)$. The red line marks the half maximum, $P(z_{HM})=1/2\\sqrt{2\\pi}$, and the blue lines the values of $z$ for which the half maximum is obtained, $z_{HM}=\\pm\\sqrt{2\\ln{2}}$."}
dat <- tibble(x=seq(-5,5,.1), y=exp(-x^2/2)/sqrt(2*pi))
seg1 <- tibble(x=c(sqrt(2*log(2)),sqrt(2*log(2))),
               y=c(0,1/2/sqrt(2*pi)))
seg2 <- tibble(x=-c(sqrt(2*log(2)),sqrt(2*log(2))),
               y=c(0,1/2/sqrt(2*pi)))
dat %>% 
    ggplot(aes(x = x, y = y)) +
        geom_line()+
        labs(x="z", y="P(z)")+
        geom_hline(yintercept = 1/2/sqrt(2*pi), col="red")+
        geom_line(data=seg2, col="royalblue")+
        geom_line(data=seg1, col="royalblue")+
        geom_segment(aes(x = -sqrt(2*log(2)), y = 1/2/sqrt(2*pi)/2, 
                         xend = sqrt(2*log(2)), yend = 1/2/sqrt(2*pi)/2),
            lineend = "round", linejoin = "round",
            arrow = arrow(ends = "both", length = unit(0.25, "cm")))+
        annotate("text", x = 0, y = .12, label="FWHM")
```

Sometimes instead of quantifying a Gaussian distribution (or any monomodal distribution, for that matter) using the variance or standard deviation, scientists will speak about the full width at half maximum (**FWHM**). 
This has the advantage that any extreme outliers of the distribution do not contribute to the quantification of the spread of data. As the name suggests, the FWHM is the width of the distribution (the spread above and below the mean) at the points where the distribution reaches half of its maximum. 

For a Gaussian distribution $P(z)$, the half maximum is attained when $z_{HM}$ is so that:

\begin{equation}
\begin{aligned}
\frac{1}{\sqrt{2\pi}}e^{-z_{HM}^2/2}&= \frac{1}{2}\frac{1}{\sqrt{2\pi}}\\
\Rightarrow z_{HM}&=\pm\sqrt{2\ln{2}}
\end{aligned}
\end{equation}

The FWHM of $P(z)$ is therefore $FWHM=2\sqrt{2\ln{2}}\simeq2.355$. Using the relation between $z$ and $\sigma$, we get the relation between the FWHM and the standard deviation:

\begin{equation}
FWHM=2\sqrt{2\ln{2}}\times\sigma
\end{equation}

As can be seen on Table \@ref(tab:tablevalues), using the FWHM ensures that roughly 76% of the data are comprised between $\mu-\sigma$ and $\mu+\sigma$, and this goes up to $\sim95$% for $\mu-2\sigma$ and $\mu+2\sigma$.

```{r, tablevalues}
x <- seq(-10,10,.001)
y <- dnorm(x,0,1)
integral <- function(x,a=1) {
    sprintf("%.5f", sum(y[abs(x) <= a] * (x[2] - x[1])))
}
dat <- tibble(`Integration range $a$` = c("$\\sigma$", "$ \\sqrt{2\\ln{2}}\\sigma$", "$ 2\\sigma$", "$ 3\\sigma$", "$ 4\\sigma$"),
       `$\\int_{-a}^aP(z)dz$` = c(integral(x, a = 1), integral(x, a = sqrt(2 * log(2))), integral(x, a = 2), integral(x, a = 3), integral(x, a = 4))
      )
kable(dat, format = "latex", escape = FALSE, 
      caption = "Integral values for various values of $a$ in $\\int_{-a}^aP(z)dz$.")
```

# Uncertainty and errors

## Central limit theorem: on the Gaussian nature of statistical uncertainty

The **central limit theorem** states that if one takes $N$ random independent samples of a distribution of data that describes some variable $x$, then as $N$ tends to infinity, the distribution of the sum of the samples tends to a Gaussian distribution. 

In other terms: the mean value of a large number $N$ of independent random variables (that can be distributed following any distribution with finite variance), obeying the same distribution with variance $\sigma_0^2$, approaches a normal distribution with variance $\sigma^2 = \sigma _0^2/N$.

> **This result is fundamental** as it implies that **independent measurements of any observable will show values that will be spread following a Gaussian distribution**, and thus statistical uncertainties that are Gaussian in nature.
> \
> 
> Moreover, we see here the typical property of statistical errors, which is that **the relative error is proportional to** $\mathbf{1/\sqrt{N}}$. Increasing the number of observations thus decreases the error, *i.e.* increases the precision.


## Combination of errors

Let us consider a function of $n$ variables, $f(u_1, u_2, ..., u_n)$. We can Taylor expand this function about the various mean values $u_i=\mean{u_i}$, so that, at the first order:

\begin{equation}
f(u_1, ..., u_n) = f(\mean{u_1}, ..., \mean{u_n}) + \sum_{i=1}^n (u_i-\mean{u_i})\frac{\partial f}{\partial u_i}
\end{equation}

Considering that the variance of a quantity $f$ is given by $\sigma^2(f) = (f - \mean{f} )^2$, it follows that the variance of our multivariable function is given by:

\begin{equation}
\begin{aligned}
\sigma^2(f) &= \left(\sum_{i=1}^n (u_i-\mean{u_i})\frac{\partial f}{\partial u_i}\right)^2\\
         &= \sum_{i=1}^n \left(\frac{\partial f}{\partial u_i}\right)^2\sigma_{u_i}^2 + 2\sum_{i\ne j}\frac{\partial f}{\partial u_i}\frac{\partial f}{\partial u_j}\sigma_{u_iu_j}\\
\end{aligned}
\end{equation}
where we have replaced $(u_i-\mean{u_i})^2$ by the variance $\sigma_{u_i}^2$ and $(u_i-\mean{u_i})(u_j-\mean{u_j})$ by the covariance $\sigma_{u_iu_j}$.
\

If the variables $u_i$ are independent then the covariance $\sigma_{u_iu_j}$ is null, and it follows the general expression of the standard error that can be applied to **any function of independent variables**:

> \begin{equation}
> \sigma(f) = \sqrt{\sum_{i=1}^n \left(\frac{\partial f}{\partial u_i}\right)^2\sigma_{u_i}^2}
> (\#eq:uncertainty)
> \end{equation}


### Functions of one variable

Let us consider a function $f$ having a form that depends only on one observable $x$, for example:

\begin{equation}
f = Ax + B
\end{equation}

Then, following Eq. \@ref(eq:uncertainty), the standard error on that function is given by:

\begin{equation}
\begin{aligned}
\sigma_f &= \sqrt{\left(\frac{\partial f}{\partial x}\right)^2\sigma_x^2}\\
         &= A\sigma_x
\end{aligned}
\end{equation}

So, **independently of any offset of the measured observable**, the resulting error must be corrected by the same factor as the intensity.

> **In practice**, let's say we measure a Raman spectrum. As we saw above, the error on each intensity count is given by the square root of this intensity count. 
> 
> - It is possible to shift vertically this spectrum without having to recompute the error bars.
> - But if you want to normalize (say, to 1) this spectrum, you have to multiply all the errors by the renormalization constant.


### Functions of two variables

Now consider the function $f = Ax + By$, where we have measured the mean and standard deviation of both $x$ and $y$, and want to compute the standard deviation on their sum/subtraction. We can use the general formula of Eq. \@ref(eq:uncertainty) to determine how to do this, hence:

\begin{equation}
\begin{aligned}
\sigma_f &= \sqrt{\left(\frac{\partial f}{\partial x}\right)^2\sigma_x^2 + \left(\frac{\partial f}{\partial y}\right)^2\sigma_y^2}\\
         &= \sqrt{A^2\sigma_x^2 + B^2\sigma_y^2}
\end{aligned}
\end{equation}

> **In practice**, let's say we measure an UV spectrum of a solution (a molecule in a solvent), and a reference spectrum of this solvent. As we saw above, the error on each intensity count is given by the square root of this intensity count. We want to subtract the signal of the solvent to get only the signal of the molecule.
> \
> 
> We thus have to perform the above operation on the errors, $\sigma_{result}=\sqrt{\sigma^2_{solution}+\sigma^2_{reference}}$. It means that in order to have a statistically sound resulting spectrum, the reference needs to be measured with a very good statistics in order to not dominate the resulting error.



# Further reading {-}

\nocite{bevan_statistical_2013, bohm_introduction_2010, watkins_introduction_2016}
